
var documents = [{
    "id": 0,
    "url": "/404.html",
    "title": "404",
    "body": "404 Page not found!Please use the search bar from the bottom left or visit our homepage! "
    }, {
    "id": 1,
    "url": "/about",
    "title": "Hey, I’m an Chinese software engineer living and working in Chengdu. I love Creating the future in digital worlds, big and small.",
    "body": "My favourites::  Vim Ruby Elixir Soccer HearthStone Detective StoryMy main career::  ThoughtWorks Jinshuju. net Sudiyi (HiveBox now)"
    }, {
    "id": 2,
    "url": "/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 3,
    "url": "/",
    "title": "Home",
    "body": "                                                                                              Hotwire: 没有JavaScript的Reactive Rails                         1 2 3 4 5                      :       本文已获得原作者（Vladimir Dementyev）和 Evil Martians 授权许可进行翻译。原文介绍了 Rails 的最新“魔法”：Hotwire。这也是 Vladimir Dementyev 在 RailsConf 2021 上的演讲内容。:                                                                               Mr. Z                 24 Apr 2021                                                                                                                          Tailwindcss底层基石的理念              :       Tailwindcss 从 2019 年开始逐渐在国外的 Web 开发圈子内盛行起来。国内倒是至今仍然不温不火。2020 年的 Ruby China 上过纯中做过一次在项目上使用 tailwindcss 体验的有关演讲。我在 2020 年也写过一篇有关的博客“在 Rails 6 中整合 Stimulus 和 Tailwind CSS”（被前者在演讲中所引用^_^）。:                                                                               Mr. Z                 10 Apr 2021                                                                                                                          Hotwire之构建Turbo应用              :       本文是对构建 Turbo 应用的具体描述，原文出自：https://turbo. hotwire. dev/handbook/building。:                                                                               Mr. Z                 26 Mar 2021                                                                                                                          Hotwire之使用Turbo Streams焕发活力              :       本文是对 Turbo Streams 的详细说明，原文出自：https://turbo. hotwire. dev/handbook/streams。:                                                                               Mr. Z                 19 Mar 2021                                                                                                                          Hotwire之使用Turbo Frame解构页面              :       本文是对 Turbo Frame 的详细说明，原文出自：https://turbo. hotwire. dev/handbook/frames。:                                                                               Mr. Z                 16 Mar 2021                                                                                                                          Hotwire之使用Turbo Drive导航              :       本文是对 Turbo Drive 的详细说明，原文出自：https://turbo. hotwire. dev/handbook/drive。:                                                                               Mr. Z                 13 Mar 2021                                   &laquo;        1        2        3        4        5        6        7        8        9       &raquo; "
    }, {
    "id": 4,
    "url": "/page2/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 5,
    "url": "/page3/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 6,
    "url": "/page4/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 7,
    "url": "/page5/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 8,
    "url": "/page6/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 9,
    "url": "/page7/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 10,
    "url": "/page8/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 11,
    "url": "/page9/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 12,
    "url": "/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 13,
    "url": "/2021/04/hotwire-reactive-rails-with-no-javascript/",
    "title": "Hotwire: 没有JavaScript的Reactive Rails",
    "body": "2021/04/24 - 本文已获得原作者（Vladimir Dementyev）和 Evil Martians 授权许可进行翻译。原文介绍了 Rails 的最新“魔法”：Hotwire。这也是 Vladimir Dementyev 在 RailsConf 2021 上的演讲内容。  原文链接：Hotwire: Reactive Rails with no JavaScript? 作者：Vladimir Dementyev 站点：Evil Martians ——位于纽约和俄罗斯的 Ruby on Rails 开发人员博客。 它发布了许多优秀的文章，并且是不少 gem 的赞助商。【正文如下】 引言: 到传播 DHH 及其公司久经考验的新魔法的时候了，并且在超过 5 分钟的教学中学习使用 Hotwire。自从今年揭开其面纱以来，这个用于构建现代 Web 界面而似乎无需任何 JavaScript 的技术的名字就备受欢迎。这个 HTML-over-the-wire 的方案正在 Rails 世界里激发起层层涟漪：不计其数的博客文章、reddit 社区帖子、录屏视频，以及今年 RailsConf 的五个演讲，而其中会包含你所期望的内容。本文中，我想要对 Hotwire 进行彻底地解释——借助代码示例和测试策略。就像我最爱的摇滚乐队说的那样，让我们 Hotwired 来……self destruct 学习新技巧吧！【译者注：摇滚乐队 Metallica 2016 年发行了专辑《Hardwired … To Self-Destruct》，作者在这儿使用了名称的谐音】 Life is short: 要概览 Hotwire 在 Rails 6 中进行使用的全貌，毋需再费其他功夫，看这个 PR 就足够了。 接下来的文章会解释上述 PR 代码——在很多细节上。它是我的 RailsConf 2021 演讲：“Frontendless Rails frontend” 的一个改编和扩展版本，所有 RailsConf 的参会者都已能够在线观看。如果你没有大会门票也不用担心：可以在这里看到其简报，而且该页面会更新演讲视频，一旦演讲可以公开发布的话。 “This is the way”: 过去五年中，我一直主要在做纯后端的开发：REST 和 GraphQL APIs、WebSocket、gRPC、数据库、缓存等。 整个前端的进化像巨浪一样席卷了我：我仍然不理解为什么我们需要为每个 Web 应用都使用 reacts 和 webpacks。传统的 HTML-first 的 Rails 方式 才是我的方式（或者说捷径😉）。还记得那些 JavaScript 在你的应用中无需什么 MVC（或 MVVM）的日子吗？我怀念那种日子。而这些日子正在悄悄地卷土重来。 今天，我们目睹了 HTML-over-the-wire 的崛起（是的，现在它是一个实际名词了）。由 Phoenix LiveView 率先提出，StimulusReflex 系列 gems 对其发扬光大，这种基于后端通过 WebSocket 把渲染的模板推送到所有所连接的客户端的方案，在 Rails 社区获得了极大的吸引力。最终，DHH 本人于今年初把 Hotwire 呈现于世界面前。 我们是否正站在 Web 开发的另一个全球范式转变的边缘？回到服务端渲染模板的简单思维模型，这一次花费很少的精力就可实现各种花里胡哨的反应式界面吗？绞尽脑汁后，我认识到这是一厢情愿的想法：技术领域已经有太多的投资在客户端渲染的应用上而很难回头了。2020 时代的前端开发已经是一种独立的资质和有其自身需求的独立行业，我们将没办法再成为“全栈”了。 然而，HOTWire（看到那个首字母缩写吗？Basecamp 这方面很聪明，是吧？），为复杂的，或者我们应该说“错综复杂的”，航天科学一般针对浏览器的现代客户端编程，提供了一种急需的替代方案。 对于厌倦了只做 API 应用而无法掌控其呈现，以及怀念创建卓越用户体验而摆脱每周 40 小时充斥着 SQL 和 JSON 的一名 Rails 开发者而言，Hotwire 就如他所渴望能带来新鲜气息的呼吸一般，让 Web 开发重拾乐趣。 本文中，我会演示如何把 HTML-over-the-wire 哲学通过 Hotwire 用到现有的 Rails 应用上。就像我最近的大多数文章一样，我会使用 AnyCable demo 应用作为小白鼠。 这个应用很应景：交互和反应，Turbolinks 驱动，以及少量自定义 JavaScripts，还有相当好的系统测试覆盖率（这意味着我们可以进行安全地重构）。我们的 Hotwire 化 将会按照如下步骤进行：  From Turbolinks to Turbo Drive Framing with Turbo Frames Streaming with Turbo Streams Beyond Turbo, or using Stimulus and custom elementsFrom Turbolinks to Turbo Drive: Turbolinks 在 Rails 世界里很长时间久负盛名；其第一个主要版本在 2013 年早期发布。然而，在我的开发生涯初期，Rails 开发者有一个经验之谈：如果你的前端出毛病了，尝试一下禁用Turbolinks。让第三方 JS 库的代码跟 Turbolinks 的伪导航（参考：pushState + AJAX）兼容可不像在公园里散步那样容易。 当 StimulusJS 出来以后，我就不再躲避 Turbolinks 了。它通过依靠现代的 DOM mutation APIs 而从根本上解决了“连接”和“断开连接” JavaScript 的问题。Turbolinks 与 Stimulus 的代码组合，DOM 操作仅以 React-Angular 几分之一的开发成本就轻而易举产生了“SPA”般的体验。 昔日诸般好处的 Turbolinks 现在更名为 Turbo Drive，就如字面上那样它驱动了 Turbo —— Hotwire 包的核心。 如果你的应用已经使用了 Turbolinks（如我一般），切换到 Turbo Drive 不费吹灰之力。不过是一些重命名的事儿罢了。 所有你需要做的就是把package. json中的turbolinks替换为@hotwired/turbo-rails，以及把Gemfile中的turbolinks替换为turbo-rails。 初始化代码稍有差异，现在的更简洁了： - import Turbolinks from 'turbolinks';- Turbolinks. start();+ import  @hotwired/turbo 注意，我们现在不需要手动启动 Turbo Drive 了（当然你可以不这么做）。 还有些“查找 &amp; 替换”工作要做：把所有 HTML 的 data 属性的data-turbolinks更新为data-turbo。 这些变更中唯一花费了我一点时间而值得一提的是处理 forms 和 redirects。之前使用 Turbolinks 时，我使用的是 remote forms（remote: true）和 Redirection concern 来响应以 JavaScript 模板。Turbo Drive 已经内置了对表单拦截的支持，所以remote: true就不再需要了。然而，事实证明 redirection 代码必须进行更新，或者更精确地说，是 redirection status code： - redirect_to workspace+ redirect_to workspace, status: :see_other使用有些晦涩的 See Other HTTP response code (303) 是一个聪明的选择：它允许 Turbo 依赖原生 Fetch API 的 redirect:  follow  选项，这样在表单提交后你就不必明确发起另一个请求以获取新内容。根据其规范，“if status is 303 and request’s method is not GET or HEAD”，GET 请求必须被自动执行。把这个跟 “if status is 301 or 302 and request’s method is POST” 比较一下——看到区别了吗？ 其他的 3xx 状态仅适用于 POST 请求，而 Rails 中我们通常使用 POST, PATCH, PUT, 和 DELETE。 Framing with Turbo Frames: 该来看一些真正的新东西了：Turbo Frames。 Turbo Frames 带来了页面在局部上的无缝更新（不像 Turbo Drive 是在整个页面上）。我们可以说它非常类似于&lt;iframe&gt;所做的，但却不用创建单独的 windows、DOM 树以及与之俱来的那些安全噩梦。 我们来看看实际的例子。 AnyCable demo 应用（称为 AnyWork）允许你创建 dashboards，其带多个 ToDo 列表和一个聊天室。用户可以与不同列表中的条目进行交互：添加、删除以及把其标注为已完成。 起初，完成和删除这些条目是通过 AJAX 请求和一个自定义 Stimulus controller 来做到的。我决定使用 Turbo Frames 来重写这部分功能以全部使用 HTML。 我们如何来解构这些 ToDo 列表项以处理单个条目的更新呢？把每个条目都转化为一个 frame！ &lt;!-- _item. html. rb --&gt;&lt;%= turbo_frame_tag dom_id(item) do %&gt; &lt;div class= any-list--item&lt;%= item. completed? ?   checked  :   %&gt; &gt;  &lt;%= form_for item do |f| %&gt;   &lt;!-- . . . --&gt;  &lt;% end %&gt;  &lt;%= button_to item_path(item), method: :delete %&gt;   &lt;!-- . . . --&gt;  &lt;% end %&gt; &lt;/div&gt;&lt;% end %&gt;这里我们做了三个重要的事情：  通过 helper 传递一个唯一识别符（来自ActionView的可爱的 dom_id 方法），把单个条目包裹在一个&lt;turbo-frame&gt; tag 之内； 添加一个 HTML form，使得 Turbo 拦截表单提交并更新该 frame 的内容；以及 使用button_to helper 并带 method: :delete 参数，在该代码处也创建了一个 HTML form。现在，任何时候该 frame 内有表单提交，Turbo 都会拦截该提交，执行 AJAX 请求，从响应返回的 HTML 中提取出有相同 ID 的 frame，把其内容替换到该 frame 上。 所有上述工作没有一行自己手写的 JavaScript！ 来看一下更新过后的 controller 代码： class ItemsController &lt; ApplicationController def update  item. update!(item_params)  render partial:  item , locals: { item } end def destroy  item. destroy!  render partial:  item , locals: { item } endend注意，当我们删除条目时，以同样的 partial 进行了响应。但我们需要移除该条目的 HTML 节点而非更新它。要如何做呢？我们可以用一个空 frame 来响应！更新 partial 为如下： &lt;!-- _item. html. rb --&gt;&lt;%= turbo_frame_tag dom_id(item) do %&gt; &lt;% unless item. destroyed? %&gt;  &lt;div class= any-list--item&lt;%= item. completed? ?   checked  :   %&gt; &gt;   &lt;!-- . . . --&gt;  &lt;/div&gt; &lt;% end %&gt;&lt;% end %&gt;你可能会问自己一个问题：“当标注一个条目为完成时如何触发表单提交呢？”换句话说，如何让 checkbox 的状态变更来触发提交表单？我们可以通过定义一个行内事件监听器做到： &lt;%= f. check_box :completed, onchange:  this. form. requestSubmit();  %&gt;提醒：使用 requestSubmit() 而非 submit() 很重要：前者触发的“submit”事件能够被 Turbo 所拦截，而后者不能。 总结一下，我们可以放弃所有专门为此功能定制的 JS 了，只需一点 HTML 模板的更改和 controller 代码的简化。非常令人兴奋，不是么？ 我们可以更进一步，把列表也转化为 frames。这会让我们在添加一个新条目时，从 Turbo Drive 的整个页面更新切换为仅特殊页面节点的更新。你大可自己尝试一下！ 假设你也期望在一个条目被完成或删除的任何时候都为用户展示一个 flash 提醒（比如，“条目已被成功删除”）。我们能借助于 Turbo Frames 做到吗？听起来我们需要把 flash 消息容器包裹在一个 frame 内，并将更新后的 HTML 跟标记一起推送给该条目。这是我初始的思路，但其并不能正常工作：frame 的更新是在所创建的 frame 定义域内的。因此，我们无法更新在其外部的任何东西。 经过一番探索之后，我发现 Turbo Streams 能帮我们做到这点。 Streaming with Turbo Streams: 较之于 Drive 和 Frames，Turbo Streams 完全是一项新技术。跟前两者不同，Streams 含义明确，易于理解。没有什么会自动发生，你得负责页面上何时更新何种内容。要做到这点，你需要使用特殊的&lt;trubo-stream&gt;元素。 来看一个 stream 元素的示例： &lt;turbo-stream action= replace  target= flash-alerts &gt; &lt;template&gt;  &lt;div class= flash-alerts--container  id= flash-alerts &gt;   &lt;!-- --&gt;  &lt;/div&gt; &lt;/template&gt;&lt;/turbo-stream&gt;该元素负责以&lt;template&gt; tag 内所传输过来的新 HTML 内容替换（action= replace ）DOM ID 为flash-alerts其下的节点。不管什么时候你把这样的&lt;turbo-stream&gt;元素下发到页面上，它都会立刻执行其 action 并销毁其自身。而在底层，它使用了 HTML 的 Custom Elements API —— 又一个为了开发乐趣（比如，更少的 JavaScript 😄）而使用现代 Web APIs 的范例。 我得说，Turbo Streams 是老式的 JavaScript 模板的一个声明式替代方案。在 2010 年代，我们写的代码类似这样： // destroy. js. erb$( #&lt;%= dom_id(item) %&gt; ). remove();而现在，我们这样写： &lt;!-- destroy. html. erb --&gt;&lt;%= turbo_stream. remove dom_id(item) %&gt;目前，仅有五种可用的 actions：append、prepend、replace、remove 和 update（仅替换节点的文本内容）。我们将在下面谈论其局限性和如何克服它。 回到我们的初始问题：为 ToDo 条目的完成或删除，展示响应结果中的 flash 提醒。 我们想要一次响应结果就带有&lt;turbo-frame&gt;和&lt;turbo-stream&gt;两种更新。如何来做？为其添加一个新的 partial 模板： &lt;!-- _item_update. html. erb --&gt;&lt;%= render item %&gt;&lt;%= turbo_stream. replace  flash-alerts  do %&gt; &lt;%= render  shared/alerts  %&gt;&lt;% end %&gt;对ItemsController添加一点小的改动： +  flash. now[:notice] =  Item has been updated -  render partial:  item , locals: { item }+  render partial:  item_update , locals: { item }不幸地是，上述代码并未如预期那样正常工作：我们没有看到任何 flash 提醒。 深入研究文档之后，我发现是 Turbo 期望 HTTP 响应具有text/vnd. turbo-stream. html content type 才可激活 stream 元素。好吧，加上它： -  render partial:  item_update , locals: { item }+  render partial:  item_update , locals: { item }, content_type:  text/vnd. turbo-stream. html 现在我们得到了相反的情况：flash 消息正常工作，但条目的内容不能更新了😞。是我对 Hotwire 要求太高了么？阅读了下 Turbo 的源码，我发现类似这样把 streams 和 frames 进行混合是不行的。 这说明，有两种方式来实现该功能：  把 streams 用在所有东西上。 把&lt;turbo-stream&gt;置于&lt;turbo-frame&gt;内部。第二个选项，对我而言，与在常规页面上重用 HTML partials 并以 Turbo 进行更新的想法背道而驰。所以，我选择第一个： &lt;!-- _item_update. html. erb --&gt;&lt;%= turbo_stream. replace dom_id(item) do %&gt; &lt;%= render item %&gt;&lt;% end %&gt;&lt;%= turbo_stream. replace  flash-alerts  do %&gt; &lt;%= render  shared/alerts  %&gt;&lt;% end %&gt;任务完成。但付出了什么代价呢？我们不得不为这种用例添加一个新的模板。并且我担心在现实中的应用程序里，这种 partials 的数量会随着应用的进化而增长。 更新（2021-04-13）：Alex Takitani 建议了一种更加优雅的解决方案：使用 layout 来更新 flash 内容。我们可以如下面这样把 application layout 定义为 Turbo Stream 响应： &lt;!-- layouts/application. turbo_stream. erb --&gt;&lt;%= turbo_stream. replace  flash-alerts  do %&gt; &lt;%= render  shared/alerts  %&gt;&lt;% end %&gt;&lt;%= yield %&gt;然后，我们需要从 controller 移除相应的渲染（因为要不然 layout 就不会被用上了）： def update   item. update!(item_params)   flash. now[:notice] =  Item has been updated --  render partial:  item_update , locals: { item }, content_type:  text/vnd. turbo-stream. html   end注意：别忘了把format: :turbo_stream添加到 controller/request specs 测试相应的请求上，以便使得 render 能正常工作。 并且把我们的_item_update partial 转换为update的 Turbo Stream 模板： &lt;!-- update. turbo_stream. erb --&gt;&lt;%= turbo_stream. replace dom_id(item) do %&gt; &lt;%= render item %&gt;&lt;% end %&gt;很酷，对吧？这正是 Rails 的方式！ 现在，让我们转到一些实时的流广播上。 Turbo Streams 经常在实时更新的语境中被提到（且常常被用来跟 StimulusReflex 比较）。 来看看我们能够如何在 Turbo Streams 之上构建列表的同步化： 在有 Turbo 之前，我不得不添加一个自定义的 Action Cable channel 和一个 Stimulus controller 来处理广播的事情。我也需要处理消息的格式，因为必须区分对条目的删除和完成。换句话说，有不少代码要维护。 而 Turbo Streams 已经照顾好了几乎一切：turbo-rails gem 自带一个通用的Turbo::StreamChannel和一个 helper（#turbo_stream_from），用来从 HTML 中创建一个 subscription： &lt;!-- worspaces/show. html. erb --&gt;&lt;div&gt; &lt;%= turbo_stream_from workspace %&gt; &lt;!-- . . . --&gt;&lt;/div&gt;在 controller 中，我们已经有了#broadcast_new_item和#broadcast_changes这样的“after action” callback 负责对更新进行播发。现在我们所有需要做的就是切换到Turbo::StreamChannel： def broadcast_changes  return if item. errors. any?  if item. destroyed?-  ListChannel. broadcast_to list, type:  deleted , id: item. id+  Turbo::StreamsChannel. broadcast_remove_to workspace, target: item  else-  ListChannel. broadcast_to list, type:  updated , id: item. id, desc: item. desc, completed: item. completed+  Turbo::StreamsChannel. broadcast_replace_to workspace, target: item, partial:  items/item , locals: { item }  end end这次迁移很顺畅，几乎——因为所有检验播发（#have_broadcasted_to）的 controller 单元测试都失败了。 不幸的是，Turbo Rails 没有提供任何测试工具（?），所以我不得不自己写一个，以自己熟悉的方式： module Turbo::HaveBroadcastedToTurboMatcher include Turbo::Streams::StreamName def have_broadcasted_turbo_stream_to(*streamables, action:, target:) # rubocop:disable Naming/PredicateName  target = target. respond_to?(:to_key) ? ActionView::RecordIdentifier. dom_id(target) : target  have_broadcasted_to(stream_name_from(streamables))   . with(a_string_matching(%(turbo-stream action= #{action}  target= #{target} ))) endendRSpec. configure do |config| config. include Turbo::HaveBroadcastedToTurboMatcherend下面是我如何把这个新的匹配器用在测试上： it  broadcasts a deleted message  do- expect { subject }. to have_broadcasted_to(ListChannel. broadcasting_for(list))-  . with(type:  deleted , id: item. id)+ expect { subject }. to have_broadcasted_turbo_stream_to(+  workspace, action: :remove, target: item+ ) end到目前为止，使用 Turbo 的实时处理进展顺利！一大堆代码都被移除了。 而我们仍然还是一行 JavaScript 代码都没有写。这也太不真实了吧？ 不过是个幻梦吗？何时我会醒来？好吧，就是现在。 Beyond Turbo, or using Stimulus and custom elements: 在向 Turbo 迁移的过程中，我碰到了好几个场景，使用已有的 API 是不够的，所以我最终不得不编写一些 JavaScript 代码！ 场景一：向 dashboard 实时添加新的列表。这跟前面提到的列表中条目的示例有何不同？在于标记。来看一下 dashboard layout： &lt;div id= workspace_1 &gt; &lt;div id= list_1 &gt;. . . &lt;/div&gt; &lt;div id= list_2 &gt;. . . &lt;/div&gt; &lt;div id= new_list &gt;  &lt;form&gt;. . . &lt;/form&gt; &lt;/div&gt;&lt;/div&gt;最后一个元素总是新列表的 form 容器。不管我们何时添加新列表，它都会被插入到#new_list节点之前。还记得 Turbo Streams 仅仅支持五种 actions 不？明白问题所在了吗？下面是我起初使用的代码： handleUpdate(data) { this. formTarget. insertAdjacentHTML( beforebegin , data. html);}要使用 Turbo Streams 实现类似的行为，我们需要添加一个 hack，在列表被通过 stream 添加之后立即把其移动到正确的位置。所以，来添加我们自己的 JavaScript 代码吧。 首先来给我们的任务一个正式的定义：“当一个新列表被 append 到 workspace 容器时，它应该出现在那个 new form 元素之前的正确位置上。”。这里的“当”意味着我们需要观察 DOM 并对变更作出反应。是不是听起来很熟悉？没错，我们已经提到过与 Stimulus 有关的 MutationObserver API！用它就对了。 幸运的是，我们不是必须编写高阶的 JavaScript 才能使用该特性；我们可以使用 stimulus-use（抱歉使用这种重言式语法。【译者注：原文是 use stimulus-use，所以作者这么说】）。Stimulus Use 是一个 Stimulus controllers 很有用的行为的集合，以简单的代码片段解决复杂的问题。我们这儿，需要 useMutation 行为。 如下的 controller 代码相当简洁，含义不言自明： import { Controller } from  stimulus ;import { useMutation } from  stimulus-use ;export default class extends Controller { static targets = [ lists ,  newForm ]; connect() {  [this. observeLists, this. unobserveLists] = useMutation(this, {   element: this. listsTarget,   childList: true,  }); } mutate(entries) {  // There should be only one entry in case of adding a new list via streams  const entry = entries[0];  if (!entry. addedNodes. length) return;  // Disable observer while we modify the childList  this. unobserveLists();  // Move newForm to the end of the childList  this. listsTarget. append(this. newFormTarget);  this. observeLists(); }}问题就这样解决了。 来讨论下第二个边界场景：实现聊天室功能。 我们有一个非常简单的聊天室附在每个 dashboard 上：用户可以发送临时消息（不会被存储到任何地方）和实时接收它们。消息具有依赖于上下文的不同外观：自己的消息有绿色边框，靠左；其他消息则是灰色，靠右。而我们是向每个所连接的客户端播发相同的 HTML。要如何使得用户看到这种区别呢？这对于聊天室类的应用是一个很常见的问题，且一般而言，它通过要么向每个用户 channel 发送个性化的 HTML，要么增强所收到的 HTML 在客户端来解决。我更喜欢第二种，所以来实现它吧。 要把当前用户的信息传递给 JavaScript，我使用 meta tags： &lt;!-- layouts/application. html. erb --&gt;&lt;head&gt; &lt;% if logged_in? %&gt;  &lt;meta name= current-user-name  content= &lt;%= current_user. name %&gt;  data-turbo-track= reload &gt;  &lt;meta name= current-user-id  content= &lt;%= current_user. id %&gt;  data-turbo-track= reload &gt; &lt;% end %&gt; &lt;!-- . . . --&gt;&lt;/head&gt;和一个小的 JS helper 来获取这些 values： let user;export const currentUser = () =&gt; { if (user) return user; const id = getMeta( id ); const name = getMeta( name ); user = { id, name }; return user;};function getMeta(name) { const element = document. head. querySelector(  `meta[name='current-user-${name}']` ); if (element) {  return element. getAttribute( content ); }}要播发聊天室消息，我们将会使用Turbo::StreamChannel： def create Turbo::StreamsChannel. broadcast_append_to(  workspace,  target: ActionView::RecordIdentifier. dom_id(workspace, :chat_messages),  partial:  chats/message ,  locals: { message: params[:message], name: current_user. name, user_id: current_user. id } ) # . . . end下面是初始的chat/message模板： &lt;div class= chat--msg &gt; &lt;%= message %&gt; &lt;span data-role= author  class= chat--msg--author &gt;&lt;%= name %&gt;&lt;/span&gt;&lt;/div&gt;以及前述根据当前用户赋予不同样式的 JS 代码，这些代码我们很快就要去掉： // Don't get attached to itappendMessage(html, mine) { this. messagesTarget. insertAdjacentHTML( beforeend , html); const el = this. messagesTarget. lastElementChild; el. classList. add(mine ?  mine  :  theirs ); if (mine) {  const authorElement = el. querySelector('[data-role= author ]');  if (authorElement) authorElement. innerText =  You ; }}现在，当 Turbo 负责更新 HTML 时，我们需要做点不同的事。当然，useMutaion也会在这里被用到。并且这有可能是我将用在现实项目上的。然而，我今天的目标是演示以不同的方式来解决问题。 还记得我们一直在谈论 Custom Elements（哦，那是好几页之前了，抱歉，这说明我们阅读太久了）？它正是令 Turbo 之所以强大的 Web API。我们干嘛不用呢！ 让我先分享一个更新后的 HTML 模板： &lt;any-chat-message class= chat--msg  data-author-id= &lt;%= user_id %&gt;&gt; &lt;%= message %&gt; &lt;span data-role= author  class= chat--msg--author &gt;&lt;%= name %&gt;&lt;/span&gt;&lt;/any-chat-message&gt;我们只添加了data-author-id属性，并把&lt;div&gt;替换为自定义 tag ——&lt;any-chat-message&gt;。 现在来对 custom element 进行注册： import { currentUser } from  . . /utils/current_user ;// This is how you create custom HTML elements with a modern APIexport class ChatMessageElement extends HTMLElement { connectedCallback() {  const mine = currentUser(). id == this. dataset. authorId;  this. classList. add(mine ?  mine  :  theirs );  const authorElement = this. querySelector('[data-role= author ]');  if (authorElement &amp;&amp; mine) authorElement. innerText =  You ; }}customElements. define( any-chat-message , ChatMessageElement);大功告成！现在当一个新的&lt;any-chat-message&gt;元素被添加到页面时，如果它来自于当前用户就自动更新自己。而且甚至我们为此都不再需要 Stimulus 了！ 你可以在这个 PR 中找到本文有关的全部源代码。 所以，那么零 JavaScript 的 Reactive Rails 到底存在吗？并不。我们移除了很多 JS 代码，但最后不得不用一些新东西来替代。这些新东西跟之前的有所区别：它更加，我得说，实用主义。它也更加高阶，需要对 JavaScript 以及最新浏览器 APIs 有很好的了解，这肯定是要权衡考虑的。 附：我对 CableReady 和 StimulusReflex 也有一个类似的 PR。你可以把它跟 Hotwire 的这个 PR 进行比较，在 Twitter 上与我们分享你的观点。 "
    }, {
    "id": 14,
    "url": "/2021/04/the-foundation-of-how-tailwindcss-works/",
    "title": "Tailwindcss底层基石的理念",
    "body": "2021/04/10 - Tailwindcss 从 2019 年开始逐渐在国外的 Web 开发圈子内盛行起来。国内倒是至今仍然不温不火。2020 年的 Ruby China 上过纯中做过一次在项目上使用 tailwindcss 体验的有关演讲。我在 2020 年也写过一篇有关的博客“在 Rails 6 中整合 Stimulus 和 Tailwind CSS”（被前者在演讲中所引用^_^）。 这篇文章来聊一下有关 Tailwindcss 底层理念的东西。了解了这个，再来看它就不觉得毫无头绪而畏惧了。 在 Tailwindcss 中，“工具类为一等公民（utility classes are king）”。这种工具类在 Bootstrap 中早已为人熟知，并非什么新鲜事物，比如text-muted,bg-success。但 Tailwindcss 又有所不同，正是这些不同使得它更“Tailwind”。 大部分时间内，在使用 Tailwindcss 时都面对两类对象。一个是 theme，通用性配置。另一个是一大堆的 property 列表，可把它们用在 theme 上。theme 有一系列的细粒度对象，提供了超多的可配置项。 比如说，在开始一个 theme 的设计时，会从这三种配置开始：screen、color、spacing。首先来看 color 和 spacing：  Color 从 0 开始，一直到 900。越接近 900，color 就越暗，反之则越亮。 Spacing 是 space 之数字，或对象之大小，在 0 - 96 之间。还有上面所提到的那些 property，比如margin,width,padding等。 只要有了这两种“变量”，你就可以把它们如此组合起来：{property}-{color/spacing}。如果想要一个暗红色的背景，就写bg-red-900，而如果想要 6 个像素的 padding，就写p-6。 再看 screen。Screen 用于响应式的 breakpoint，如 desktop / tablet / mobile 的设计上（sm,md,lg,xl）。有了 screen 之后，上面的公式就变为{screen}:{property}-{color/spacing}。 用一个完整的示例来演示。 我想要一个有暗红色背景，白色文字，以及 padding 为 6 的 DIV： &lt;div class= bg-red-900 text-white p-6 &gt;Messi&lt;/div&gt;接下来可以再添加一些别的工具类。想要把这个 DIV 在浏览器内居中，可用mx-auto。想要让其宽度自动扩展，可用w-auto。最后，想要让其文字居中，显然，用text-center即可： &lt;div class= bg-red-900 p-6 mx-auto w-auto       text-white text-center  &gt;Messi&lt;/div&gt;这样，在 desktop 上看起来已经很好了。不过，我想在 mobile 上 padding 要更小一点。此时，就是 screen 发挥作用的时候了。适配不同的 breakpoint 和 property 能带来很多好处。假设默认 padding 为 3，而从 phone 过渡到 tablet 的尺寸，则使其 padding 变为 6： &lt;div class= bg-red-900 p-3 md:p-6 mx-auto       w-auto text-white text-center &gt;Messi&lt;/div&gt;最后，再针对各 breakpoint 分别适配以适当的文字： &lt;div class= bg-red-900 text-center p-3       mx-auto w-auto text-white text-center       text-md md:text-lg lg:text-2xl  &gt;Messi&lt;/div&gt;Tailwindcss 还有很多其他类似的工具类， 它的官方文档中都有详细讲述。 这一切看起来是“过度设计”了，然而 Tailwindcss 的闪光之处，就是“组件驱动设计（component driven design）”。把这些工具类用于组件之上，然后组件用于系统各处。多个组件则可构成组件库。组件库即类似于 Bootstrap 里的 modal 之类。 一些相关的 Tailwindcss component 链接:  https://tailwindui. com/ Tailwindcss 官方的 UI kit 站点 https://tailwindcomponents. com/ 一个社区类 components 集合站点 https://tailwind. build/editor 一个 Tailwindcss component builder"
    }, {
    "id": 15,
    "url": "/2021/03/hotwire-build-turbo-application/",
    "title": "Hotwire之构建Turbo应用",
    "body": "2021/03/26 - 本文是对构建 Turbo 应用的具体描述，原文出自：https://turbo. hotwire. dev/handbook/building。 Turbo 之所以快是因为当你点击链接或者提交表单时它防止了整个页面的重新加载。你的应用成为浏览器中常驻而不停运转的进程。这就需要你重新考虑组织你的 JavaScript 的方式。 特别是，你可以不再依赖于页面的每次导航时整个页面的载入来重置你的环境。JavaScript 的window和document对象在页面变更期间保持其状态，而任何其他你放入内存的对象将会留在内存中。 有了这个意识，并稍微小心一点，你就能对应用进行设计以优雅地处理这种约束，而无需与 Turbo 紧密耦合在一起。 Working with Script Elements: 浏览器会自动载入并执行任何初始页面加载的&lt;script&gt;元素。 当你导航到一个新页面时，Turbo Drive 就查找在新页面的&lt;head&gt;中的任何&lt;script&gt;元素，且其未出现于当前页面上。然后，Turbo Drive 把它们 append 到当前的&lt;head&gt;，浏览器对其进行加载和执行。你可以由此来按需加载额外的 JavaScript 文件。 Turbo Drive 在每次渲染页面时执行该页面&lt;body&gt;中的&lt;script&gt;元素。你可以使用 inline body script 来建立每个页面的 JavaScript 状态或者启动客户端的 model。要建立某些行为，或在页面变更时执行更复杂的操作，避免使用 script 元素，使用turbo:load事件来代替。 如果你不想 Turbo 在页面渲染后执行&lt;script&gt;元素，就使用data-turbo-eval= false 对其进行注解。注意，这种注解不会防止浏览器在初始页面加载时执行这些 scripts。 Loading Your Application’s JavaScript Bundle: 要总是确保在&lt;head&gt;中使用&lt;script&gt;元素来加载你应用的 JavaScript 打包文件。否则，Turbo Drive 将会在每次页面变更时都重载它。 &lt;head&gt; . . .  &lt;script src= /application-cbd3cd4. js  defer&gt;&lt;/script&gt;&lt;/head&gt;你也应该考虑把 asset 打包系统配置成给每个 script 加上指纹，这样当其内容变动时就会是一个新的 URL。然后，你可以使用data-turbo-track属性来强制当部署了新 JavaScript 打包文件之后做一次全页面重载。参看 Assets 变更后重载 可获得更多信息。 Understanding Caching: Turbo Drive 维护着近期访问页面的一个缓存。该缓存有两个目的：在 restoration 访问期间展示页面无需访问网络，和在 application 访问期间通过显示临时性的预览来提升可感知的性能。 当通过 history 导航（以 Restoration 访问），只要可能，Turbo Drive 将从缓存恢复页面而无需从网络加载一个全新的拷贝。 否则，在标准导航期间（以 Application 访问），Turbo Drive 将从缓存中立即恢复页面作为一个预览，并同时从网络加载一个全新拷贝。这就给经常访问的位置带来了瞬时页面加载的错觉。 Turbo Drive 在渲染新页面之前，把当前页面的一个拷贝保存到其缓存中。注意，Turbo Drive 使用了cloneNode(true) 来拷贝页面，这意味着任何 attach 的事件监听及关联数据都被丢弃掉。 Preparing the Page to be Cached: 如果你需要在 Turbo Drive 缓存对其之前准备 document，可以监听turbo-cache事件。使用该事件来重置表单、收起所展开的 UI 元素、或清理任何第三方的 widgets，以便准备好重新显示该页面。 document. addEventListener( turbo:before-cache , function() { // . . . })Detecting When a Preview is Visible: 当 Turbo Drive 从缓存中展示一个预览时，它向&lt;html&gt;元素上添加了一个turbo-preview属性。当一个预览可见的时候，你就可以检查该属性的存在而有选择地启用或禁用行为。 if (document. documentElement. hasAttribute( data-turbo-preview )) { // Turbo Drive is displaying a preview}Opting Out of Caching: 您可以通过在页面的&lt;head&gt;中包含&lt;meta name = turbo-cache-control &gt;元素并声明一个缓存指令来控制每个页面的缓存行为。 使用no-preview指令来指定在 application 访问期间，页面的一个缓存版本不作为预览被显示。被标注了 no-preview 的页面将只被用于 restoration 访问。 要指定一个页面完全不应该被缓存，使用no-cache指令。被标注了 no-cache 的页面将总是通过网络获取，包括在 restoration 访问期间。 &lt;head&gt; . . .  &lt;meta name= turbo-cache-control  content= no-cache &gt;&lt;/head&gt;要完全禁用应用中的缓存，就要确保每个页面都包含 no-cache 指令。 Installing JavaScript Behavior: 你可能习惯于加入 JavaScript 行为来响应window. onload，DOMContentLoaded，或者 JQuery 的ready事件。使用 Turbo 时，这些事件将只在初始页面加载时被触发，之后的任何后续页面变更都不会触发。我们来比较下把 JavaScript 行为连接到 DOM 的两种策略。 Observing Navigation Events: Turbo Drive 在导航期间触发一系列事件。其中最重要的是turbo:load事件，其只在初始页面加载时触发一次，并在每次 Turbo Drive 访问时触发一次。 你可以在DOMContentLoaded内监听turbo:load事件来在每次页面变更时建立 JavaScript 行为： document. addEventListener( turbo:load , function() { // . . . })记住，当该事件被触发时，你的应用不会总是在一个崭新的状态，而你就可能需要清理前一个页面所建立的行为。 也要注意，Turbo Drive 的导航可能不是你应用中的页面更新的唯一源头，所以你可能期望把你的初始化代码移到一个单独的函数内，你可以从turbo:load和任何其他可能变更 DOM 的地方来调用它 可能的话，避免使用turbo:load事件来把其他事件监听器直接添加到页面 body 的元素上。相反，考虑使用事件委托来把事件监听器注册到document或window仅一次。 参看全部事件列表获取更多信息。 Attaching Behavior With Stimulus: 新 DOM 元素可以在任何时刻呈现在页面上，通过 frame 导航、stream 信息、或客户端渲染操作等方式，而这些元素经常需要如同出现于一个全新页面加载那样被初始化。 对于所有这些更新，包括从 Turbo Drive 页面加载的更新，你可以借助于 Turbo 的兄弟框架 Stimulus 所提供的约定和回调生命周期，在一个单独的地方来处理它们。 Stimulus 让你使用 controller、action 和 target 属性对 HTML 进行注释： &lt;div data-controller= hello &gt; &lt;input data-hello-target= name  type= text &gt; &lt;button data-action= click-&gt;hello#greet &gt;Greet&lt;/button&gt;&lt;/div&gt;实现其相应的 controller 并由 Stimulus 自动连接到它： // hello_controller. jsimport { Controller } from  stimulus export default class extends Controller { greet() {  console. log(`Hello, ${this. name}!`) } get name() {  return this. targets. find( name ). value }}Stimulus 使用 MutationObserver API，只要 document 发生变更，这些 controllers 和它们相关联的事件处理器就会被连接及断开连接。结果就是，它会处理 Turbo Drive 页面变更、Turbo Frames 导航、以及 Turbo Streams 信息，以它处理其他类型 DOM 更新完全同样的方式。 Making Transformations Idempotent: 你经常会想要在客户端对接收自服务端的 HTML 执行变换。例如，你可能想利用浏览器对用户当前时区的了解来按日期对元素集合进行分组。 假设你已经对一组元素注释以data-timestamp属性来指示其 UTC 创建时间。你有一个 JavaScript 函数来查询 document 中所有这样的元素，把时间戳转换为本地时间，并在每个发生于新的一天的元素之前插入日期表头。 考虑下如果你已经配置好该函数在turbo:load上执行的话会发生什么。当你导航到该页面，你的函数插入了日期表头。再离开该页面，则 Turbo Drive 把一个变换后页面的拷贝存入缓存。现在按下浏览器回退按钮——Turbo Drive 恢复页面，再次触发turbo:load，则你的函数插入第二个日期表头。 为了避免这个问题，就要让你的转换函数是幂等的。幂等转换能安全地执行多次而不会改变其最初应用的结果。 使转换幂等的一种技术是通过在每个处理的元素上设置一个data属性来跟踪是否已经执行了变换。当 Turbo Drive 从缓存中恢复页面时，这些属性会仍然存在。在你的函数中检测这些属性以确定哪个元素已经被处理过了。 更健壮的一种技术是简单地检测转换自身。在上面日期分组的示例中，这意味着在插入一个新日期分割线之前检查其存在与否。这个方案优雅地处理了未被初始转换所处理的新插入元素。 Persisting Elements Across Page Loads: Turbo Drive 允许你把某些元素标注为 permanent。Permanent 元素在页面加载之间都存留着，所以你对这些元素的任何变更都不需要在页面导航之后再次赋予。 考虑下一个带购物车的 Turbo Drive 应用。在每个页面顶部都是一个图标，有当前在购物车内的商品数量。这个计数器是借助 JavaScript 动态更新的，当商品被添加和移除时。 现在想象一下，一个用户在该应用中已经导航到过多个页面。她添加一个商品到购物车，然后按下浏览器回退按钮。Turbo Drive 从缓存中恢复了前一个页面的状态，而购物车的商品计数被错误地从 1 变为 0。 你可以通过使计数器元素 permanent 来避免这个问题。要制定元素为 permanent，赋予它们一个 HTML id，并以data-turbo-permanent来注释它们。 &lt;div id= cart-counter  data-turbo-permanent&gt;1 item&lt;/div&gt;每次渲染之前，Turbo Drive 通过 ID 匹配到所有的 permanent 元素，并把它们从初始页面转换到新页面，保留其数据和事件监听器。 "
    }, {
    "id": 16,
    "url": "/2021/03/hotwire-turbo-streams/",
    "title": "Hotwire之使用Turbo Streams焕发活力",
    "body": "2021/03/19 - 本文是对 Turbo Streams 的详细说明，原文出自：https://turbo. hotwire. dev/handbook/streams。 Turbo Streams 将页面的更改发布为包在自解释的&lt;turbo-stream&gt;元素中的 HTML 片段。每个 stream 元素都会同时指定一个 action 和 target ID，以声明其内的 HTML 会怎样处理。这些元素被服务端通过 WebSocket、SSE 或其他传输方式发布，借助由其他用户或进程进行的更新使应用程序焕发活力。抵达你的 imbox 的新邮件就是一个绝佳的范例。 Stream Messages and Actions: 一个 Turbo Streams 消息是一个由&lt;turbo-stream&gt;元素组成的 HTML 片段。下面的 stream 消息演示了五种可用的 stream actions： &lt;turbo-stream action= append  target= messages &gt; &lt;template&gt;  &lt;div id= message_1 &gt;   This div will be appended to the element with the DOM ID  messages .   &lt;/div&gt; &lt;/template&gt;&lt;/turbo-stream&gt;&lt;turbo-stream action= prepend  target= messages &gt; &lt;template&gt;  &lt;div id= message_1 &gt;   This div will be prepended to the element with the DOM ID  messages .   &lt;/div&gt; &lt;/template&gt;&lt;/turbo-stream&gt;&lt;turbo-stream action= replace  target= message_1 &gt; &lt;template&gt;  &lt;div id= message_1 &gt;   This div will replace the existing element with the DOM ID  message_1 .   &lt;/div&gt; &lt;/template&gt;&lt;/turbo-stream&gt;&lt;turbo-stream action= update  target= unread_count &gt; &lt;template&gt;  &lt;!-- The contents of this template will replace the  contents of the element with ID  unread_count . --&gt;  1 &lt;/template&gt;&lt;/turbo-stream&gt;&lt;turbo-stream action= remove  target= message_1 &gt; &lt;!-- The element with DOM ID  message_1  will be removed.  The contents of this stream element are ignored. --&gt;&lt;/turbo-stream&gt;注意，每个&lt;turbo-stream&gt;元素都必须把它内含的 HTML 包裹在一个&lt;template&gt;元素之内。 你可以在一个单独的 stream 消息中渲染任意数量的 stream 元素，该消息来自于 WebSocket、SSE 或者是一个表单提交后的响应。 Streaming From HTTP Responses: Turbo 知道自动附带上那些&lt;turbo-stream&gt;元素，当它们以&lt;form&gt;提交的响应返回并声明了一个text/vnd. turbo-stream. html的 MIME type 时。当提交其 method 属性被设置为POST、PUT、PATCH或DELETE 的 &lt;form&gt;元素时，Turbo 就会把 text/vnd. turbo-stream. html 注入到请求的 Accept header 中的响应格式集之内。在对其 Accept header 包含了这些值的请求进行响应时，服务端就可以调整相应以处理 Turbo Streams，HTTP 重定向，或者不支持 streams 的其他类型的客户端（比如原生应用）。 在一个 Rails controller 中，看起来会是这样的： def destroy @message = Message. find(params[:id]) @message. destroy respond_to do |format|  format. turbo_stream { render turbo_stream: turbo_stream. remove(@message) }  format. html     { redirect_to messages_url } endendReusing Server-Side Templates: Turbo Streams 的关键是重用你现有的服务端模板以执行实时的部分页面更改的能力。在页面首次加载时用来渲染消息列表中每一条消息的 HTML 模板，跟用来在之后动态添加到列表的一条新消息的模板，会是相同的。这就是 HTML-over-the-wire 方案的本质和精华：你不需要再把新消息序列化为 JSON，在 JavaScript 中接收它，并渲染一个客户端的模板。它就是标准的可重用的服务端模板。 另一个 Rails 中的例子看起来是这样的： &lt;!-- app/views/messages/_message. html. erb --&gt;&lt;div id= &lt;%= dom_id message %&gt; &gt; &lt;%= message. content %&gt;&lt;/div&gt;&lt;!-- app/views/messages/index. html. erb --&gt;&lt;h1&gt;All the messages&lt;/h1&gt;&lt;%= render partial:  messages/message , collection: @messages %&gt;# app/controllers/messages_controller. rbclass MessagesController &lt; ApplicationController def index  @messages = Message. all end def create  message = Message. create!(params. require(:message). permit(:content))  respond_to do |format|   format. turbo_stream do    render turbo_stream: turbo_stream. append(:messages, partial:  messages/message ,     locals: { message: message })   end   format. html { redirect_to messages_url }  end endend当创建一条新消息的表单提交给MessagesController#create action 时，跟MessagesController#index中用来渲染消息列表完全同样的 partial 模板被用来渲染 turbo-stream action。这将作为如下所示的响应出现： Content-Type: text/vnd. turbo-stream. html; charset=utf-8&lt;turbo-stream action= append  target= messages &gt; &lt;template&gt;  &lt;div id= message_1 &gt;   The content of the message.   &lt;/div&gt; &lt;/template&gt;&lt;/turbo-stream&gt;这个messages/message partial 模板然后可以被用来渲染后续 edit/update 操作的消息，或者支持由其他用户通过 WebSocket 或 SSE 连接所创建的新消息。能够在整个使用范围内重用相同的模板非常强大，这是减少创建这些现代，快速应用程序所需工作量的关键。 Progressively Enhance When Necessary: 一开始你的交互设计不借助 Turbo Streams 是一种好的实践。当 Turbo Streams 不可用时，让整个应用程序如预期那样运行，然后将它们分层升级。这意味着你将不必依赖更新来处理那些需要在原生应用或其他没有它们的地方都能正常工作的流程。 对于 WebSocket 更新也是同样的。在连接不好，或有服务端问题，你的 WebSocket 可能会中断。如果应用程序被设计为不借助于它而正常工作，这就更有适应性。 But What About Running JavaScript?: Turbo Streams 有意识地把所能执行的 actions 限制为五种：append、prepend、replace、update 和 remove。如果你在执行这些 actions 时想要触发额外的行为，那你应该使用 Stimulus controller 来附加这些行为。这种限制让 Turbo Streams 专注于通过网络发布 HTML 的本质任务，把额外的逻辑留给专门的 JavaScript 文件。 拥抱这些约束将使你避免将单个响应转变为无法重复使用的行为，从而使应用程序难以遵循。得自于 Turbo Streams 的关键受益是重用初始化渲染页面的模板的能力，贯穿于所有后续更新的过程中。 Integration with Server-Side Frameworks: 在 Turbo 附带的所有技术中，与 Turbo Streams 一起使用，你将看到与后端框架的紧密集成所带来的最大优势。作为官方 Hotwire 套件的一员，我们已经创建了这种集成看起来如何的一个实现参考，即 turbo-rails gem。该 gem 依赖于 Rails 中内置的 WebSocket 和 异步渲染的支持，其分别通过 Action Cable 和 Active Job 框架。 使用加入 Active Record 其中的 Broadcastable concern，你就可以直接触发来自 领域模型（domain model）的 WebSocket 更新。而使用 Turbo::Streams::TagBuilder，你就可以渲染在 inline controller 响应或专门模板中的&lt;turbo-stream&gt;元素，通过一个简单的 DSL 执行那五种 actions 以及相关的渲染。 但是，Turbo 本身是完全与后端无关的。因此我们鼓励其他生态圈中的其他框架来看看针对 Rails 提供的实现参考，以创建它们自己的紧密整合。 另外，将任何后端应用程序与 Turbo Streams 集成的直接方法是依靠 Mercure 协议。Mercure 通过 Server-Sent Events (SSE)，为服务端把页面变更播发到每个所连接的客户端提供了一种方便的方式。这里可以学习如何 Turbo Streams 是如何跟 Mercure 一起使用的。 "
    }, {
    "id": 17,
    "url": "/2021/03/hotwire-turbo-frame/",
    "title": "Hotwire之使用Turbo Frame解构页面",
    "body": "2021/03/16 - 本文是对 Turbo Frame 的详细说明，原文出自：https://turbo. hotwire. dev/handbook/frames。 Turbo Frames 允许你预定义页面中那些根据需要来更新的部分。任何 frame 内的链接和表单都会被捕获，而 frame 的内容会在接收到响应后被自动更新。不管服务端提供的是整个 document，还是仅包含所请求 frame 的更新版本的片段，都只有那个特定的 frame 会从响应中被提取出来以替代现有的内容。 把页面的某一部分包在&lt;turbo-frame&gt;元素中就创建了 Frames 。每个元素必须有一个唯一 ID，用来从服务端请求行页面时匹配要被替换的内容。一个单独页面可以有多个 frames，每一个都确立其自己的上下文： &lt;body&gt; &lt;div id= navigation &gt;Links targeting the entire page&lt;/div&gt; &lt;turbo-frame id= message_1 &gt;  &lt;h1&gt;My message title&lt;/h1&gt;  &lt;p&gt;My message content&lt;/p&gt;  &lt;a href= /messages/1/edit &gt;Edit this message&lt;/a&gt; &lt;/turbo-frame&gt; &lt;turbo-frame id= comments &gt;  &lt;div id= comment_1 &gt;One comment&lt;/div&gt;  &lt;div id= comment_2 &gt;Two comments&lt;/div&gt;  &lt;form action= /messages/comments &gt;. . . &lt;/form&gt; &lt;/turbo-frame&gt;&lt;/body&gt;这个页面有两个 frames：一个展示消息，带一个编辑链接；一个列出所有评论，带一个表单以添加评论。每个 frame 都创建了其自身的上下文，来捕获其中的链接和表单提交。 当编辑消息的链接被点击时，由/messages/1/edit提供的响应会把它的&lt;trubo-frame id= message_1 &gt;部分提取出来，而其内容则会替换到链接点击所在的 frame 上。编辑消息的响应可能是这样的： &lt;body&gt; &lt;h1&gt;Editing message&lt;/h1&gt; &lt;turbo-frame id= message_1 &gt;  &lt;form action= /messages/1 &gt;   &lt;input name= message[name]  type= text  value= My message title &gt;   &lt;textarea name= message[name] &gt;My message content&lt;/textarea&gt;   &lt;input type= submit &gt;  &lt;/form&gt; &lt;/turbo-frame&gt;&lt;/body&gt;注意，&lt;h1&gt;并不在&lt;turbo-frame&gt;内。这意味着当表单替换展示的消息时它会被忽略掉。只有所匹配的&lt;turbo-frame&gt;之内的内容才会在 frame 被更新时用到。 因此，你的页面就可轻松实现双重目的：在整个页面专用于操作的 frame 之内或之外进行编辑。 Lazily Loading Frames: 当页面加载时，其中所包含的 frames 并不必都填充内容。如果&lt;turbo-frame&gt;上有一个&lt;src&gt;属性，那么一旦该 tag 出现在页面上时其中的 URL 就将被自动加载： &lt;body&gt; &lt;h1&gt;Imbox&lt;/h1&gt; &lt;div id= emails &gt;  . . .  &lt;/div&gt; &lt;turbo-frame id= set_aside_tray  src= /emails/set_aside &gt; &lt;/turbo-frame&gt; &lt;turbo-frame id= reply_later_tray  src= /emails/reply_later &gt; &lt;/turbo-frame&gt;&lt;/body&gt;这个页面在加载后会立即列出你的 imbox 中所有的可用邮件，但随后会发送两个后续请求，用来在页面底部为搁置或等待稍后回复的邮件展现小托盘。这些托盘都是根据src中所指定的 URL 所发出的单独 HTTP 请求而创建的。 上面示例中，托盘起初都是空的，但其也可以填充一些初始化内容，当从src获取到了内容时，这些初始化内容就会被覆盖掉： &lt;turbo-frame id= set_aside_tray  src= /emails/set_aside &gt; &lt;img src= /icons/spinner. gif &gt;&lt;/turbo-frame&gt;加载 imbox 页面后，set-aside托盘会从/emails/set_aside载入内容，而响应必须包含一个跟原始用例中相对应的&lt;turbo-frame id= set_aside_tray &gt;元素： &lt;body&gt; &lt;h1&gt;Set Aside Emails&lt;/h1&gt; &lt;p&gt;These are emails you've set aside&lt;/p&gt; &lt;turbo-frame id= set_aside_tray &gt;  &lt;div id= emails &gt;   &lt;div id= email_1 &gt;    &lt;a href= /emails/1 &gt;My important email&lt;/a&gt;   &lt;/div&gt;  &lt;/div&gt; &lt;/turbo-frame&gt;&lt;/body&gt;该页面现在以其最小化形式工作，即仅将具有单独邮件的div加载到 imbox 页面上的托盘 frame 中，而且还可以作为提供 header 和描述的直接目标。 就像在带有编辑消息表单的示例中那样。 注意，在/emails/set_aside上的&lt;turbo-frame&gt;不包含src属性。这个属性仅仅添加到需要 lazy 加载内容的 frame 上，而不是添加到提供内容的被渲染的 frame 上。 Cache Benefits to Lazily Loading Frames: 把页面片段转换成 frames 能够帮助使得页面实现更加简单，但同样重要的是这样做能够改善缓存动态。带有很多片段的复杂页面难以被有效缓存，特别是如果它们将许多人共享的内容与专门针对单个用户的内容混合在一起的话。这些片段越多，需要缓存查找的依赖的 key 越多，缓存流失的频率就越高。 Frames 是对在不同时间范围和不同受众上变化的片段进行分离的理想选择。有时，把页面中针对每个用户的元素转换为 frame 是有道理的，如果页面其他部分都是由所有用户共享的话。有时，则是相反的做法更有理由，例如在一个重度的个人化页面，把一个共享的片段转换为 frame 以便共享缓存服务于它。 虽然获取 lazy 加载的 frames 的开销通常很低，但是你仍然应该明智地确定要加载的数量，特别是如果这些 frames 会在页面上造成加载抖动时。然而，如果其内容在页面加载时不是立即可见的，那么 frames 基本上都是 free 的。可能是因为它们隐藏于 modal 或首屏之下。 Targeting Navigation Into or Out of a Frame: 默认情况下，在一个 frame 内部的导航就只针对那个 frame。对于点击链接和提交表单都是如此。但通过设置目标为_top，导航可以切换到整个页面而非闭合的 frame。或者也可以切换到另一个命名 frame 上，通过设置目标到该 frame 的 ID 的方式。 在 set-aside tray 的示例中，tray 之内到链接指向单独的邮件。你不会想要这些链接去找匹配set_aside_tray ID 的 frame tag。你想要的是直接导航到那些邮件。这通过把 tray frames 标记其target属性来实现： &lt;body&gt; &lt;h1&gt;Imbox&lt;/h1&gt; . . .  &lt;turbo-frame id= set_aside_tray  src= /emails/set_aside  target= _top &gt; &lt;/turbo-frame&gt;&lt;/body&gt;&lt;body&gt; &lt;h1&gt;Set Aside Emails&lt;/h1&gt; . . .  &lt;turbo-frame id= set_aside_tray  target= _top &gt;  . . .  &lt;/turbo-frame&gt;&lt;/body&gt;有时，你想要大多数链接在 frame 上下文中进行操作，但少部分不是。对于表单也是同样的。那么你可以把data-trubo-frame属性添加到非 frame 的元素上来实现： &lt;body&gt; &lt;turbo-frame id= message_1 &gt;  . . .   &lt;a href= /messages/1/edit &gt;   Edit this message (within the current frame)  &lt;/a&gt;  &lt;a href= /messages/1/permission  data-turbo-frame= _top &gt;   Change permissions (replace the whole page)  &lt;/a&gt; &lt;/turbo-frame&gt; &lt;form action= /messages/1/delete  data-turbo-frame= message_1 &gt;  &lt;input type= submit  value= Delete this message &gt;  (with a confirmation shown in a specific frame) &lt;/form&gt;&lt;/body&gt;"
    }, {
    "id": 18,
    "url": "/2021/03/hotwire-turbo-drive/",
    "title": "Hotwire之使用Turbo Drive导航",
    "body": "2021/03/13 - 本文是对 Turbo Drive 的详细说明，原文出自：https://turbo. hotwire. dev/handbook/drive。 Turbo Drive 是 Turbo 的一部分，用于增强页面级别的导航。它监控着点击链接和表单提交，使其在后台执行，并更新页面而无需做全页面的重载。它是Turbolinks 库的进化版本。 Page Navigation Basics: Turbo Drive 把页面导航建模为使用一种行为对一个页面位置（URL）的访问。 这种访问展现了从点击到渲染的整个导航生命周期，包含更改浏览器访问历史，发出网络请求，从缓存中恢复一个页面的拷贝，渲染最终的响应，以及更新页面滚动的位置。 有两种类型的访问：一种是 application 访问，其行为是 advance 或 replace。而另一种是 restoration 访问，其行为是 restore。 Application Visits: Application 访问由一个启用 Turbo Drive 的链接，或者程序中调用Turbo. visit(location)，而被初始化的。 一个 application 访问总是会发出一个网络请求。当响应返回，Turbo Drive 就渲染其页面并结束这次访问。 可能的话，Turbo Drive 会在访问开始时立即从缓存中渲染一个页面的预览。这就提高了在相同页面间频繁导航的感知速度。 如果访问的位置包含一个锚点，Turbo Drive 会尝试滚动到所锚住的元素。否则，会滚动到页面顶部。 Application 访问会导致浏览器访问历史的变更，而访问的行为决定了该变更是怎样的。 默认的访问行为是 advance。在一次 advance 访问期间，Turbo Drive 使用history. pushState向浏览器访问历史推入一个新条目。 使用 Turbo Drive 的 iOS adapter 的应用程序通常处理 advance 访问是推入一个新的 view controller 到导航堆栈之上。类似地，使用 Android adapter 的应用程序通常是推入一个新的 activity 到 back stack 之上。 你可能想要访问一个位置而不推入一个新访问历史条目到历史堆栈之上。replace 行为使用history. replaceState来丢弃最顶层的历史条目，并以新的位置来替换它。 要指定如下链接能触发一个 replace 访问，以data-turbo-action= replace 注释链接即可： &lt;a href= /edit  data-turbo-action= replace &gt;Edit&lt;/a&gt;要在程序中以 replace 行为访问一个位置，把action:  replace 选项传入Turbo. visit即可： Turbo. visit( /edit , { action:  replace  })使用 Turbo Drive 的 iOS adapter 的应用程序通常处理 replace 访问是放弃最顶层的 view controller 并推入一个新的 view controller 到导航堆栈之上，不带动画的方式。 Restoration Visits: 当你使用浏览器的后退或前进按钮导航时，Turbo Drive 会自动初始化一个复位访问。使用 iOS 或 Android adapters 的应用程序在导航堆栈中后移时会初始化一个复位访问。 可能的话，Turbo Drive 就从缓存中渲染一个页面的拷贝而不发出请求。否则，它将会通过网络获取一个页面的全新拷贝。查看下面的 Understanding Caching 以了解更多细节。 Turbo Drive 在导航离开每个页面时都会记住其滚动位置，在复位访问时会自动滚动回该位置处。 复位访问有一个 restore 行为，Turbo Drive 预留它们是为了内部使用。你无需尝试使用restore的行为来注释链接或执行Turbo. visit。 Canceling Visits Before They Start: Application 访问在其开始之前是可以被取消的，而不管它们是由点击链接还是由调用Turbo. visit来初始化的。 在访问开始的时候监听turbo:before-visit事件，并使用event. detail. url（使用 jQuery 时是$event. originalEvent. detail. url）来检查访问的位置。然后通过调用event. preventDefault()来取消访问。 复位访问不能被取消，且不会触发turbo:before-visit。Turbo Drive 发出复位访问来响应对已有的历史记录进行导航，通常是通过浏览器的后退或前进按钮。 Disabling Turbo Drive on Specific Links or Forms: 可以禁用一个元素的Turbo Drive，通过在它或其上级元素上添加data-turbo= fale 的方式。 &lt;a href= /  data-turbo= false &gt;Disabled&lt;/a&gt;&lt;form action= /messages  method= post  data-turbo= false &gt; . . . &lt;/form&gt;&lt;div data-turbo= false &gt; &lt;a href= / &gt;Disabled&lt;/a&gt; &lt;form action= /messages  method= post &gt;  . . .  &lt;/form&gt;&lt;/div&gt;在上级元素已禁用时要重新启用，使用data-turbo= true ： &lt;div data-turbo= false &gt; &lt;a href= /  data-turbo= true &gt;Enabled&lt;/a&gt;&lt;/div&gt;禁用了 Turbo Drive 的链接和表单仍然会被浏览器正常处理。 ﹟Displaying Progress: 在 Turbo Drive 导航期间，浏览器将不会展示其原生的进度条了。Turbo Drive 安装了一个基于 CSS 的进度条来在发出请求时给以反馈。 该进度条默认是启用的。它对任何载入时间超过 500 ms 的页面都会自动显示。（你可以使用Turbo. setProgressBarDelay方法修改这个值。） 该进度条是一个&lt;div&gt;元素，其 class 名为turbo-progress-bar。它的默认样式会首先呈现于 document，且可以被之后的规则所覆盖。 例如，下面的 CSS 将会呈现一个绿色的窄进度条： . turbo-progress-bar { height: 5px; background-color: green;}要完全禁用该进度条，把其visibility样式设为hidden： . turbo-progress-bar { visibility: hidden;}Reloading When Assets Change: Turbo Drive 可以在一个页面到另一个页面时跟踪&lt;head&gt;中的 asset 元素的 URL，如果它们有所变动则自动请求一次全量重载。这确保了用户始终会拥有你应用程序的 script 和 style 的最新版本。 使用data-turbo-track= reload 注释 asset 元素并在 asset URL 中包含一个版本标识符。该标识符可以是一个数字，一个最近修改的时间戳，或者更好点，一个 asset 内容的 digest，类似如下： &lt;head&gt; . . .  &lt;link rel= stylesheet  href= /application-258e88d. css  data-turbo-track= reload &gt; &lt;script src= /application-cbd3cd4. js  data-turbo-track= reload &gt;&lt;/script&gt;&lt;/head&gt;Ensuring Specific Pages Trigger a Full Reload: 你可以通过在页面的&lt;head&gt;中包含一个&lt;meta name= turbo-visit-control &gt;元素来确保在访问某个页面时始终会触发一次全量重载。 &lt;head&gt; . . .  &lt;meta name= turbo-visit-control  content= reload &gt;&lt;/head&gt;这个设置可能在第三方 JavaScript 库不能很好地跟 Turbo Drive 页面变更进行交互时是一个有用的解决方案。 Setting a Root Location: 默认情况下，Turbo Drive 只会加载同源 URL——比如，跟当前页面相同的协议，域名，端口。访问任何其他 URL 都会导致一次全量的页面加载。 有些场景中，你可能想要进一步定义 Turbo Drive 到同源的一个 path 上。例如，如果你的 Turbo Drive 应用位于/app，而非 Turbo Drive 的帮助站点位于/help，从前者到后者的链接不应该使用 Turbo Drive。 在页面的&lt;head&gt;中包含一个&lt;meta name= turbo-root &gt;元素以定义 Turbo Drive 到特定的 root 位置。Turbo Drive 将只加载那个 path 前缀的同源 URL。 &lt;head&gt; . . .  &lt;meta name= turbo-root  content= /app &gt;&lt;/head&gt;Redirecting After a Form Submission: Turbo Drive 以一种类似于链接点击的方式处理表单提交。关键区别是表单提交可以使用 HTTP POST 方法发出有状态的请求，而链接点击只会发出无状态的 HTTP GET 请求。 在有状态的表单提交后，Turbo Drive 期望服务端返回一个 HTTP 303 redirect response，其将被用以导航和页面更新而无需重载。 这条规则的例外情况是当响应是 4xx 或 5xx 的时候。这可以让服务端响应为422 Unprocessable Entity时呈现表单的校验错误，以及响应为500 Internal Server Error时显示服务端出错了。 Turbo 不允许对于 200 的常规渲染的原因是浏览器已有内置的行为来处理 POST 访问的重载，会展示一个“你确认想要重新提交表单吗？”的对话框，这是 Turbo 无法复制的。相反，Turbo 会在一个表单提交后尝试渲染页面时驻留在当前 URL 上，而不是变更到表单的 action，因为重载会发出到那个 action 所指向 URL 的一个 GET 请求，而该 URL 甚至可能根本不存在。 Streaming After a Form Submission: 服务端也可以借助 Turbo Streams 消息，通过发送 header Content-Type: text/vnd. turbo-stream. html，然后在响应内容中包含一个或多个&lt;turbo-stream&gt;元素，来响应表单提交，这让你可以无需导航即可更新页面的多个部分。 "
    }, {
    "id": 19,
    "url": "/2021/03/a-awesome-neovim-lsp-plugin/",
    "title": "让你的NeoVim Builtin LSP美轮美奂",
    "body": "2021/03/11 - 在配置 NeoVim Builtin LSP 的过程中，发现它自带的原生 UI 界面实在简陋了些。虽然功能用起来没问题，但远远够不上赏心悦目的程度。这对于像我这样的“视觉动物”来说，实在是坚决不能容忍的“大罪”了。 经过一番搜寻，终于找到一个很不错的 NeoVim Builtin LSP 增强插件：lspsaga. nvim。 它对自己的介绍是：“A light-weight lsp plugin based on neovim built-in lsp with highly a performant UI. ”，可见其完全聚焦于打造一个优雅精致的 UI 上。 它的安装和配置都没什么可说的，按照 Readme 文档的来就是了。 重点是来瞅瞅它的一些范例：  异步 LSP 查找  查看帮助文档  变量重命名  浮动终端窗口 确实相当精致！ 这种独有的 “Terminal 美学”，真的有一种让人欲罢不能的魅力。 "
    }, {
    "id": 20,
    "url": "/2021/03/hotwire-turbo-introduction/",
    "title": "Hotwire之Turbo介绍",
    "body": "2021/03/10 - 我去年 7 月的博客对 Hey. com 技术栈的期待 中提到了 DHH 在 HEY 中所使用的新技术栈。而在去年 12 月 23 日，DHH 不负所望，如期宣布了他的“NEW MAGIC”：Hotwire Hotwire 由 Turbo、Stimulus 和 Strada（将于2021公布） 构成。Strada 尚未发布先不论，Stimulus 也是早就发布的技术，并非新事物，所以真正的焦点就是 Turbo 了。这篇博客即是 Turbo 的一个概览。原文出自：https://turbo. hotwire. dev/handbook/introduction Introduction: Turbo 把几种技术集成到一起以创建快速的、现代的、渐进式增强的 Web 应用而毋需使用太多的 JavaScript。对于流行的把所有业务逻辑都置于前端，且把服务端限制在仅提供 JSON API 的那些客户端框架，它提供了一种更简单的替代方案。 借助 Turbo，你让服务端直接发布 HTML，这意味着所有业务逻辑，诸如权限检查、直接与你的领域模型交互，以及编写应用程序所需的其他一切，都能或多或少地只用你所喜欢的编程语言即可实现。而不用再把这些逻辑在被 JSON 所划分的两端都镜像实现。所有的逻辑都位于服务端，而浏览器只处理最终的 HTML。 你可以在 Hotwire 站点阅读到关于 HTML-over-the-wire 方案的更多优点。下面则是 Turbo 带来的使之成为可能的技术。 Turbo Drive: Navigate within a persistent process: 传统单页面应用的关键吸引力，比较于老式、单独页面的方案而言，就是导航跳转的速度。SPA 是通过仅在第一个页面初始化而非不断地中断应用程序进程从而获得如此高的响应速度的。 Turbo Drive 通过使用同样的持久化进程模型为你赋予了同样的高速响应，但无需你围绕范式来构建你的应用。没有了要维护的客户端路由，没有了要仔细管理的客户端状态。持久化进程由 Turbo 来管理，而你编写自己的服务端代码，仿佛又回到了早年间的时光——与当今 SPA 怪兽的复杂性幸福地隔离开了。 这通过拦截所有在链接上的点击到相同的领域而实现。当你点击一个链接时，Turbo Drive 阻止浏览器的响应，使用History API改变浏览器的 URL，使用 fetch 来获取新页面，然后渲染 HTML 响应。 对于表单是同样的处理。它们的提交都被转化为 fetch 请求，Turbo Drive 将根据这些请求遵循重定向并渲染 HTML 响应。 在渲染期间，Turbo Drive 会完全替换掉当前的&lt;body&gt;元素并合并&lt;head&gt;元素的内容。JavaScript 的 window 和 document 对象，以及&lt;html&gt;元素，会在一个页面到下一个页面的渲染中保持持久化。 尽管可以直接与 Turbo Drive 进行交互以控制访问如何发生或进入请求的生命周期，但在大多数情况下，这是一种即插即用的替代方法，只需遵循一些约定即可免费享受那种高速响应。 Turbo Frames: Decompose complex pages: 大多数 Web 应用程序显示的页面都包含几个独立的片段。对一个讨论区页面，你可能有一个导航栏在顶部，一个消息列表在中间，一个表单在底部以添加新消息，以及一个包含相关主题的侧边栏。生成这种讨论区页面通常意味着以一种序列化方式生成每个片段，把它们拼接在一起，然后把结果以单个 HTML 的响应发布给浏览器。 借助 Turbo Frames，你可以把这些独立片段置于 frame 元素之内，以限制其导航范畴并做懒式加载。限制其导航范畴意味着所有交互在 frame 内部，比如点击链接或者提交表单，都发生于那个 frame 之内，而避免更改或重新加载页面的其余部分。 要将一个独立片段封装在它自己的导航上下文中，把它包在一个&lt;turbo-frame&gt; 标签内即可。例如： &lt;turbo-frame id= new_message &gt; &lt;form action= /messages  method= post &gt;  . . .  &lt;/form&gt;&lt;/turbo-frame&gt;当你提交上面的表单时，Turbo 把匹配的&lt;turbo-frame id= new_message &gt;元素从重定向的 HTML 响应中提取出来，并把其内容交换到已有的的new_message frame 元素中。页面的其余部分保持原样。 除了限制导航范畴之外，Frames 也可以懒式加载其内容。要懒式加载一个 frame，添加一个其值为 URL 的src属性即可自动加载。与作用域导航一样，Turbo 从响应结果中查找并提取出匹配的 frame 并交换其内容到定位中： &lt;turbo-frame id= messages  src= /messages &gt; &lt;p&gt;This message will be replaced by the response from /messages. &lt;/p&gt;&lt;/turbo-frame&gt;这可能听起来很像旧时的 frames，甚至&lt;iframe&gt;，然而 Turbo Frames 是同一 DOM 的一部分，因此没有任何与“真实” frames 相关的怪异或妥协。Turbo Frames 由同样的 CSS 定义样式，是同一 JavaScript 上下文的一部分，且不受任何其他内容安全性限制。 除了把你的片段转化为独立上下文，Turbo Frames 还为你提供了：  高效缓存：在上面的讨论区页面范例中，相关主题的侧边栏当一个新主题呈现时（缓存）就需要过期，但中间的消息列表则不用。所有东西只在一个页面时，一旦任何一个片段过期则整个缓存都会过期。借助 frames，每个片段都被独立缓存，因此你可以获得更少依赖 key 的寿命更长的缓存。 并行执行：每个懒式加载的 frame 都由其自己的 HTTP 请求/响应所生成，这意味着它可以被单个进程来处理。这允许并行执行而不用手动管理进程。一个复杂的组合页面需要花费 400 ms 完成端到端渲染，则可以被分解为 frames，初始请求可能只需要 50 ms，而三个懒式加载的 frames 每个都耗费 50 ms。现在整个页面 100 ms 就可加载完成，因为那三个 frames 的50 ms 请求是并发而非序列式的。 Mobile 友好：在 mobile app 中，你通常不会有大而复杂的组合页面。每个片段都需要专用的屏幕。借助 Turbo Frames 构建的应用，你就已经完成了把复杂页面转换为片段的工作。然后这些片段就可以呈现在原生的表格和屏幕中，无需改动（因为它们都有独立的 URL）。Turbo Streams: Deliver live page changes: 响应异步操作而进行页面的部分更改是我们使应用保持活力的方式。尽管 Turbo Frames 给予了我们这种更新以响应在单个 frame 内的直接交互，而 Turbo Streams 则让我们得以响应通过 WebSocket 连接、SSE 或其他传输所发送的更新来更改页面的任何部分。 Turbo Streams 提供了一个&lt;turbo-stream&gt;元素，带有五种基本行为：append、prepend、replace、update和remove。借助这些行为，跟指定了你所想操作元素的 ID 的target属性一起，就可以编码所有需要的变更内容来刷新页面。你甚至可以组合几个 stream 元素在一个单独的 stream 消息中。只用把你需要插入或替换的 HTML 包含在一个 template tag 中，Turbo 会做好剩下的一切： &lt;turbo-stream action= append  target= messages &gt; &lt;template&gt;  &lt;div id= message_1 &gt;My new message!&lt;/div&gt; &lt;/template&gt;&lt;/turbo-stream&gt;该 stream 元素将会获取带新消息的div并把它 append 到 ID 为messages的容器中。要简单地替换掉已有的元素则可： &lt;turbo-stream action= replace  target= message_1 &gt; &lt;template&gt;  &lt;div id= message_1 &gt;This changes the existing message!&lt;/div&gt; &lt;/template&gt;&lt;/turbo-stream&gt;这是 Rails 世界中的一种概念上的持续（起初被称作 RJS，然后被称作 SJR），却是无需任何 JavaScript 的一种实现。优点是同样的：  重用服务端模板：实时页面改动的生成都跟初次加载页面一样使用同样的服务端模板。 HTML over the wire：由于所有我们所发送的都是 HTML，你就无需任何客户端 JavaScript （超越 Turbo，当然如此）来处理更新了。是的，HTML 的负荷可能比 JSON 要大一点，但借助 gzip，这点差异通常都是微不足道的，然而你节省了所有客户的的工作：获取 JSON 并把其转为 HTML。 更简单的控制流：当消息通过 WebSocket、SSE 到达或者响应表单提交时，会发生什么是很清晰的。再也没有路由、事件冒泡或其他的间接处理。它就仅是被变更的 HTML 而已，被封装在一个单独的 tag 中。现在，不像 RJS 和 SJR ，调用自定义的 JavaScript 函数来作为 Turbo Streams 行为的一部分是不可能的了。但这是一个特性，不是 bug。当发送太多 JavaScript 跟响应一起时，这些技术很容易最终导致乱七八糟的混乱状况。Turbo 专注于仅更新 DOM，然后假设你会使用 Stimulus 的 action 和生命周期回调来连接任何额外行为。 Turbo Native: Hybrid apps for iOS &amp; Android: Turbo Native 是构建适用于 iOS 和 Android 的 hybrid apps 的理想选择。你可以使用你现有的服务端渲染的 HTML 来得到在一个原生封装里的 app 功能的基线范畴。然后你就可以把所有节省下来的时间花在使受益于高保真原生控件的几个屏幕变得更好上。 像 Basecamp 这样的应用由上百个屏幕。重写每个单独的屏幕都会是一个收益极低的巨大工作。为真正需要高保真的重度交互保留“火力”才更好。例如，类似于 Basecamp 中的“New For You” inbox，这里我们使用轻扫控制需要恰到好处。但大多数页面，比如显示一条单独消息的页面，则不会比完全原生的做到更好。 使用 hybrid 不仅可以加快开发过程，还可以让你拥有更大的自由来升级 app，而无需经历缓慢而繁琐的应用商店发布过程。任何 HTML 做成的东西都可以在你的 web 应用程序中被更改，并即刻为所有用户所用。无需等待技术大牛来批准你的更改，也无需让用户等待升级。 Turbo Native 假设你正在使用对于 iOS 和 Android 所推荐的可用的开发实践。它不是一个抽象出原生 API 或者甚至让你的原生代码在多个平台之间共享的框架。共享的部分是在服务端渲染的 HTML。但是原生的控制是由所推荐的原生 API 写就。 参考 Turbo Native: iOS 和 Turbo Native: Android 代码库的更多文档。去看看 HEY 在 iOS 和 Android 上的原生 app 以感受下使用 Turbo 所制作的 hybrid app 有多么出色吧。 Integrate with backend frameworks: 你不需要任何后端框架即可使用 Turbo。它的所有特性都可直接使用，无需更多的抽象。但如果你有机会使用一个后端框架来整合 Turbo，将会发现一切变得更简单了。我们已经为与 Ruby on Rails 做这样的整合创建了一份实现的参考。 "
    }, {
    "id": 21,
    "url": "/2021/03/neovim-builtin-lsp-keymappings/",
    "title": "行云流水般的NeoVim Builtin LSP操作",
    "body": "2021/03/06 - 既然我们的 NeoVim 已经配置好 Builtin LSP 的 Server 和 Client，就该来看看如何使用它的问题了，也就是相关的 Keybinding 设定。好的 Keybinding 设定会让人在使用时运指如飞。 nvim-lspconfig 的 Readme 文档里已经给出了它的默认 Keybinding 设置，比如下面这几个比较常用的（更多可参考它的文档）： -- 查看函数声明buf_set_keymap('n', 'gD', '&lt;Cmd&gt;lua vim. lsp. buf. declaration()&lt;CR&gt;', opts)-- 查看函数定义buf_set_keymap('n', 'gd', '&lt;Cmd&gt;lua vim. lsp. buf. definition()&lt;CR&gt;', opts)-- 查看函数帮助文档buf_set_keymap('n', 'K', '&lt;Cmd&gt;lua vim. lsp. buf. hover()&lt;CR&gt;', opts)-- 查看函数相关引用buf_set_keymap('n', 'gr', '&lt;cmd&gt;lua vim. lsp. buf. references()&lt;CR&gt;', opts)-- 查看前一处语法错误buf_set_keymap('n', '[d', '&lt;cmd&gt;lua vim. lsp. diagnostic. goto_prev()&lt;CR&gt;', opts)-- 查看后一处语法错误buf_set_keymap('n', ']d', '&lt;cmd&gt;lua vim. lsp. diagnostic. goto_next()&lt;CR&gt;', opts)然而，试用之后发现，这样的配置用起来并不舒服。这些快捷键定义杂乱无章，不方便记忆；当结果有多个条目时，它默认使用 Vim 的 Quickfix 窗口，也不方便查询。 对于后者， 自然而然就会想到 FZF 这把犀利的瑞士军刀。Google 之后，发现早就有人想到这点，已经做了一个插件：nvim-lspfuzzy。这个插件让 NeoVim 把 LSP 的结果列表使用 FZF 来显示，并可进行高效筛选以及代码跳转。这是它的 demo 演示： 参照其文档安装好之后，在~/. config/nvim/init. nvim中添加一行配置： require('lspfuzzy'). setup {}然后我在. vimrc中做了这样的 Keybinding 设定： nnoremap &lt;silent&gt;&lt;leader&gt;ls &lt;cmd&gt;lua vim. lsp. buf. document_symbol()&lt;CR&gt;nnoremap &lt;silent&gt;&lt;leader&gt;ll &lt;cmd&gt;lua vim. lsp. buf. references()&lt;CR&gt;nnoremap &lt;silent&gt;&lt;leader&gt;lg &lt;cmd&gt;lua vim. lsp. buf. definition()&lt;CR&gt;nnoremap &lt;silent&gt;&lt;leader&gt;la &lt;cmd&gt;lua vim. lsp. buf. code_action()&lt;CR&gt;nnoremap &lt;silent&gt;&lt;leader&gt;l; &lt;cmd&gt;lua vim. lsp. diagnostic. goto_prev()&lt;CR&gt;nnoremap &lt;silent&gt;&lt;leader&gt;l, &lt;cmd&gt;lua vim. lsp. diagnostic. goto_next()&lt;CR&gt;可以看到，我把全部快捷键都定义为以&lt;leader&gt;l开头，这样即有规律，便于记忆了。而 FZF 的加持更是如虎添翼，写起代码来行云流水，流畅自如。 下一篇再介绍另一个很棒的 LSP 插件，让人写起代码来更加赏心悦目😄 "
    }, {
    "id": 22,
    "url": "/2021/02/neovim-builtin-lsp-basic-configuration/",
    "title": "NeoVim Builtin LSP的基本配置",
    "body": "2021/02/27 - 上一篇博客说过，现在官方对 NeoVim Builtin LSP 的配置已经做到了足够简洁易用的地步。这一篇就来看看怎样按照 NeoVim 官方的说明，一步一步把 LSP 给配置为可用于实际开发中。 为了简化 LSP 的安装和配置，NeoVim 官方专门创建了 nvim-lspconfig 插件来帮助我们。这个插件把所有 LSP 背后的繁琐都封装到其内部，让使用者再也毋需担心出现费了大半天功夫结果仍然无法用起来的事。 我自己的日常开发工作中主要会用到 Ruby、JavaScript 和 Golang 这几种编程语言，所以当然就需要把 NeoVim 的 LSP 配置为支持它们了。 首先，安装好 nvim-lspconfig 插件： Plug 'neovim/nvim-lspconfig'然后，按照 nvim-lspconfig 的文档说明，打开~/. config/nvim/init. vim，在其中添加如下配置： lua &lt;&lt; EOFrequire'lspconfig'. solargraph. setup{}require'lspconfig'. tsserver. setup{}require'lspconfig'. gopls. setup{}EOF接下来的步骤是……没了！😄 我的 Ruby、JavaScript 和 Golang 三种语言在 NeoVim 上的 LSP 支持就此完成！ 当然了，全部工作其实还没完。但 NeoVim 层面的部分确实已经结束了，算是足够简洁了吧？ 上面我都是使用了 nvim-lspconfig 的默认配置，因为足够了。但如果你想要针对每种语言单独做一些特别的设置，那么可以参考 nvim-lspconfig 的这个文档。 接下来，我还需要在自己系统上把上述三种语言的 LSP Server 安装上，然后 NeoVim 以上所配置好的 LSP Client 就可以正确地连接上它们。 Ruby 的 LSP Server 我使用 Solargraph，通过 Gem 安装： gem install solargraphJavaScript 的 LSP Server 我使用 tsserver，通过 npm 安装： npm install -g typescript typescript-language-serverGolang 的 LSP Server 我使用 gopls，通过 go mod 安装： go get golang. org/x/tools/gopls@latest全部装好之后，用 NeoVim 打开一个上述三种语言的任意一个项目的代码文件，在 Vim 命令行模式下输入:LspInfo，回车，应该就可以看到所有配置成功的 LSP 信息了。 "
    }, {
    "id": 23,
    "url": "/2021/02/switch-to-neovim-builtin-lsp/",
    "title": "正式切换到NeoVim Builtin LSP了！",
    "body": "2021/02/22 - 从 2019 年起，我的 Vim（实际是 NeoVim）都使用 coc. nvim作为 LSP（Language Server Protocol，微软的一套标准）。但现在，时代变了。 不可否认，coc. nvim 刚诞生的时候很是惊艳了众人一把。目前它仍然很火🔥，GitHub 的 star 数已经达到了 15. 2K。 但是，由于 coc. nvim 是基于 JS 开发的，它的生态圈（它的各种 Plugin）也都是JS。而要在 NeoVim 之外再依赖于另一套语言的Plugin（不光是JS，有些依赖Python的也是），总是会莫名其妙地经常发生各种问题。比如，最近我的 coc. nvim 的自动提示就出毛病了：刚打开 NeoVim 时，提示菜单能出来，但出去吃个午饭回来，下午再继续写代码时，它就死活没响应了（我的 NeoVim 上班时打开后基本一整天都不会关的）。 最近实在对 coc. nvim 的各种毛病忍受不了，于是把目光转向了 NeoVim 的内置 LSP。 调研了一下，发现随着 NeoVim 0. 5 版本愈来愈临近，其内置的 LSP 也愈加完善了。如果它的原生 LSP 支持就能满足我的日常使用需求，那 coc. nvim 及其相关 Plugin 整个一套都可以丢掉了。岂不美哉？😄 更令人欣喜地是，目前 NeoVim 官方也认识到了 LSP 对于开发者的重要性，因此对于各种语言 LSP 如何安装和配置，都通过插件和文档的形式，力求让使用者能够毫不费力地经过简单几个步骤就能完成。非常贴心！可以说，比当初我配置 coc. nvim 的时候简单轻松一百倍❤️ 接下来，后续准备用几篇博客来详细记述一下自己的 NeoVim 有关 LSP 相关配置的内容了。 "
    }, {
    "id": 24,
    "url": "/2020/12/how-to-graphql-no-n-plus-one/",
    "title": "GraphQL on Rails——避免N+1问题",
    "body": "2020/12/15 - 本文已获得原作者（Dmitry Tsepelev）和 Evil Martians 授权许可进行翻译。原文重点介绍了如何在 GraphQL 的世界里避免 N + 1 问题——以六种不同的方案。“六瞳之中莲花开放，七尺红绫击排空巨浪”。  原文链接：How to GraphQL with Ruby, Rails, Active Record, and no N+1 作者：Dmitry Tsepelev 站点：Evil Martians ——位于纽约和俄罗斯的 Ruby on Rails 开发人员博客。 它发布了许多优秀的文章，并且是不少 gem 的赞助商。【正文如下】 引言: 你工作在一个成熟的 Web 应用程序上，其被拆分为清晰的后端和前端。服务端代码用 Ruby 写成，主要负责把 HTTP 请求传递到 SQL 语句内（在 ORM 的帮助下），有丰富且很好文档的 API。你选择了 GraphQL 而非 REST 来简化端点，但数据库对于额外的查询并不满意。经过大量搜索后，你从 一个使用 GraphQL 的 Rubyist 那里发现了一个有关对抗 N + 1 问题的详尽指南……诺，它来了！ GraphQL可以在仅后端的 Rails 应用程序中创造奇迹，为你的客户端（无论是前端框架还是其他 API 使用者）提供一个单一端点，以获取他们可能需要的任何形式和大小的数据。 不过存在一个问题，一个大问题。N + 1 的大问题。 因为要加载的关联数据总是在 runtime 时才确定的，关于查询数据库很难做到智能化。 要么你可以接受令人难过的现实：一个处理父记录的 query + 针对每个关联记录的 query（所以是“N+1”，尽管严格的说应该是“1+N”）——要么你可以使用可爱的 SQL 语句一次性加载所有有关记录。但如果你有一个丰富的 schema，而其正是切换到 GraphQL 的原因，预加载会对数据库造成更大的压力，乃至超过 N+1 的压力。幸运的是，在 Ruby-GraphQL 世界里有一些工具，让我们对于加载什么，何时加载以及如何加载，有了更多的选择，也更智能化。 It’s always better to have an example: 让我们勾画一个实例，它有简单的 schema，作为简单的 Twitter 克隆应用。这里的目标不是要做到原生那样，而是能立即跟 types 关联起来。它们是Tweet，User和Viewer。Viewer是查看其他用户的 tweet 的用户。我们为“current user”创建一个单独的 type，因为它可以暴露出“general”用户所不可访问的那些属性。 class Types::BaseObject &lt; GraphQL::Schema::Object def current_user  context[:current_user] endendclass Types::Tweet &lt; Types::BaseObject field :content, String, null: false field :author, Types::User, null: falseendclass Types::User &lt; Types::BaseObject field :nickname, String, null: falseendclass Types::Viewer &lt; Types::BaseObject field :feed, [Types::Tweet], null: false def feed  # In this case, FeedBuilder is a query object  # that returns a Tweet relation based on passed params  FeedBuilder. for(current_user) endendclass Types::Query &lt; Types::BaseObject field :viewer, Types::Viewer, null: true, resolver_method: :current_userendclass GraphqlSchema &lt; GraphQL::Schema query Types::Queryend我也准备了一个 gist，在单个文件内包含了整个 Rails “应用”。你无法运行它，但它的功能足以通过我们包含的用于比较本文中讨论的不同优化方法的 specs。要查看代码和运行 specs，你可以在 terminal 中的任何临时目录下运行如下命令： curl  https://gist. githubusercontent. com/DmitryTsepelev/d0d4f52b1d0a0f6acf3c5894b11a52ca/raw/cba338548f3f87c165fc7ec07eb2c5b55120f7a2/2_demo. rb  &gt; demo. rbcreatedb nplusonedb # To create a PostgreSQL test database, requires Postgres installationrspec demo. rb # to run tests that compare different N+1-fighting techniques这段代码目前包含一个 N + 1 问题。查询含 tweet 作者昵称的 feed 将会对tweets表触发一个单独的 query，以及在users表中的 N 个 query。 { query {  viewer {   feed {    content    author {     nickname    }   }  } }}Solution #0: Load all the associations!: 让我们从清理代码并把 feed 的加载解构到一个 resolver 开始——resolver 是一个特别的 class，为数据库查询逻辑的一个概述。 class Resolvers::FeedResolver &lt; Resolvers::BaseResolver type [Types::Tweet], null: false def resolve  FeedBuilder. for(current_user) endendclass Types::Viewer &lt; Types::BaseObject field :feed, resolver: Resolvers::FeedResolverend如果你感兴趣，下面是我们FeedBuilder模块的定义，抽象了一些 Active Record 调用： module FeedBuilder module_function def for(user)  Tweet. where(author: user. followed_users)     . order(created_at: :desc)     . limit(10) endend把逻辑抽出来放到 resolver 使得我们能够创建替代的 resolver 并实时交换它们以比较结果。下面是一个通过预加载所有关联记录来解决 N+1 问题的 resolver： class Resolvers::FeedResolverPreload &lt; Resolvers::BaseResolver type [Types::Tweet], null: false def resolve  FeedBuilder. for(current_user). includes(:author) # Use AR eager loading magic endend这个解决办法很容易想到的，但不那么理想：我们总是会做一次额外的 SQL 查询来预加载 users，不管什么情况，甚至我们只请求 tweets 而不关心其作者时也如此（我知道，这难以想象，但假设一下匿名数据挖掘操作）。 还有，我们必须在顶层（在Query type 或属于它的 resolver 内部）就定义关联记录的列表。当一个新的嵌套字段出现在 graph 的深处时，很容易忘记把新的关联关系添加到列表里。 不过，当你知晓客户端大多情况下就是要求作者数据时，这个方案就很有用了（比如，当你掌控前端代码时）。 Solution #1: Lookaheads: 在处理 query 时，GraphQL 的 execution engine 知道哪些数据被请求，所以在运行时找到哪些数据应该被加载是可以做到的。graphql-ruby gem 自带一个可爱的 Lookahead 特性，可以提前告知我们某个特定字段是否被请求。让我们在一个单独的 resolver 中试试： class Resolvers::FeedResolverLookahead &lt; Resolvers::BaseResolver type [Types::Tweet], null: false extras [:lookahead] def resolve(lookahead:)  FeedBuilder. for(current_user)        . merge(relation_with_includes(lookahead)) end private def relation_with_includes(lookahead)  # . selects?(:author) returns true when author field is requested  return Tweet. all unless lookahead. selects?(:author)  Tweet. includes(:author) endend此处，我们仅当客户端要求author字段时才在users表内进行查询。这个方案仅仅在关联记录最小且没有嵌套时才能很好地工作。如果我们有着更复杂的数据 model，其中 users 有 avatars 和其所喜欢的 tweets，那么 resolver 很快就会脱离现实了： class Resolvers::FeedResolverLookahead &lt; Resolvers::BaseResolver type [Types::Tweet], null: false extras [:lookahead] def resolve(lookahead:)  scope =   Tweet. where(user: User. followed_by(current_user))      . order(created_at: :desc)      . limit(10)  scope = with_author(scope, lookahead) if lookahead. selects?(:author)  scope = with_liked_by(scope, lookahead) if lookahead. selects?(:liked_by)  scope end private def with_author(scope, lookahead)  if lookahead. selection(:author). selects?(:avatar)   scope. includes(user: :avatar_attachment)  else   scope. includes(:user)  end end def with_liked_by(scope, lookahead)  if lookahead. selection(:liked_by). selects?(:user)   if lookahead. selection(:liked_by). selection(:user). selects?(:avatar)    scope. includes(likes: { user: :avatar_attachment })   else    scope. includes(likes: :user)   end  else   scope. includes(:likes)  end endend没错，这一点都不优雅！有没有一种方式仅当关联记录被访问时才加载它们？Lazy preloading 就可以！ Solution #2: Lazy preloading (by Evil Martians): 在 Evil Martian 的一些同事帮助下，我写了一个名为 ar_lazy_preload 的小 gem，让我们回退到预加载方案，但无需任何额外成本即可使其变得更智能。它仅在关联对象被第一次访问后，用一个单独的请求来获取所有关联对象。当然，它也可以在 GraphQL 示例之外使用，并且在 REST API 中或在构建服务端渲染视图时非常方便。你只需把gem  ar_lazy_preload 添加到 Gemfile，bundle install，然后就可以像这样来写 resolver 了： class Resolvers::FeedResolverLazyPreload &lt; Resolvers::BaseResolver type [Types::Tweet], null: false def resolve  FeedBuilder. for(current_user). lazy_preload(:author) endend这个 gem 是在 懒惰 中创建出来的，所以如果你甚至都懒于敲. lazy_preload这点代码的话，可以在全局针对所有 Active Record 调用都启用它，只要在配置中添加下面这行： ArLazyPreload. config. auto_preload = true然而，这个方案有一些不足：  我们最终引入了第一个外部依赖； 我们对所做的 query 请求无法有太多控制，将难以对其进行自定义； 如果 lazy preloading 被打开，我们仍然不得不在顶层列举所有可能的关联数据； 如果一张表从两个地方被引用，我们就将请求数据库两次。还有其他我们能做的么？ Solution #3: graphql-ruby lazy resolvers: 让我们在 Ruby 应用中得以用上 GraphQL 的graphql-ruby gem 带来了一种使用 lazy execution 的方式：  代之以返回数据，你可以返回一个特别的 lazy object（该 object 会记得它所代替的数据）； 当一个 lazy value 从 resolver 被返回时，execution engine 停止当前 subtree 的进一步处理； 当所有非 lazy values 都被 resolved 时，execution engine 再要求处理 lazy object； lazy object 加载其所需处理的数据，并返回给每个 lazy object。这需要花一点时间来理解，所以我们来一步步实现一个 lazy resolver。首先，我们可以重用还未加入关联的初始FeedResolver： class Resolvers::FeedResolver &lt; Resolvers::BaseResolver type [Types::Tweet], null: false def resolve  FeedBuilder. for(current_user) endend然后，我们将从Tweet type 返回一个 lazy object。我们需要传递 user 的 ID 和 query context，因为将用它来存储要加载的 ID 列表： class Types::Tweet &lt; Types::BaseObject field :content, String, null: false field :author, Types::User, null: false def author  Resolvers::LazyUserResolver. new(context, object. user_id) endend每次一个新 object 被初始化，我们都添加一个待处理的 user ID 到 query context，并且，当#user被第一次调用时，我们做一次单独的数据库请求以得到所有所需的 users。之后，我们就可以为所有 lazy fields 填充 user 的数据了。下面是如何实现它的代码： class Resolvers::LazyUserResolver def initialize(context, user_id)  @user_id = user_id  @lazy_state = context[:lazy_user_resolver] ||= {   user_ids: Set. new,   users_cache: nil  }  @lazy_state[:user_ids] &lt;&lt; user_id end def user  users_cache[@user_id] end private def users_cache  @lazy_state[:users_cache] ||=   begin    user_ids = @lazy_state[:user_ids]. to_a    @lazy_state[:user_ids]. clear    User. where(id: user_ids). index_by(&amp;:id)   end endend想知道 execution engine 是如何区分出常规 object 与 lazy object 之间的不同吗？我们将在 schema 中定义 lazy resolver： class GraphqlSchema &lt; GraphQL::Schema lazy_resolve(Resolvers::LazyUserResolver, :user) query Types::Queryend它告诉 execution engine，当Resolvers::LazyUserResolver object 被返回时停止处理 users，并仅当所有其他非 lazy fields 被 resolved 之后才回来处理它。 这样一切运转正常，但是你可能经常需要重复很多样板代码。此外，当我们的 lazy resolvers 需要处理其他 lazy object 时，代码会变得相当复杂。幸运的是，有一个不太冗长的替代方案。 Solution #4: Batch loading: 来自 Shopify 的 graphql-batch 使用了与graphql-ruby相同的 lazy 机制，但隐藏了那些丑陋的样板代码部分。我们所需做的就是继承GraphQL::Batch::Loader并实现perform方法： class RecordLoader &lt; GraphQL::Batch::Loader def initialize(model)  @model = model end def perform(ids)  @model. where(id: ids). each { |record| fulfill(record. id, record) }  ids. each { |id| fulfill(id, nil) unless fulfilled?(id) } endend这个 loader（取自于其官方 repo 的 examples 目录）所期望的是 initializer 中的 model class（以决定数据将从哪里被加载）。#perform方法负责获取数据，#fulfill方法用来把一个 key 跟加载的数据关联起来。 Batch loader 的用法和 lazy 版本是类似的。我们传入User到 initializer，user 的 ID 用来懒式加载（该 ID 将被用做 key 来获取关联的 user）： class Types::Tweet &lt; Types::BaseObject field :content, String, null: false field :author, Types::User, null: false def author  RecordLoader. for(::User). load(object. author_id) endend跟通常一样，我们需要在 schema 中打开 lazy loading： class GraphqlSchema &lt; GraphQL::Schema query Types::Query use GraphQL::Batchend这是如何工作的呢？当use GraphQL::Batch被添加到 schema 时，Promise#sync就被注册了，从而做到懒式 resolve（其在幕后使用了 Promise. rb）。当#load方法在一个继承自GraphQL::Batch::Loader的 class 上被调用时，它返回一个Promise object——这就是为什么 execution engine 把其视为一个 lazy value。 这个方案有一个很有用的边际效应——你可用如下方式进行链式加载： def product_image(id:) RecordLoader. for(Product). load(id). then do |product|  RecordLoader. for(Image). load(product. image_id) endendSolution #5: Better schema design: 但甚至即使使用了上述的所有高级技术之后，还是可能最终出现 N+1 问题。想象一下，我们正在加上一个管理面板，在其中可以看到一个 users 列表。当一个 user 被选中时，用户信息弹窗弹出，你可以看到他的 followers 列表。在 GraphQL 世界里，数据将从其所属的地方被访问，我们会做类似如下的事情： class Types::User &lt; Types::BaseObject field :nickname, String, null: false field :followers, [User], null: false do  argument :limit, Integer, required: true, default_value: 2  argument :cursor, Integer, required: false end def followers(limit:, cursor: nil)  scope = object. followers. order(id: :desc). limit(limit)  scope = scope. where( id &lt; cursor , cursor) if cursor  scope endendclass Types::Query &lt; Types::BaseObject field :users, [User], null: false field :user, User, null: true do  argument :user_id, ID, required: true end def users  ::User. all end def user(user_id:)  ::User. find(user_id) endend用户列表可使用如下 query 来获取： query GetUsers($limit: Int) { users(limit: $limit) {  nickname }}follow 一个特定用户的用户列表可以像下面这样来加载： query GetUser($userId: ID, $followersLimit: Int, $followersCursor: ID) { user(userId: $userId) {  followers(limit: $limit, cursor: $followersCursor) {   nickname  } }}当有人试图加载一个用户列表，且在同一个 query 中包含其 followers 时，问题就出现了： query GetUsersWithFollowers( $limit: Int $followersLimit: Int $followersCursor: ID) { users(limit: $limit) {  nickname  followers(limit: $limit, cursor: $followersCursor) {   nickname  } }}这个场景里，我们根本无法摆脱 N+1：我们不得不对每一个 user 都做一次数据库调用，由于分页指针的缘故。要处理这种情况，我们可以使用不那么优雅的方案，把分页移至顶层： class Types::Query &lt; Types::BaseObject field :users, [User], null: false field :user, User, null: true do  argument :user_id, ID, required: true end field :user_followers, [User], null: false do  argument :limit, Integer, required: true, default_value: 2  argument :cursor, Integer, required: false end def users  ::User. all end def user(user_id:)  ::User. find(user_id) end def user_followers(user_id:, limit:, cursor: nil)  scope = UserConnection. where(user_id: user_id). order(user_id: :desc). limit(limit)  scope = scope. where( user_id &lt; cursor , cursor) if cursor  scope endend这个设计仍然能够加载 users 及其 followers，但事实证明，我们把服务端的 N+1 变成了 N+1 个 HTTP 请求。该解决方案看起来很好，但是嘿伙计，我们热爱 GraphQL 的 logical schema structure 呀！我们期望从User type 获取到 followers！ 没问题。我们可以在多个 users 被请求时限制对followers field 的获取。当其发生时我们来返回一个 error： class Types::Query &lt; Types::BaseObject field :users, [User], null: false, extras: [:lookahead] field :user, User, null: true do  argument :user_id, ID, required: true end def users(lookahead:)  if lookahead. selects?(:followers)   raise GraphQL::ExecutionError,  followers can be accessed in singular association only   end  ::User. all end def user(user_id:)  ::User. find(user_id) endend使用这个 schema，获取单个 user 的 followers 依旧是可以的，并且我们完全阻止了所不期望的场景。别忘了在文档中注明这点！ 就到这里了！你已经到达了本指南的结尾，现在你至少有六种不同方案可以在 Ruby-GrapgQL 代码中试用，以使应用程序摆脱 N + 1 问题。 也别忘了看看我们博客上其他有关 GraphQL 和 N+1 问题的文章：从对初学者友好且带代码的关于以 React 前端构建 Rails GraphQL 应用的教程（三个部分，从这里开始），到更加特定用例的 using GraphQL with Active Storage Direct Upload，处理来自 Apollo 的 persisted queries，以及graphql-ruby中的 reporting non-nullable violations。 我们也有一些 gems 让你在“经典” Rails 应用中处理 N+1 问题更容易，和一些相关文章：Squash N+1 queries early with n_plus_one_control test matchers for Ruby and Rails 和 Fighting the Hydra of N+1 queries。 "
    }, {
    "id": 25,
    "url": "/2020/11/graphql-on-rails-series-3/",
    "title": "GraphQL on Rails——至臻",
    "body": "2020/11/28 - 本文已获得原作者（Dmitry Tsepelev）、（Polina Gurtovaya）和 Evil Martians 授权许可进行翻译。原文是 Rails + React 使用 GraphQL的系列教程第三篇，介绍了以 Rails 作为后端，React + Apollo 作为前端，如何进行重构、错误处理以及实时更新等高级主题和技巧。  原文链接：GraphQL on Rails: On the way to perfection 作者：Dmitry Tsepelev，Polina Gurtovaya 站点：Evil Martians ——位于纽约和俄罗斯的 Ruby on Rails 开发人员博客。 它发布了许多优秀的文章，并且是不少 gem 的赞助商。【正文如下】 引言: 这是一个在后端使用 Rails、前端使用 React/Apollo 来开发 GraphQL 应用程序的旅行者指导。本教程的第三也是最后一部分都是关于实时更新的内容，以及 DRY 代码和实现更好的错误处理。 在本教程的前一部分里，我们已经构建了 Martian Library 应用程序的原型：用户可以在一个现代的 SPA 页面中动态地管理有关红色星球的制品。但还没到坐下休息放松的时候，因为我们还有一些重构要做。 如果你是从前两部分一路编写了代码——就用你自己的代码即可，如果不是——那么可以从这里拉取。 All you need is DRY: 让我们从后端开始来对 item 的 mutation（AddItemMutation和UpdateItemMutation）进行 一些 DRY 处理。我们有一些验证用户是否登录的重复代码： # app/graphql/mutations/add_item_mutation. rbmodule Mutations class AddItemMutation &lt; Mutations::BaseMutation  # . . .   def resolve   if context[:current_user]. nil?    raise GraphQL::ExecutionError,        You need to authenticate to perform this action    end   save_item  end endend把其移到BaseMutation类中： # app/graphql/mutations/base_mutation. rbmodule Mutations class BaseMutation &lt; GraphQL::Schema::Mutation  def check_authentication!   return if context[:current_user]   raise GraphQL::ExecutionError,       You need to authenticate to perform this action   end endend经过这样的修改，你就可以把AddItemMutation和UpdateItemMutation中的代码替换为check_authentication!的调用。这只是一个我们可以如何使用BaseMutation的例子罢了。在真实的应用程序中，它可以包含许多针对重复性工作的有用的帮助方法。 现在，让我们来看看前端代码。这里有怎样的代码重复呢？ 三个 query 看起来都非常类似：在Item query 中我们所选择的字段几乎是一样的。我们可以怎样避免这些重复？ 幸运的是，GraphQL 有其自身的“variables”，称作 fragments. 。一个 fragment 就是一个在特定类型上的命名字段集。 该是创建我们第一个 fragment 的时候了： $ mkdir -p app/javascript/fragments &amp;&amp; touch app/javascript/fragments/Item. graphql把所有重复的字段放入其中： fragment ItemFragment on Item { id title imageUrl description}现在我们需要把 fragment 添加到AddItemForm、UpdateItemForm和Library中的所有 operation 上。例如，Library组件中的 query 看起来会是这样： #app/javascript/components/Library/operations. graphql#import '. . /. . /fragments/Item. graphql'query LibraryQuery { items {  . . . ItemFragment  user {   id   email  } }}Dealing with errors: 我们知道，如果请求没有引起服务端报错的话，GraphQL 总是响应为 200 OK。通常有两种类型的错误发生：用户输入错误（校验）和异常。  校验错误 仅出现在 mutation 中，它们被包含在所返回的数据里。为用户提供有用的反馈，以显示在 UI 上。 异常 可以出现在任何 query 中，指示 query 里有什么东西出错了：例如，身份验证/权限的问题，无法处理的输入数据，等等（看后面内容）。如果响应包含异常，客户端就必须“尽全力失败”（比如，展示一个错误页面）。从前端角度来看，对于错误我们可以如何做呢？ 首先，我们可以设置一个错误日志记录器，来快速检测并修复错误（我们已经在第一部分中配置好了）。 其次，把组件封装在错误边界内是一个好主意，并在出现问题时用悲伤的开发者面孔显示错误屏幕。 第三，我们应该通过查阅文档来避免常见错误。当心点号并正确处理那些可为 null 的字段！看看你的 GraphiQL 文档中的me query： 根据文档，me是一个可为 null 的字段。我们不能随便使用诸如me. email的表达式，而需要确保 user 是存在的。 最后，我们应该在 render prop 函数内处理 GraphQL 错误。下面很快会向你展示如何来做。 当用户提交了非法数据，后端返回一个字符串的错误消息列表。让我们来修改下处理错误的方式：将会返回一个 object，包含同样的错误消息列表，但也包含一些 JSON 编码的细节。细节可用于生成客户端消息，或向用户提供额外的反馈（比如，高亮非法的表单字段）。 首先，来定义一个新的ValidationErrorsType： # app/graphql/types/validation_errors_type. rbmodule Types class ValidationErrorsType &lt; Types::BaseObject  field :details, String, null: false  field :full_messages, [String], null: false  def details   object. details. to_json  end endend现在，我们需要修改AddItemMutation来使用所定义的新类型（请为UpdateItemMutation做同样的事）： # app/graphql/mutations/add_item_mutation. rbmodule Mutations class AddItemMutation &lt; Mutations::BaseMutation  argument :title, String, required: true  argument :description, String, required: false  argument :image_url, String, required: false  field :item, Types::ItemType, null: true  field :errors, Types::ValidationErrorsType, null: true # this line has changed  def resolve(title:, description: nil, image_url: nil)   check_authentication!   item = Item. new(    title: title,    description: description,    image_url: image_url,    user: context[:current_user]   )   if item. save    { item: item }   else    { errors: item. errors } # change here   end  end endend最后，来为 item model 添加对应的校验： # app/models/item. rbclass Item &lt; ApplicationRecord belongs_to :user validates :title, presence: true validates :description, length: { minimum: 10 }, allow_blank: trueend现在，我们需要在接口中使用这些校验。我们应该为AddItemForm和UpdateItemForm更新逻辑。我们将为你展示对AddItemForm如何做，至于UpdateItemForm的代码，就留给读者作为一个练习了（当然，你可以在这儿找到解决办法）。 让我们先来为operations. graphql添加一个errors字段： #/app/javascript/components/AddItemForm/operations. graphql#import '. . /. . /fragments/Item. graphql'mutation AddItemMutation( $title: String! $description: String $imageUrl: String) { addItem(title: $title, description: $description, imageUrl: $imageUrl) {  item {   . . . ItemFragment   user {    id    email   }  }  errors { # new field   fullMessages  } }}现在，我们需要在AddItemForm及其上一级的ProcessItemForm中做一点小的改动，为错误添加一个新元素： // app/javascript/components/ProcessItemForm/index. jsconst ProcessItemForm = ({ // . . .  errors,}) =&gt; { // . . .  return (  &lt;div className={cs. form}&gt;   {errors &amp;&amp; (    &lt;div className={cs. errors}&gt;     &lt;div className= error &gt;{errors. fullMessages. join('; ')}&lt;/div&gt;    &lt;/div&gt;   )}   {/* . . . */}  &lt;/div&gt; );};export default ProcessItemForm;而在Mutation组件中，我们就从data property 抓取错误： // app/javascript/components/AddItemForm/index. js// . . . &lt;Mutation mutation={AddItemMutation}&gt; {(addItem, { loading, data }) =&gt; ( // getting data from response  &lt;ProcessItemForm   buttonText= Add Item    loading={loading}   errors={data &amp;&amp; data. addItem. errors} /&gt;   // . . .   ) }&lt;/Mutation&gt;如果你想要错误信息显示得更漂亮一点，就在/app/javascript/components/ProcessItemForm/styles. module. css添加如下样式： . form { position: relative;}. errors { position: absolute; top: -20px; color: #ff5845;}现在，让我们来谈论下 GraphQL 的第二种错误：异常。教程的前一章里，我们已经实现了身份验证，但没有实现一种处理用户带不存在 email 的方式。这不是所期望的行为，所以我们要确保抛出一个异常： # app/graphql/mutations/sign_in_mutation. rbmodule Mutations class SignInMutation &lt; Mutations::BaseMutation  argument :email, String, required: true  field :token, String, null: true  field :user, Types::UserType, null: true  def resolve(email:)   user = User. find_by!(email: email)   token = Base64. encode64(user. email)   {    token: token,    user: user   }  rescue ActiveRecord::RecordNotFound   raise GraphQL::ExecutionError,  user not found   end endend我们需要更改前端代码来优雅处理这种情形。在UserInfo组件中来做。从 render prop 函数所提供的对象中为Mutation组件抓取错误参数： // app/javascript/components/UserInfo/index. jsconst UserInfo = () =&gt; { // . . .  {(signIn, { loading: authenticating, error /* new key */ }) =&gt; { }} // . . . }并在&lt;/form&gt; 前添加一个元素来显示错误： // app/javascript/components/UserInfo/index. jsconst UserInfo = () =&gt; { &lt;form&gt;  // . . .   {error &amp;&amp; &lt;span&gt;{error. message}&lt;/span&gt;} &lt;/form&gt; // . . . }Handling input data: 让我们再回到AddItemMutation和UpdateItemMutation。看看 argument 列表，问问你自己，为什么我们有两个几乎相同的列表呢？每次我们向Item model 添加新字段，都需要添加新 argument 两次，这可不好。 解决办法相当简单：使用一个单独的 argument，包含所有需要的字段。graphql-ruby有一个称为BaseInputObject的特殊事物，用来定义类似如此的 argument 类型。我们来创建一个名为item_attributes. rb的文件： # app/graphql/types/item_attributes. rbmodule Types class ItemAttributes &lt; Types::BaseInputObject  description  Attributes for creating or updating an item   argument :title, String, required: true  argument :description, String, required: false  argument :image_url, String, required: false endend这个看起来很像之前所创建的类型，但有一个根本的区别：argument替代了 field。这是为什么？因为 GraphQL 遵循了 CQRS 原则，以两个不同的 model 来处理数据：读的 model（type）和写的 model（input）。 当心：你不能使用复杂类型作为 argument 类型——它只能是标量类型或其他 input 类型。 现在，我们可以把 mutation 更改为使用这个 argument 了。来从AddItemMutation开始： # app/graphql/mutations/add_item_mutation. rbmodule Mutations class AddItemMutation &lt; Mutations::BaseMutation  argument :attributes, Types::ItemAttributes, required: true # new argument  field :item, Types::ItemType, null: true  field :errors, Types::ValidationErrorsType, null: true # &lt;= change here  # signature change  def resolve(attributes:)   check_authentication!   item = Item. new(attributes. to_h. merge(user: context[:current_user])) # change here   if item. save    { item: item }   else    { errors: item. errors }   end  end endend如你所见，我们用一个名为attributes的单独 argument 替换了整个 argument 列表，修改#resolve以接收它，并稍微变更了我们创建 item 的方式。请对UpdateItemMutation进行同样的调整。现在我们需要修改前端代码来适配这些改动了。 我们唯一要做的就是添加一个单词和两个大括号到 mutation 上（对于UpdateItem也应该做同样的修改）： #/app/javascript/components/AddItemForm/operations. graphql#import '. . /. . /fragments/Item. graphql'mutation AddItemMutation( $title: String! $description: String $imageUrl: String) { addItem(  attributes: { # just changing the shape   title: $title   description: $description   imageUrl: $imageUrl  } ) {  item {   . . . ItemFragment   user {    id    email   }  }  errors {   fullMessages  } }}Implementing real-time updates: 服务端发起的更新在现代应用中很常见：我们的场景里，对于用户，在有人添加新 item 或修改现有 item 时，让其列表得到更新是很有用的。这正是 GraphQL subscriptions 的目的所在！ Subscription 是一种把服务端所发起的更新发布到客户端的机制。每个更新都返回特别类型的数据：例如，我们可以添加一个 subscription，当有新 item 被添加时就提醒客户端。当我们发送 Subscription operation 到服务端时，它会给我们返回一个 Event Stream。你可以使用任何方式，包括 post，来传输 events，但 Websockets 特别适合这种情形。对我们的 Rails 应用而言，意味着可以使用 ActionCable 来传输。下面是一个典型的 GraphQL subscription 所呈现的样子： Laying the cable: 首先，我们要创建app/graphql/types/subscription_type. rb并注册 subscription，使其在新 item 被添加时触发。 # app/graphql/types/subscription_type. rbmodule Types class SubscriptionType &lt; GraphQL::Schema::Object  field :item_added, Types::ItemType, null: false, description:  An item was added   def item_added; end endend其次，我们要配置 schema 以使用ActionCableSubscriptions，并能从SubscriptionType中找到可用的 subscriptions： # app/graphql/martian_library_schema. rbclass MartianLibrarySchema &lt; GraphQL::Schema use GraphQL::Subscriptions::ActionCableSubscriptions mutation(Types::MutationType) query(Types::QueryType) subscription(Types::SubscriptionType)end第三，我们要生成一个 ActionCable channel 来处理已订阅的客户端： $ rails generate channel GraphqlChannel让我们从文档中借用 channel 的实现代码： # app/channels/graphql_channel. rbclass GraphqlChannel &lt; ApplicationCable::Channel def subscribed  @subscription_ids = [] end def execute(data)  result = execute_query(data)  payload = {   result: result. subscription? ? { data: nil } : result. to_h,   more: result. subscription?  }  @subscription_ids &lt;&lt; context[:subscription_id] if result. context[:subscription_id]  transmit(payload) end def unsubscribed  @subscription_ids. each do |sid|   MartianLibrarySchema. subscriptions. delete_subscription(sid)  end end private def execute_query(data)  MartianLibrarySchema. execute(   query: data[ query ],   context: context,   variables: data[ variables ],   operation_name: data[ operationName ]  ) end def context  {   current_user_id: current_user&amp;. id,   current_user: current_user,   channel: self  } endend确认把:channel传给了 context。还有，我们传递了current_user使其在 resolvers 内部可用，跟:current_user_id一样，可被用来传递范围内的 subscriptions。 现在，我们需要添加在 channel 中获取当前用户的一种方式。以如下方式修改ApplicationCable::Connection： # app/channels/application_cable/connection. rbmodule ApplicationCable class Connection &lt; ActionCable::Connection::Base  identified_by :current_user  def connect   self. current_user = current_user  end  private  def current_user   token = request. params[:token]. to_s   email = Base64. decode64(token)   User. find_by(email: email)  end endend触发 event 相当简单：我们应传递驼峰式的字段名作为第一个 argument，options 是第二个 argument，而订阅的更新的 root object 作为第三个 argument。把其加到AddItemMutation： # app/graphql/mutations/add_item_mutation. rbmodule Mutations class AddItemMutation &lt; Mutations::BaseMutation  argument :attributes, Types::ItemAttributes, required: true  field :item, Types::ItemType, null: true  field :errors, [String], null: false  def resolve(attributes:)   check_authentication!   item = Item. new(attributes. merge(user: context[:current_user]))   if item. save    MartianLibrarySchema. subscriptions. trigger( itemAdded , {}, item)    { item: item }   else    { errors: item. errors. full_messages }   end  end endendArgument hash 可以包含 arguments，后者被定义在 subscription 中（其将被作为 resolver arguments 传递）。有一个称为:scope的第四个可选 argument，用来限制会接收到这些更新的用户的范围。 让我们来添加另一个 subscription，这次是更新 items： # app/graphql/types/subscription_type. rbmodule Types class SubscriptionType &lt; GraphQL::Schema::Object  field :item_added, Types::ItemType, null: false, description:  An item was added   field :item_updated, Types::ItemType, null: false, description:  Existing item was updated   def item_added; end  def item_updated; end endend下面是在UpdateItemMutation中我们将如何触发这种类型的更新： # app/graphql/mutations/update_item_mutation. rbmodule Mutations class UpdateItemMutation &lt; Mutations::BaseMutation  argument :id, ID, required: true  argument :attributes, Types::ItemAttributes, required: true  field :item, Types::ItemType, null: true  field :errors, [String], null: false  def resolve(id:, attributes:)   check_authentication!   item = Item. find(id)   if item. update(attributes. to_h)    MartianLibrarySchema. subscriptions. trigger( itemUpdated , {}, item)    { item: item }   else    { errors: item. errors. full_messages }   end  end endend我们应该提到一点，这种 subscriptions 方式是在 graphql-ruby 中为 ActionCable 实现的，会有性能上的瓶颈：大量 Redis 往返，并对每个连接的客户端进行查询重新评估（可在这里查看更多有关的深度解析）。 对 AnyCable 用户这已经不再是问题了——从我们为 eBay 项目的工作成果中抽取出来的 graphql-anycable gem 带来了高效的 GraphQL subscriptions。 Plugging in: 要让我们的应用程序发送数据给 ActionCable，需要一些配置。首先，我们要安装一些新 modules 来处理通过 ActionCable 的 Subscriptions： $ yarn add actioncable graphql-ruby-client然后，我们需要添加一些新“魔法”到/app/javascript/utils/apollo. js： // /app/javascript/utils/apollo. js. . . import ActionCable from 'actioncable';import ActionCableLink from 'graphql-ruby-client/subscriptions/ActionCableLink';. . . const getCableUrl = () =&gt; { const protocol = window. location. protocol === 'https:' ? 'wss:' : 'ws:'; const host = window. location. hostname; const port = process. env. CABLE_PORT || '3000'; const authToken = localStorage. getItem('mlToken'); return `${protocol}//${host}:${port}/cable?token=${authToken}`;};const createActionCableLink = () =&gt; { const cable = ActionCable. createConsumer(getCableUrl()); return new ActionCableLink({ cable });};const hasSubscriptionOperation = ({ query: { definitions } }) =&gt; definitions. some(  ({ kind, operation }) =&gt;   kind === 'OperationDefinition' &amp;&amp; operation === 'subscription' );//. . // we need to update our link link: ApolloLink. from([  createErrorLink(),  createLinkWithToken(),  ApolloLink. split(   hasSubscriptionOperation,   createActionCableLink(),   createHttpLink(),  ), ]),//. . 尽管这代码事实上看起来有点可怕，但是思路很简单：  我们在createActionCableLink内为 subscriptions 创建一个新的 Apollo link； 在 ApolloLink. split 内决定向哪里发送数据； 如果hasSubscriptionOperation返回 true，operation 就会被发送到actionCableLink。现在我们需要使用生成器创建一个新组件： $ npx @hellsquirrel/create-gql-component create /app/javascript/components/Subscription让我们来添加 subscription 到operations. graphql： #/app/javascript/components/Subscription/operations. graphql#import '. . /. . /fragments/Item. graphql'subscription ItemSubscription { itemAdded {  . . . ItemFragment  user {   id   email  } } itemUpdated {  . . . ItemFragment  user {   id   email  } }}毫无新意，对吧？再来创建Subscription组件： // /app/javascript/components/Subscription/index. jsimport React, { useEffect } from 'react';import { ItemSubscription } from '. /operations. graphql';const Subscription = ({ subscribeToMore }) =&gt; { useEffect(() =&gt; {  return subscribeToMore({   document: ItemSubscription,   updateQuery: (prev, { subscriptionData }) =&gt; {    if (!subscriptionData. data) return prev;    const { itemAdded, itemUpdated } = subscriptionData. data;    if (itemAdded) {     const alreadyInList = prev. items. find(e =&gt; e. id === itemAdded. id);     if (alreadyInList) {      return prev;     }     return { . . . prev, items: prev. items. concat([itemAdded]) };    }    if (itemUpdated) {     return {      . . . prev,      items: prev. items. map(el =&gt;       el. id === itemUpdated. id ? { . . . el, . . . itemUpdated } : el      ),     };    }    return prev;   },  }); }, []); return null;};export default Subscription;又一个 hook！这里是useEffect。它在初始渲染时被调用，并在用户更改时重新运行。 我们要求 hook 来订阅 add和update 的 event streams。当相应事件被触发时我们就添加或更新 items。 最后一步是把Subscription组件添加到Library，在Query组件内到最后一个div的尾部： import Subscription from '. . /Subscription';//. . . const Library = () =&gt; { const [item, setItem] = useState(null); return (  &lt;Query query={LibraryQuery}&gt;   {({ data, loading, subscribeToMore /* we need subscribe to more arg */}) =&gt; (    &lt;div&gt;     // . . .      &lt;Subscription subscribeToMore={subscribeToMore} /&gt;    &lt;/div&gt;   )}  &lt;/Query&gt; );};//. . . react-apollo 库的Query组件提供了特别的函数subscribeToMore，其被Subscription组件所使用。我们把这个函数传递给了Subscription组件。 现在我们可以来测试自己的 subscriptions 了！试着在一个浏览器的 tab 中添加新 item 或者修改已有的——你将会看到所有打开的 tabs 中都会出现变化。 结语: 祝贺你！ 这就是我们穿越 Ruby-GraphQL-Apollo 世界的令人激动的冒险之旅的终点了。使用一个小的范例应用，我们实践了所有的基础技术，强调了常见的问题，还介绍了一些高级主题。 这可能是一个具有挑战性的练习，但我们确信你将从中受益。无论如何，你现在都有足够的理论和实践来为自己创建利用上 GraphQL 强大威力的 Rails 应用了！ "
    }, {
    "id": 26,
    "url": "/2020/11/graphql-on-rails-series-2/",
    "title": "GraphQL on Rails——更新",
    "body": "2020/11/24 - 本文已获得原作者（Dmitry Tsepelev）、（Polina Gurtovaya）和 Evil Martians 授权许可进行翻译。原文是 Rails + React 使用 GraphQL的系列教程第二篇，介绍了以 Rails 作为后端，React + Apollo 作为前端，如何进行数据更新的教学。  原文链接：GraphQL on Rails: Updating the data 作者：Dmitry Tsepelev，Polina Gurtovaya 站点：Evil Martians ——位于纽约和俄罗斯的 Ruby on Rails 开发人员博客。 它发布了许多优秀的文章，并且是不少 gem 的赞助商。【正文如下】 引言: 这是一个在后端使用 Rails、前端使用 React/Apollo 来开发 GraphQL 应用程序的旅行者指导。本教程的第二部分将涵盖 mutation（更新数据的方式）和有关客户端缓存的高级主题 在该指南的第一部分中，我们学到了 GraphQL 是什么，并创建了一个 Martian Library 应用程序的很初级的版本。如果你还没阅读的话，现在正好去看一下。 我们已经配置了graphql-ruby gem 和 Apollo 框架以确保它们能一起很好地工作，也通过添加一个很初级的查询节点到 schema 上来实战检验了其配置。现在该继续前行了！ Introducing mutations: 我们已经知道，在 GraphQL 中有三种基础 operation—— query，mutation，及 subscriptions。本文中，我们将介绍 mutation——一种从 GraphQL 进行数据更改的机制。 从客户端的角度看，mutation 看起来跟 query 很像，只有一点细微的差别——它们从“mutation”节点开始： mutation SignInUser($email: String) { signIn(email: $email) {  id }}然而，其主要的区别，是语义上的：首先，mutation 负责修改（或转变）数据。在执行引擎处理它们的方式上，也有一个差别：根据规范，GraphQL 服务端必须确保 mutation 是被连续执行的，而 query 则能被并行执行。 在上面的 mutation 示例中，我们通过用户的 email 向服务端请求身份验证，以如下方式：  我们以一个 operation 名SignInUser和一个变量$email（所有 GraphQL 中的变量都以$开头）来定义一个 mutation 开始。 我们有一个想要执行修改的列表在大括号内（该列表称作 selection set）——这里我们只有一个叫signIn的字段。 跟 query 一样，在根字段内我们可以有嵌套的 selection sets（即，从 mutation 返回值选择特定字段）。 这些就是理论方面我们所需要了解的东西了。接下来的内容将专注于实践：我们将添加 mutation 来对用户进行身份验证，以及让用户添加新 items 到 Martian library。 Housekeeping: 先来快速看下在前一部分教程完成后我们的成果。你可在这里找到源码——别忘了在首次运行前执行bundle install &amp;&amp; yarn install。Master 分支则代表了该项目的当前状态。 我们使用graphql-tag库来执行查询，并使它们在同一个文件中靠近组件： // app/javascript/components/Library/index. jsimport React from  react ;import { Query } from  react-apollo ;import gql from  graphql-tag ;const LibraryQuery = gql` {  items {   id   title   user {    email   }  } }`;export default () =&gt; ( &lt;Query query={LibraryQuery}&gt;  {({ data, loading }) =&gt; (   &lt;div&gt;    {loading     ?  loading. . .       : data. items. map(({ title, id, user }) =&gt; (       &lt;div key={id}&gt;        &lt;b&gt;{title}&lt;/b&gt; {user ? `added by ${user. email}` : null}       &lt;/div&gt;      ))}   &lt;/div&gt;  )} &lt;/Query&gt;);或者，你可以把这些 operation 放在不同的文件中，以. graphql（或. gql）扩展名，保存在同一个目录下，作为组件定义。这种方案在开发中型——到大型——的应用程序时尤其有用，提供了清晰的项目结构。我们在本教程中对于所有的新 operation 都将使用它。 要让 Webpack “理解”. gql文件，我们需要在/config/webpack/environment. js中配置一个特别的 loader： // config/webpack/environment. jsconst { environment } = require( @rails/webpacker );environment. loaders. append( graphql , { test: /\. (graphql|gql)$/, exclude: /node_modules/, loader:  graphql-tag/loader });module. exports = environment;别忘了重启来让配置生效。 现在已经准备好实现身份验证逻辑了。 Implementing authentication: GraphQL 规范没有告诉你如何实现身份验证逻辑，甚至不需要你有这个——这取决于开发者。然而，你很难想象一个真实的应用程序没有它，我们的 Martian Library 也不例外——我们需要一种方式来追踪所有被添加到图书馆的 items 的拥有者。 我们让事情简单些，以用户的 email 进行验证，毋需密码，短信，及其他确认方式。 下面是我们的身份验证机制的概览：  用户提供 email 来发起身份验证请求 服务端验证该用户存在并以一个身份验证 tokan 返回响应 用户每次后续请求都带上该 token（比如，通过 HTTP Header）以证明其身份我们将使用一个 GraphQL mutation，signIn，来执行身份验证，并以一个 base64 加密的 email 作为身份验证 token，以及一个“Authorization” header 来传递该 token。注意，使用 GraphQL API 来验证用户并非是必须的：其可以在“外部”完成，比如，通过 REST。这在当你仅允许已验证用户访问 GraphQL API 时特别有用。 我们也期望在 UI 中指示用户是否已经通过身份验证。为此，我们将添加一个 panel ，如果用户已登录则显示其名称，否则显示“Sign In”按钮： Crafting authentication schema: 让我们先来添加一个 API 以获取当前用户的信息。 我们想让事情简单些：添加一个me字段到 query 的根上来返回其 context 的当前用户： # app/graphql/types/query_type. rbmodule Types class QueryType &lt; Types::BaseObject  # . . .   field :me, Types::UserType, null: true  def me   context[:current_user]  end endend如何得到:current_user？我们来添加一个ApplicationController#current_user方法，实现上述的身份验证逻辑： # app/controllers/application_controller. rbclass ApplicationController &lt; ActionController::Base private def current_user  token = request. headers[ Authorization ]. to_s  email = Base64. decode64(token)  User. find_by(email: email) endend最后，我们更新GraphqlController#execute方法以传递current_user到 context 内： # app/controllers/graphql_controller. rbclass GraphqlController &lt; ApplicationController def execute  result = MartianLibrarySchema. execute(   params[:query],   variables: ensure_hash(params[:variables]),   # Only this line has chagned   context: { current_user: current_user },   operation_name: params[:operationName]  )  render json: result end # . . . end漂亮！现在我们的客户端就能拿到当前用户的信息了。但不幸的是，它总是返回nil——我们还没有加上告知当前谁正在使用应用的方法。来修复它！ 打开Mutations::BaseMutation类并粘贴如下代码（默认生成器继承自更复杂的GraphQL::Schema::RelayClassicMutation类）： # app/graphql/mutations/base_mutation. rbmodule Mutations class BaseMutation &lt; GraphQL::Schema::Mutation endend我们将使用这个类作为SignInMutation的父类： # app/graphql/mutations/sign_in_mutation. rbmodule Mutations class SignInMutation &lt; Mutations::BaseMutation  argument :email, String, required: true  field :token, String, null: true  field :user, Types::UserType, null: true  def resolve(email:)   user = User. find_by!(email: email)   return {} unless user   token = Base64. encode64(user. email)   {    token: token,    user: user   }  end endend如你所见，我们指定了 mutation 可以返回一个 token 和一个当前的用户，而唯一接收的参数是email。在#resolve方法内，我们查找用户，如果找到了，就以 base64 加密的 email 作为 token 返回，否则返回null。 第一眼看去，mutation 类就像一个常规的 Rails controller，但它有一个重要的优点：它是强类型的，通过其 schema 来验证输入的数据。 最后，我们需要在MutationType中暴露这第一个 mutation： # app/graphql/types/mutation_type. rbmodule Types class MutationType &lt; Types::BaseObject  field :sign_in, mutation: Mutations::SignInMutation endend总结一下，为了添加一个新 mutation，你需要完成如下步骤：  添加一个类实现 mutation 逻辑，其包含： 输入值的类型定义（arguments）； 返回值的类型定义； #resolve方法 添加一个新的入口到MutationType中注意，我们根本没有提到 spec 测试：可以使用在之前编写的 query spec 所用过的相同技术来添加这里的 spec。或者去看看我们在示例代码库中写好的测试！ Adding user info panel: 让我们暂时先忘掉 Ruby 一会，把注意力放到前端应用来。 由于我们的代码库在不断增长，所以需要考虑一个更好的代码组织方式。我们对于 UI 组件推荐如下结构：  每个组件存放到一个单独的目录中（比如，app/javascript/components/MyComponent） index. js包含实现部分 query 定义在operations. graphql中 样式放到styles. module. css中（如文件名所建议的那样，我们使用css modules而毋需担心样式冲突）为了避免为每个组件都手动创建这些文件的繁琐，我们写了一个gql-component generator（graphql 组件生成器）。用它来创建一个称为UserInfo的组件吧： $ npx @hellsquirrel/create-gql-component create app/javascript/components/UserInfo注意：样式代码在本文中被去掉了，以保持简洁，但你可以在 GitHub 的 repo 中找到所有的样式文件。如果你使用我们的生成器，样式会被自动添加。 这将是你的文件结构看起来的样子： UserInfo组件负责“Sign In”的功能，以及当通过身份验证时展示当前用户名。让我们来首先添加这些功能所需要的 API 查询到operations. graphql中： query Me { me {  id  fullName }}mutation SignMeIn($email: String!) { signIn(email: $email) {  token  user {   id   fullName  } }}我们定义了SignMeIn operation，带所需的$email参数，为String类型，“执行”signIn mutation 并在成功时返回一个验证 token 和当前用户信息。你可能注意到了Me和SignMeIn operation上的某些重复——别担心，稍后我们会展示如何处理它们。 再打开index. js并使用上面定义的 operation 来定义我们的组件。我们期望先加载用户信息，且仅当用户没有被身份验证时才展示“Sign In”表单： &lt;Query query={Me}&gt; {({ data, loading }) =&gt; {  if (loading) return  . . . Loading ;  if (!data. me) {   // Show login form   return;  }  const { fullName } = data. me;  return &lt;div className={cs. info}&gt;😈 {fullName}&lt;/div&gt;; }}&lt;/Query&gt;要显示表单，我们应当使用Mutation组件并传递SignMeIn operation 为一个mutation property： &lt;Mutation mutation={SignMeIn}&gt; {(signIn, { loading: authenticating }) =&gt;  authenticating ? (    . . .    ) : (   &lt;form onSubmit={() =&gt; signIn(/* email here */)}&gt;    &lt;input type= email  /&gt;    &lt;input type= submit  value= Sign In  /&gt;   &lt;/form&gt;  ) }&lt;/Mutation&gt;别忘了导入 userRef hook，Query和Mutation组件，跟该组件中使用的 query 一起： import React, { useRef } from 'react';import { Query, Mutation } from  react-apollo ;import { Me, SignMeIn } from  . /operations. graphql ;这段代码看起来很像前面创建的Library组件。Mutation组件的 render prop 接收一个执行 mutation 的函数作为第一个参数（signIn），而第二个参数是一个 mutation 结果 object 的 object，包含返回的数据，加载的状态等等。 要传递 email 给 mutation，我们需要从 input（使用ref）来获取它，把它放入variable内，并执行 mutation： const UserInfo = () =&gt; { const input = useRef(null); // . . .  return (  &lt;form   onSubmit={event =&gt; {    event. preventDefault();    signIn({     variables: { email: input. current. value }    });   }}  &gt;   &lt;input    ref={input}    type= email     className={cs. input}    placeholder= your email    /&gt;  &lt;/form&gt; );};当在 JavaScript 中调用 mutation 时，我们以如下方式把值绑定到 variables：使用跟 operation 中同样的名称，但不要$前缀，比如，signIn({ variables: { email: '. . . ' } })。 让我们确保把 token 存储到某个地方以便在随后的请求和页面重载重用它： &lt;form onSubmit={event =&gt; {  event. preventDefault();  signIn({   variables: { email: input. current. value },  }). then(({ data: { signIn: { token } } }) =&gt; {   if (token) {    localStorage. setItem('mlToken', token);   }  }); }}&gt;在我们执行“Sign In”之后，就应该更新用户信息了（通过Me query）。 Dealing with cache: 有两种选择可以做到这点：  当 mutation 完成时重新请求me query（我们可以使用Mutation组件上refetchQueries property）——这个是有用的，但有更好的方式。 等待 mutation 完成并手动更新缓存。apollo-cache-inmemory为此提供了writeQuery函数。而react-apollo库的Mutation组件有一个称为update的特殊 property。它接收cache作为第一个参数，mutation 结果作为第二个参数。我们想要使用writeQuery方法手动添加一个新的缓存数据。这就好比在说“Hey，Apollo！这儿有一些数据，假装你是从服务端接收到它们的吧。”&lt;Mutation mutation={SignMeIn} update={(cache, { data: { signIn } }) =&gt; {  cache. writeQuery({   query: Me,   data: { me: signIn. user },  }); }}&gt;如下就是UserInfo组件最终看起来的样子： import React, { useRef } from  react ;import { Query, Mutation } from  react-apollo ;import { Me, SignMeIn } from  . /operations. graphql ;import cs from  . /styles ;const UserInfo = () =&gt; { const input = useRef(null); return (  &lt;div className={cs. panel}&gt;   &lt;Query query={Me}&gt;    {({ data, loading }) =&gt; {     if (loading) return  . . . Loading ;     if (!data. me) {      return (       &lt;Mutation        mutation={SignMeIn}        update={(cache, { data: { signIn } }) =&gt; {         cache. writeQuery({          query: Me,          data: { me: signIn. user }         });        }}       &gt;        {(signIn, { loading: authenticating }) =&gt;         authenticating ? (           . . .           ) : (          &lt;div className={cs. signIn}&gt;           &lt;form            onSubmit={event =&gt; {             event. preventDefault();             signIn({              variables: { email: input. current. value }             }). then(({ data: { signIn: { token } } }) =&gt; {              if (token) {               localStorage. setItem( mlToken , token);              }             });            }}           &gt;            &lt;input             ref={input}             type= email              className={cs. input}             placeholder= your email             /&gt;            &lt;input             type= submit              className={cs. button}             value= Sign In             /&gt;           &lt;/form&gt;          &lt;/div&gt;         )        }       &lt;/Mutation&gt;      );     }     const { fullName } = data. me;     return &lt;div className={cs. info}&gt;😈 {fullName}&lt;/div&gt;;    }}   &lt;/Query&gt;  &lt;/div&gt; );};export default UserInfo;恭喜！我们刚刚通过添加useRef到组件而购买了一张称作“React Hooks”的火车票。 更好的做法是把UserInfo拆分为两个单独的组件。第一个负责“Sign In”逻辑，第二个负责用户信息展示。你来自己搞定它吧！ 别忘了把组件添加到/javascript/packs/index. js： // app/javascript/packs/index. jsimport React from  react ;import { render } from  react-dom ;import Provider from  . . /components/Provider ;import Library from  . . /components/Library ;import UserInfo from  . . /components/UserInfo ;render( &lt;Provider&gt;  &lt;UserInfo /&gt;  &lt;Library /&gt; &lt;/Provider&gt;, document. querySelector( #root ));Adding tokens to Apollo client: 运行我们的应用程序，试着使用一个合法 email 登录。 一切正常，除了当你重新加载页面时——你会看到登录表单再次出现，即使你之前已成功登录了！解释很简单：我们把 token 存放在浏览器中，但没有“教” Apollo 使用它。让我们来修复这个问题！ 看一下utils/apollo. js： // app/javascript/utils/apollo. js// . . . const getToken = () =&gt; document. querySelector('meta[name= csrf-token ]'). getAttribute( content );const token = getToken();const setTokenForOperation = async operation =&gt; operation. setContext({  headers: {    X-CSRF-Token : token  } });我们已经有一个 CSRF token 发送到服务端了。再来添加一个新的——“Authorization” token： // app/javascript/utils/apollo. js// . . . const getTokens = () =&gt; { const tokens = {   X-CSRF-Token : document   . querySelector('meta[name= csrf-token ]')   . getAttribute( content ) }; const authToken = localStorage. getItem( mlToken ); return authToken ? { . . . tokens, Authorization: authToken } : tokens;};const setTokenForOperation = async operation =&gt; { return operation. setContext({  headers: {   . . . getTokens()  } });};再登录试试，重载页面——你会看到信息栏的用户名了！我们的“幸运之路”看起来畅通无阻。身份验证流程 ✅ Mutating the library: 现在我们要添加一些更多的 mutation ——这里没什么新东西，但我们需要它来使范例应用看起来更好，并得到更多的实践机会。 我们来增加一个 mutation 以向图书馆添加新 item。照例，我们需要定义传入参数和返回类型： # app/graphql/mutations/add_item_mutation. rbmodule Mutations class AddItemMutation &lt; Mutations::BaseMutation  argument :title, String, required: true  argument :description, String, required: false  argument :image_url, String, required: false  field :item, Types::ItemType, null: true  field :errors, [String], null: false  def resolve(title:, description: nil, image_url: nil)   if context[:current_user]. nil?    raise GraphQL::ExecutionError,        You need to authenticate to perform this action    end   item = Item. new(    title: title,    description: description,    image_url: image_url,    user: context[:current_user]   )   if item. save    { item: item }   else    { errors: item. errors. full_messages }   end  end endend这段代码里有几个要注意的地方：  我们检查context[:current_user]的存在，如果其未设定则抛出异常。 我们返回的类型包含两个字段：item和errors。为什么不用save!并抛出异常？用户输入的校验错误不应该被看作异常；我们的前端应用应把其视为一种合法响应并反馈给用户。其他的一切都看起来像是典型的 Rails controller 中的旧式#create行为。而如同#update的类似行为也非常简单： # app/graphql/mutations/update_item_mutation. rbmodule Mutations class UpdateItemMutation &lt; Mutations::BaseMutation  argument :id, ID, required: true  argument :title, String, required: true  argument :description, String, required: false  argument :image_url, String, required: false  field :item, Types::ItemType, null: true  field :errors, [String], null: false  def resolve(id:, title:, description: nil, image_url: nil)   if context[:current_user]. nil?    raise GraphQL::ExecutionError,        You need to authenticate to perform this action    end   item = Item. find(id)   if item. update(title: title, description: description, image_url: image_url)    { item: item }   else    { errors: item. errors. full_messages }   end  end endend你可能已经注意到在这两个类中有很多重复——不用担心，本系列的第三部分将涵盖重构的技术内容来修复这个问题。 最后，把新 mutation 注册到MutationType中： # app/graphql/types/mutation_type. rbmodule Types class MutationType &lt; Types::BaseObject  # . . .   field :add_item, mutation: Mutations::AddItemMutation  field :update_item, mutation: Mutations::UpdateItemMutation endendUpdating Library component: 在开始之前，来重新生成一下我们的 library 组件以遵循新架构（解构 operation，添加样式）： $ npx @hellsquirrel/create-gql-component create app/javascript/components/Library把如下 query 放入operations. graphql中： query LibraryQuery { items {  id  title  imageUrl  description  user {   id   email  } }}并“刷新” library 组件的实现方式： // app/javascript/components/Libraryimport React, { useState } from  react ;import { Query } from  react-apollo ;import { LibraryQuery } from  . /operations. graphql ;import cs from  . /styles ;const Library = () =&gt; { const [item, setItem] = useState(null); return (  &lt;Query query={LibraryQuery}&gt;   {({ data, loading }) =&gt; (    &lt;div className={cs. library}&gt;     {loading || !data. items      ?  loading. . .        : data. items. map(({ title, id, user, imageUrl, description }) =&gt; (        &lt;button         key={id}         className={cs. plate}         onClick={() =&gt; setItem({ title, imageUrl, id, description })}        &gt;         &lt;div className={cs. title}&gt;{title}&lt;/div&gt;         &lt;div&gt;{description}&lt;/div&gt;         {imageUrl &amp;&amp; &lt;img src={imageUrl} className={cs. image} /&gt;}         {user ? (          &lt;div className={cs. user}&gt;added by {user. email}&lt;/div&gt;         ) : null}        &lt;/button&gt;       ))}    &lt;/div&gt;   )}  &lt;/Query&gt; );};export default Library;注意，我们把每个 item 都包裹在button HTML 元素内：我们期望它们是可点击的，以展示更新过的表单。现在，我们的前端应用看起来漂亮多了。让我们来添加一些新的亮点吧！ Adding form components: 我们来为创建和编辑 item 添加更多的组件。这些组件都很类似，所以我们可以把很多逻辑都放到可重用的ProcessItemForm组件内。 $ npx @hellsquirrel/create-gql-component create app/javascript/components/ProcessItemForm组件代码如下： // app/javascript/components/ProcessItemForm/index. jsimport React, { useState } from  react ;import cs from  . /styles ;const ProcessItemForm = ({ initialTitle =   , initialDescription =   , initialImageUrl =   , onProcessItem, buttonText, loading}) =&gt; { const [title, setTitle] = useState(initialTitle); const [description, setDescription] = useState(initialDescription); const [imageUrl, setImageUrl] = useState(initialImageUrl); return (  &lt;div className={cs. form}&gt;   &lt;input    type= text     placeholder= title     value={title}    className={cs. input}    onChange={e =&gt; setTitle(e. currentTarget. value)}   /&gt;   &lt;input    type= text     placeholder= description     value={description}    className={cs. input}    onChange={e =&gt; setDescription(e. currentTarget. value)}   /&gt;   &lt;input    type= text     placeholder= url     value={imageUrl}    className={cs. input}    onChange={e =&gt; setImageUrl(e. currentTarget. value)}   /&gt;   {loading ? (     . . . Loading    ) : (    &lt;button     onClick={() =&gt; onProcessItem({ title, description, imageUrl })}     className={cs. button}    &gt;     {buttonText}    &lt;/button&gt;   )}  &lt;/div&gt; );};export default ProcessItemForm;我们唯一所需要添加的是创建 item 的 form——我们把其称为AddItemForm。 $ npx @hellsquirrel/create-gql-component create app/javascript/components/AddItemForm我们要把 AddItemMutation 添加到operations. graphql： # /app/javascript/components/AddItemForm/operations. graphqlmutation AddItemMutation( $title: String! $description: String $imageUrl: String) { addItem(title: $title, description: $description, imageUrl: $imageUrl) {  item {   id   title   description   imageUrl   user {    id    email   }  } }}并在index. js中使用它： import React from  react ;import { Mutation } from  react-apollo ;import { AddItemMutation } from  . /operations. graphql ;import ProcessItemForm from  . . /ProcessItemForm ;const AddItemForm = () =&gt; ( &lt;Mutation mutation={AddItemMutation}&gt;  {(addItem, { loading }) =&gt; (   &lt;ProcessItemForm    buttonText= Add Item     loading={loading}    onProcessItem={({ title, description, imageUrl }) =&gt;     addItem({      variables: {       title,       description,       imageUrl      }     })    }   /&gt;  )} &lt;/Mutation&gt;);export default AddItemForm;别忘了添加 form 到/javascript/packs/index. js： import React from  react ;import { render } from  react-dom ;import Provider from  . . /components/Provider ;import Library from  . . /components/Library ;import UserInfo from  . . /components/UserInfo ;import AddItemForm from  . . /components/AddItemForm ;render( &lt;Provider&gt;  &lt;UserInfo /&gt;  &lt;AddItemForm /&gt;  &lt;Library /&gt; &lt;/Provider&gt;, document. querySelector( #root ));现在我们遭遇了跟在UserInfo组件中同样的问题。我们需要告知应用：LibraryQuery应该被更新。因此我们必须刷新缓存：通过读取整个列表并把新 item 合并到列表上以设置一个新列表。 来改一下javascript/components/AddItemForm/index. js： // javascript/components/AddItemForm/index. js// . . . import { LibraryQuery } from '. . /Library/operations. graphql';// . . . &lt;ProcessItemForm //. . .  // Update library query after Mutation will be finished onProcessItem={({ title, description, imageUrl }) =&gt;  addItem({   variables: {    title,    description,    imageUrl,   },   // adding the second argument to 'addItem' method   update: (cache, { data: { addItem } }) =&gt; {    const item = addItem. item;    if (item) {     const currentItems = cache. readQuery({ query: LibraryQuery });     cache. writeQuery({      query: LibraryQuery,      data: {       items: [item]. concat(currentItems. items),      },     });    }   },  }) } // . . . 搞定！现在我们会看到新的 item 被添加到页面列表了。 来为更新 item 再添加一个组件，称为UpdateItemForm。代码非常类似于 AddItemForm。运行生成器： $ npx @hellsquirrel/create-gql-component create app/javascript/components/UpdateItemForm下面是 operations 文件中的内容： mutation UpdateItemMutation( $id: ID! $title: String! $description: String $imageUrl: String) { updateItem(  id: $id  title: $title  description: $description  imageUrl: $imageUrl ) {  item {   id   title   description   imageUrl  } }}这是组件文件中的内容： // /app/javascript/components/UpdateItemFormimport React from  react ;import { Mutation } from  react-apollo ;import { UpdateItemMutation } from  . /operations. graphql ;import ProcessItemForm from  . . /ProcessItemForm ;import cs from  . /styles ;const UpdateItemForm = ({ id, initialTitle, initialDescription, initialImageUrl, onClose}) =&gt; ( &lt;div className={cs. overlay}&gt;  &lt;div className={cs. content}&gt;   &lt;Mutation mutation={UpdateItemMutation}&gt;    {(updateItem, { loading }) =&gt; (     &lt;ProcessItemForm      initialImageUrl={initialImageUrl}      initialTitle={initialTitle}      initialDescription={initialDescription}      buttonText= Update Item       loading={loading}      onProcessItem={({ title, description, imageUrl }) =&gt; {       updateItem({        variables: {         id,         title,         description,         imageUrl        }       });       onClose();      }}     /&gt;    )}   &lt;/Mutation&gt;   &lt;button className={cs. close} onClick={onClose}&gt;    Close   &lt;/button&gt;  &lt;/div&gt; &lt;/div&gt;);export default UpdateItemForm;并把 UpdateItemForm 添加到 library（位于 button 之后）： // /app/javascript/components/Library/index. js//. . . import UpdateItemForm from  . . /UpdateItemForm ;// . . . &lt;button /&gt;;{ item !== null &amp;&amp; (  &lt;UpdateItemForm   id={item. id}   initialTitle={item. title}   initialDescription={item. description}   initialImageUrl={item. imageUrl}   onClose={() =&gt; setItem(null)}  /&gt; )}// . . . 现在如果我们点击 item 并修改，它就会神奇地更新了。为什么呢？ 当获取一个 item 列表时，响应结果被规范化，且每个 item 都被添加到缓存。apollo为每个有__typename和id的实体都生成一个 key：${object__typename}:${objectId}。当 mutation 完成的时候，我们获取到有相同__typename和id的对象，apollo在缓存中找到它，并进行更改（组件也被重新渲染）。 我们能做得更好一些么？当然！ 为什么我们要等待服务端的响应呢？如果我们对服务端有足够的信心，那么我们可以使用乐观式更新。让我们再添加一个参数到 updateItem 函数： // /app/javascript/components/UpdateItemForm//. . . updateItem({ variables: {  //. . .  }, // adding the second argument to 'updateItem' method optimisticResponse: {  __typename:  Mutation ,  updateItem: {   __typename:  UpdateItemMutationPayload ,   item: {    id,    __typename:  Item ,    title,    description,    imageUrl   }  } }});//. . 这些就是本文的全部内容了！我们学习了 mutation 和 query 之间的区别，学习了在后端如何实现它们，以及如何在前端使用它们。现在，我们的应用支持用户登录和图书馆的管理，所以几乎已准备好发布到 production 了！然而，代码看起来还有些笨拙，有重构的空间——这正是我们将在第三部分中要做的，并添加一些其他改进，例如实时更新和更好的错误处理。敬请关注！ "
    }, {
    "id": 27,
    "url": "/2020/11/graphql-on-rails-series-1/",
    "title": "GraphQL on Rails——启程",
    "body": "2020/11/12 - 本文已获得原作者（Dmitry Tsepelev）、（Polina Gurtovaya）和 Evil Martians 授权许可进行翻译。原文是 Rails + React 使用 GraphQL的系列教程第一篇，介绍了以 Rails 作为后端，React + Apollo 作为前端，如何经过基础的配置，构建一个简单图书馆列表页面。  原文链接：GraphQL on Rails: From zero to the first query 作者：Dmitry Tsepelev，Polina Gurtovaya 站点：Evil Martians ——位于纽约和俄罗斯的 Ruby on Rails 开发人员博客。 它发布了许多优秀的文章，并且是不少 gem 的赞助商。【正文如下】 引言: 这是一个在后端使用 Rails、前端使用 React/Apollo 来开发 GraphQL 应用程序的旅行者指导。跟随该系列教程可通过范例学到既有基础的、也有高级的主题内容，让你领略现代技术的威力。 GraphQL 是我们在任何地方（博客、会议、播客，甚至报纸）都能见到的新颖事物之一。听起来你应该抓紧时间，尽快开始以 GraphQL 而非 REST 来重写应用程序，对吧？事实并非如此。记住：没有银弹。在进行决策之前理解该技术的优劣是完全有必要的。 本系列中，我们将给你一个 GraphQL 应用程序开发的简洁指南，不止谈到其优点，也会讨论其注意事项乃至陷阱（当然，还有如何处理它们的方法）。 GraphQL in a nutshell: 根据规范，GraphQL是一种查询语言和 runtime（或执行引擎）。查询语言，按照定义，描述了如何与一个信息系统进行通信。Runtime 则负责实现数据的查询。 每个 GraphQL 应用程序的核心都在于一个 schema：它以有向图的形式描述底层数据。Runtime 必须根据该 schema（及规范中的一些通用规则）来执行查询。这意味着，每个有效的 GraphQL 服务端都以相同的方式运行查询，并以相同的格式返回相同 schema 的数据。换句话说，schema 就是客户端应了解到的关于 API 的一切。 下面是一个简单的 GraphQL 查询的例子： query getProduct($id: Int!) { product(id: $id) {  id  title  manufacturer {   name  } }}让我们来一行一行解析它：  我们定义了一个具名查询（getProduct是操作名），接收单独一个参数（$id）。操作名是可选的，但它会对可读性有所帮助，也能用于前端进行缓存。 我们从 schema 的“根”上“选择”了product字段，并传递$id值作为参数。 我们描述了期望获取的那些字段：该场景中，是想要得到 product 的id和title，以及 manufacturer 的name。本质上，一个查询代表了 schema 的一个子图，这带来了 GraphQL 的第一个好处——我们可以在单个查询中，仅获取自己所需要的数据，也可以获取一次所需的所有数据。 这样，我们就解决了传统 REST API 的一个常见问题——overfetching（过量获取）。 另一个关于 GraphQL schema 的明显特性是它们为强类型的（strongly typed）：客户端和 runtime 两边都确保了从应用程序的类型系统角度看，所传递的数据是合法的。例如，如果有人错误传递了一个字符串的值作为$id给上面的查询，客户端就会因抛出异常而失败，甚至不会尝试执行查询。 最后但并非最终的一个好处是 schema 的自省：客户端可以从 schema 自身来学习 API，而无需任何额外的文档资源。 那么，我们已经了解了 GraphQL 的不少理论部分。现在该来做一些代码练习了，以确保你不会明早起来就忘掉一切。 What are we going to build?: 通过这个系列，我们将构建一个代表“Martian Library”的应用程序——一个影视、书籍及其他与《红色星球》有关的事物的个人在线收藏。 对于本教程，我们将使用：  后端使用Ruby 2. 6 和 Rails 6（RC 版本在此）【译者注：Rails 6 正式版目前已经发布了】 前端使用 Node. js 9+，React 16. 3+，和 Apollo（客户端版本 2+），要确保你已经根据指导安装了 yarn。你可以在这里找到源码——别忘了在首次运行前执行bundle install &amp;&amp; yarn install。Master 分支是该项目的当前最新状态。 Setting up a new Rails project: 如果阅读本文的时候 Rails 6. 0 还没有发布，那么你可能需要先安装 rc 版本： $ gem install rails --pre$ rails -v=&gt; Rails 6. 0. 0. rc1现在我们就可以来运行下面这个超级长的rails new命令了： $ rails new martian-library -d postgresql --skip-action-mailbox --skip-action-text --skip-spring --webpack=react -T --skip-turbolinks比起 Rails 官方的“主厨精选”，我们更喜欢自己来定制：略去所不需要的框架和库，选择 PostgreSQL 作为数据库，以预配置的 Webpacker 来使用 React，去掉了测试（别担心——我们会很快加上 RSpec 的）。 在你开始之前，强烈建议关闭config/application. rb内所有不必要的生成器： config. generators do |g| g. test_framework false g. stylesheets   false g. javascripts   false g. helper     false g. channel     assets: falseendPreparing the data model: 我们需要至少两个 model：  用Item来描述任何我们想要存储在图书馆中的实体（书籍、电影等）。 用User来代表应用程序里能够管理收藏品中这些 items 的用户。让我们来生成它们： $ rails g model User first_name last_name email$ rails g model Item title description:text image_url user:references别忘了添加has_many :items的关联关系到app/models/user. rb： # app/models/user. rbclass User &lt; ApplicationRecord has_many :items, dependent: :destroyend来添加一些预生成的数据到db/seeds. rb： # db/seeds. rbjohn = User. create!( email:  john. doe@example. com , first_name:  John , last_name:  Doe )jane = User. create!( email:  jane. doe@example. com , first_name:  Jane , last_name:  Doe )Item. create!( [  {   title:  Martian Chronicles ,   description:  Cult book by Ray Bradbury ,   user: john,   image_url:  https://upload. wikimedia. org/wikipedia/en/4/45/The-Martian-Chronicles. jpg   },  {   title:  The Martian ,   description:  Novel by Andy Weir about an astronaut stranded on Mars trying to survive ,   user: john,   image_url:  https://upload. wikimedia. org/wikipedia/en/c/c3/The_Martian_2014. jpg   },  {   title:  Doom ,   description:  A group of Marines is sent to the red planet via an ancient   \           Martian portal called the Ark to deal with an outbreak of a mutagenic virus ,   user: jane,   image_url:  https://upload. wikimedia. org/wikipedia/en/5/57/Doom_cover_art. jpg   },  {   title:  Mars Attacks! ,   description:  Earth is invaded by Martians with unbeatable weapons and a cruel sense of humor ,   user: jane,   image_url:  https://upload. wikimedia. org/wikipedia/en/b/bd/Mars_attacks_ver1. jpg   } ])最后，我们就可以来初始化数据库了： $ rails db:create db:migrate db:seed现在我们已经往自己的系统里塞入了一些内容，那就来添加访问它们的方式吧！ Adding a GraphQL endpoint: 为了“制作”我们的 GraphQL API，将使用graphql-ruby gem： # First, add it to the Gemfile$ bundle add graphql --version= ~&gt; 1. 9 # Then, run the generator$ rails generate graphql:install你可能会惊讶于一个最小化的graphql-ruby应用程序所需文件的数量：如下的样板就是我们为上述所有物品所支付的代价。 首先，我们来看看 schema，martian_library_schema. rb： # app/graphql/martian_library_schema. rbclass MartianLibrarySchema &lt; GraphQL::Schema query(Types::QueryType) mutation(Types::MutationType)end该 schema 宣布了所有 query 都应该在Types::QueryType，而所有 mutation 都应该在Types::MutationType。我们将在本系列教程的第二部分来深入探讨 mutation。本文的目标则是学习如何编写和执行 query。因此，让我们打开types/query_type. rb 类——它是所有 query 的入口。里面有什么呢？ # app/graphql/types/query_type. rbmodule Types class QueryType &lt; Types::BaseObject  # Add root-level fields here.   # They will be entry points for queries on your schema.   # TODO: remove me  field :test_field, String, null: false,   description:  An example field added by the generator   def test_field    Hello World!   end endend这证明了QueryType就是一个通用 type：其继承于Types::BaseObject（我们会把它用作所有 type 的基本类），并且它有 field 定义——我们数据图的节点。唯一使得QueryType有所不同的是 GraphQL 需要这个 type 必须存在（而mutation和subscription 两种 type 是可选而非必须）。 注意到上面的代码实际上仅是一个”hello world”了吗？在继续往下走之前（且大量的代码使你厌倦），我们会向你展示如何在浏览器中获取该“hello world”的内容。 让我们来看下生成器已经往config/routes. rb中添加了什么： # config/routes. rbRails. application. routes. draw do mount GraphiQL::Rails::Engine, at:  /graphiql , graphql_path:  /graphql  if Rails. env. development? post  /graphql , to:  graphql#execute endMount 的GraphiQL::Rails::Engine让我们能使用一个称为 GraphiQL 的 web 界面来测试自己的 query 和 mutation。如前所述，schema 是可被检查的，而 GraphiQL 则使用这个特性为我们来构建交互文档。来试一试吧！ # Let's run a Rails web server$ rails s在浏览器中打开 http://localhost:3000/graphiql： 在左侧窗口，你可以输入一个 query 来执行，然后点击“play”按钮（或按下Ctrl/Cmd+Enter），即可在右侧窗口看到响应结果。点击右上角的“Docs”链接，你就可以浏览 schema。 来看下日志——我们想要知道当按下执行按钮时发生了什么。 请求被发送到GraphlController，其也是由graphql gem 的生成器添加到应用程序的。 看一眼GraphlController#execute方法： # app/controllers/graphql_controller. rbdef execute variables = ensure_hash(params[:variables]) query = params[:query] operation_name = params[:operationName] context = {  # Query context goes here, for example:  # current_user: current_user, } result = GraphqlSchema. execute(  query,  variables: variables,  context: context,  operation_name: operation_name ) render json: resultrescue StandardError =&gt; e raise e unless Rails. env. development? handle_error_in_development eend该方法调用了GraphqlSchema#execute方法，以如下参数：  query和variables分别代表一个 query 字符串和客户端发送的参数； context是一个任意 hash，在 query 执行的任何地方都是可用的； operation_name从进来的请求中取出一个命名操作来执行（可以为空）。所有的魔法都发生在这个方法内：它解析 query，检测所有将被用来构建响应的 type，并决定所有被请求到的字段。我们唯一需要做的事就是定义这些 type，并声明字段应该被怎样决定。 What’s in the Martian Library?: 让我们从“hello world”转到更真实的东西：从Types::QueryType移除范例内容并注册一个称为:items的字段，其将返回所有图书馆的 items。我们也需要为该字段添加一个 resolver 方法（resolver 方法名必须匹配字段名）： # app/graphql/types/query_type. rbmodule Types class QueryType &lt; Types::BaseObject  field :items,     [Types::ItemType],     null: false,     description:  Returns a list of items in the martian library   def items   Item. all  end endend每个字段定义都包含一个名称，一个其结果类型，及一些选项。:null是需要的，必须设为true或者false。我们也定义了可选的:description——为字段添加易于阅读的信息是一种好的实践：它会被自动添加到文档中，为开发者提供更多相关信息。对于结果类型的数组表示，[Types::ItemType]，意味着字段的值必须是一个数组，且其每个元素都必须是Types::ItemType类型。 但我们还没有定义ItemType，对吧？幸运的是，graphql gem 给了一个方便的生成器： $ rails g graphql:object item现在我们就可以修改新创建的app/graphql/types/item_type. rb为想要的样子了。 # app/graphql/types/item_type. rbmodule Types class ItemType &lt; Types::BaseObject  field :id, ID, null: false  field :title, String, null: false  field :description, String, null: true  field :image_url, String, null: true endend如上所见，我们在ItemType中暴露了三个字段：  非 null 的字段，id和title 可为 null 的字段description我们的执行引擎解析决定字段时是使用如下算法（略有简化）：  首先，它在 type 类自身内查找同名定义的方法（如同前面我们在QueryType中对items做的一样）；我们可以使用object方法来访问被解析决定的对象。 如果没有找到这样定义的方法，它就尝试在object上去调用同名方法。我们在 type 类中没有定义任何方法，因此假定底层实现了所有字段的方法。 回到http://localhost:3000/graphiql，执行如下 query，确认在响应中获取到了所有 items 的列表： { items {  id  title  description }}到目前为止，我们还没有添加任何体现 graph 威力的功能——当前的 graph 深度只有一层。让我们来添加一个非初始节点到ItemType上，让 graph 复杂一点。比如，添加一个user字段来代表 item 的创建者： # app/graphql/types/item_type. rbmodule Types class ItemType &lt; Types::BaseObject  # . . .   field :user, Types::UserType, null: false endend重复使用相同的生成器来创建一个新的 type 类： $ rails g graphql:object user这一次我们还想要添加一个计算字段——full_name： # app/graphql/types/user_type. rbmodule Types class UserType &lt; Types::BaseObject  field :id, ID, null: false  field :email, String, null: false  field :full_name, String, null: false  def full_name   # `object` references the user instance   [object. first_name, object. last_name]. compact. join(   )  end endend使用如下 query 来跟 items 一起获取 users： { items {  id  title  user {   id   email  } }}到这一步时，我们就可以把目光从后端移到前端了。让我们来为这个 API 构建一个客户端吧！ Configuring the frontend application: 正如已经提到的，我们推荐你安装Apollo 框架来处理客户端的 GraphQL。 要让一切顺利运转，我们需要安装所有需要的依赖库： $ yarn add apollo-client apollo-cache-inmemory apollo-link-http apollo-link-error apollo-link graphql graphql-tag react-apollo来看下所安装的一些库：  我们使用graphql-tag构建第一个 query。 apollo-client是一个通用的、与框架无关的库，来执行并缓存 GraphQL 请求。 apollo-cache-inmemory是一个 Apollo 缓存的存储实现。 react-apollo包含一套 React 组件来展示数据。 apollo-link与其他 links 给apollo-client的操作（你可以在这里找到更多细节）实现了一个中间件模式。现在我们需要为前端应用程序创建一个入口。从packs目录移除hello_react. jsx并添加index. js： $ rm app/javascript/packs/hello_react. jsx &amp;&amp; touch app/javascript/packs/index. js为了调试目的，加入如下内容： // app/javascript/packs/index. jsconsole. log('👻');生成一个用于前端的 controller： $ rails g controller Library index --skip-routes更新app/views/library/index. html. erb以包含 React 根元素及一个到 pack 的javascript_pack_tag： &lt;!-- app/views/library/index. html. erb --&gt;&lt;div id= root  /&gt;&lt;%= javascript_pack_tag 'index' %&gt;最后，在config/routes. rb注册一个新的路由： # config/routes. rbroot 'library#index'重启 Rails server，确认看到那个 👻 出现在浏览器的 console 中。 Configuring Apollo: 创建一个文件来存储 Apollo 的配置： $ mkdir -p app/javascript/utils &amp;&amp; touch app/javascript/utils/apollo. js该文件中，我们想要配置 Apollo 应用的两个核心东西，客户端和缓存（或更准确地说，是创建二者的函数）： // app/javascript/utils/apollo. js// clientimport { ApolloClient } from 'apollo-client';// cacheimport { InMemoryCache } from 'apollo-cache-inmemory';// linksimport { HttpLink } from 'apollo-link-http';import { onError } from 'apollo-link-error';import { ApolloLink, Observable } from 'apollo-link';export const createCache = () =&gt; { const cache = new InMemoryCache(); if (process. env. NODE_ENV === 'development') {  window. secretVariableToStoreCache = cache; } return cache;};让我们花一点时间来看看缓存是如何工作的。 每个 query 响应结果都被放到缓存中（相应的请求通常被用做缓存的 key）。在进行请求之前，apollo-client确保响应结果还未被缓存，而如果其已被缓存——请求就不会被执行。该行为是可配置化的：比如，我们可以为某一个特别请求关闭缓存，或者让客户端查找一个不同的 query 的缓存数据。 关于缓存机制，对本教程而言，一个我们需要了解的重要事情是，默认情况下，缓存的 key 是id和__typename的组合串。因此，获取同样对象两次也只会导致一个请求。 回到代码上来。由于我们使用 HTTP POST 作为传输，所以需要附带一个适当的 CSRF token 到每个请求上以通过 Rails 中的 forgery protection check。我们可以从meta[name= csrf-token ]拿到它（其是通过&lt;%= csrf_meta_tags %&gt;生成的）： // app/javascript/utils/apollo. js// . . . // getToken from meta tagsconst getToken = () =&gt; document. querySelector('meta[name= csrf-token ]'). getAttribute('content');const token = getToken();const setTokenForOperation = async operation =&gt; operation. setContext({  headers: {   'X-CSRF-Token': token,  }, });// link with tokenconst createLinkWithToken = () =&gt; new ApolloLink(  (operation, forward) =&gt;   new Observable(observer =&gt; {    let handle;    Promise. resolve(operation)     . then(setTokenForOperation)     . then(() =&gt; {      handle = forward(operation). subscribe({       next: observer. next. bind(observer),       error: observer. error. bind(observer),       complete: observer. complete. bind(observer),      });     })     . catch(observer. error. bind(observer));    return () =&gt; {     if (handle) handle. unsubscribe();    };   }) );来看下我们如何记录错误日志： // app/javascript/utils/apollo. js//. . . // log erorsconst logError = (error) =&gt; console. error(error);// create error linkconst createErrorLink = () =&gt; onError(({ graphQLErrors, networkError, operation }) =&gt; { if (graphQLErrors) {  logError('GraphQL - Error', {   errors: graphQLErrors,   operationName: operation. operationName,   variables: operation. variables,  }); } if (networkError) {  logError('GraphQL - NetworkError', networkError); }})生产环境上，更好的做法是使用异常追踪服务（exception tracking service）（比如，Sentry 或者 Honeybadger）：只用覆盖logError函数把错误发送到外部系统即可。 曙光在前了——让我们把入口告知客户端以进行查询： // app/javascript/utils/apollo. js//. . . // http linkconst createHttpLink = () =&gt; new HttpLink({ uri: '/graphql', credentials: 'include',})最后，我们就可以创建 Apollo 客户端的实例了： // app/javascript/utils/apollo. js//. . . export const createClient = (cache, requestLink) =&gt; { return new ApolloClient({  link: ApolloLink. from([   createErrorLink(),   createLinkWithToken(),   createHttpLink(),  ]),  cache, });};The very first query: 我们将使用provider pattern来把客户端实例传给 React 组件： $ mkdir -p app/javascript/components/Provider &amp;&amp; touch app/javascript/components/Provider/index. js这是我们第一次使用react-apollo的ApolloProvider组件： // app/javascript/components/Provider/index. jsimport React from 'react';import { ApolloProvider } from 'react-apollo';import { createCache, createClient } from '. . /. . /utils/apollo';export default ({ children }) =&gt; ( &lt;ApolloProvider client={createClient(createCache())}&gt;  {children} &lt;/ApolloProvider&gt;);修改index. js以使用新创建的 provider： // app/javascript/packs/index. jsimport React from 'react';import { render } from 'react-dom';import Provider from '. . /components/Provider';render(&lt;Provider&gt;👻&lt;/Provider&gt;, document. querySelector('#root'));如果你使用了Webpacker v3，则需要导入babel-polyfill以用上诸如 async/await等很酷的 JavaScript 特性。不用担心 polyfill 的大小。babel-preset-env会帮你移除掉所不需要的一起。 我们来创建一个Library组件，在页面上展示 items 的列表： $ mkdir -p app/javascript/components/Library &amp;&amp; touch app/javascript/components/Library/index. js我们会使用react-apollo的Query组件，接收query字符串作为 property 以获取所 mount 的数据： // app/javascript/components/Library/index. jsimport React from 'react';import { Query } from 'react-apollo';import gql from 'graphql-tag';const LibraryQuery = gql` {  items {   id   title   user {    email   }  } }`;export default () =&gt; ( &lt;Query query={LibraryQuery}&gt;  {({ data, loading }) =&gt; (   &lt;div&gt;    {loading     ? 'loading. . . '     : data. items. map(({ title, id, user }) =&gt; (       &lt;div key={id}&gt;        &lt;b&gt;{title}&lt;/b&gt; {user ? `added by ${user. email}` : null}       &lt;/div&gt;      ))}   &lt;/div&gt;  )} &lt;/Query&gt;);我们可以通过相应的loading和data property 分别访问载入状态和已加载数据（使用所谓的render-props 模式传递）。 别忘了把组件添加到主页面上： // app/javascript/packs/index. jsimport React from 'react';import { render } from 'react-dom';import Provider from '. . /components/Provider';import Library from '. . /components/Library';render( &lt;Provider&gt;  &lt;Library /&gt; &lt;/Provider&gt;, document. querySelector('#root'));如果你刷新页面，将会看到 items 列表，以及添加它们的用户的 email： 祝贺你！你刚刚迈出了通向 GraphQL 的第一步。很棒！ …And the very first problem: 一切看起来都工作得很好，但来看一眼我们的服务端日志： SQL 查询SELECT * FROM users WHERE id = ?被执行了四次，意味着我们撞上了著名的 N+1 问题——服务端对集合中的每个 item 都进行了一次查询，以获取相应的用户信息。 在修复这个问题之前，我们需要确保进行代码调整是安全的，不会搞坏任何东西——所以，来写测试吧，少年！ Writing some specs: 现在该来安装配置 RSpec 了，更准确地说，是rspec-rails gem： # Add gem to the Gemfile$ bundle add rspec-rails --version= 4. 0. 0. beta2  --group= development,test # Generate the initial configuration$ rails generate rspec:install为了易于生成测试数据，也安装上 factory_bot： $ bundle add factory_bot_rails --version= ~&gt; 5. 0  --group= development,test 为了让 factory 方法（create，build等）在测试中全局可见，添加config. include FactoryBot::Syntax::Methods到rails_helper. rb中。 由于我们在添加 Factory Bot 之前就创建了 model，所以我们得手动生成 factory。单独创建一个文件，spec/factories. rb，如下： # spec/factories. rbFactoryBot. define do factory :user do  # Use sequence to make sure that the value is unique  sequence(:email) { |n|  user-#{n}@example. com  } end factory :item do  sequence(:title) { |n|  item-#{n}  }  user endend现在已经准备好写我们的第一个测试了。来为QueryType创建一个 spec 文件： $ mkdir -p spec/graphql/types$ touch spec/graphql/types/query_type_spec. rb最简单的 query 测试，就像下面这样： # spec/graphql/types/query_type_spec. rbrequire  rails_helper RSpec. describe Types::QueryType do describe  items  do  let!(:items) { create_pair(:item) }  let(:query) do   %(query {    items {     title    }   })  end  subject(:result) do   MartianLibrarySchema. execute(query). as_json  end  it  returns all items  do   expect(result. dig( data ,  items )). to match_array(    items. map { |item| {  title  =&gt; item. title } }   )  end endend首先，我们创建在数据库中创建一对 items。然后，定义了要被测试的 query 和subject（result），后者通过调用MartianLibrarySchema. execute方法所得到。还记得我们在GraphqlController#execute那里有一行类似的代码吗？ 这个用例非常简单：我们对execute的调用既没有传递variables也没有传递context，当然，有需要的时候我们显然可以这么做。 现在，我们就有足够自信来修复上面的 N+1 问题了！ GraphQL vs. N+1 problem: 最简单的避免 N+1 问题的方式是使用 eager loading。我们这里，需要在进行查询以获取QueryType中的 items 时预加载用户： # /app/graphql/types/query_type. rbmodule Types class QueryType &lt; Types::BaseObject  # . . .   def items   Item. preload(:user)  end endend这个方案在简单的场景下是有用的，但并非十分高效：比如，如下代码也会预加载用户，即使客户端不需要它们时： items { title}要讨论解决 N+1 问题的其他方式，值得单独写一篇文章，已经超出了本文的范畴。 大多数解决方案都不外乎以下两种：  lazy eager loading（比如，使用 ar_lazy_preload gem） batch loading（比如，使用 graphql-batch gem）本文就到这儿了！我们学习了关于 GraphQL 的很多东西，完成了配置后端和前端应用程序的常规工作，进行了第一个查询，甚至还发现并修复了第一个 bug。而这只是我们旅程中微小的一步（尽管文章的篇幅并不微小）。我们会很快回来的，届时将推出如何使用 GraphQL 的 mutation 来操作数据，以及 subscription 来使数据保持最新的内容。敬请关注！ "
    }, {
    "id": 28,
    "url": "/2020/11/auto-run-mongodb-docker-in-replica-mode-update/",
    "title": "自动运行MongoDB Docker为Replica模式（新）",
    "body": "2020/11/10 - 在前一篇博客《自动运行MongoDB Docker为Replica模式》中，提到了如何自动实现让 MongoDB 在 docker 中以 Replica 模式运行的做法。但最近发现，当时的做法是有点问题的——如果跟其他 docker 镜像用docker-compose搭成环境后，其他的 docker 容器无法正常连接到该 MongoDB 使用。 经过一番查询，问题原因很简单。大概是由于docker-compose 在启动所有 docker 容器时，构建自身整体的 Network 需要一定时间，而 MongoDB 的 Replica 设置是立即读取生效的，所以 MongoDB 在被启动时还不能正确读取到 Network 的环境配置，从而造成其他容器无法连接到它。 最后，是通过一种比较取巧的方法来绕过了这个问题。即让 MongoDB 启动后不断检测其 Network 环境是否已经正确，直到当其准备好为止，这时再自动触发运行 Replica 模式的 MongoDB，就可以提供正常的服务连接了。 相关的Dockerfile配置文件和脚本如下： Dockerfile: ARG MONGODB_VERSIONFROM mongo:$MONGODB_VERSIONRUN apt-get update -qq &amp;&amp; apt-get install -yq netcatADD init_replicaset. js /docker-entrypoint-initdb. d/init_replicaset. jsADD resolve_replicaset_host. sh /root/resolve_replicaset_host. shADD https://raw. githubusercontent. com/vishnubob/wait-for-it/master/wait-for-it. sh /usr/local/bin/wait-for-itRUN chmod +x /usr/local/bin/wait-for-itCMD bash -c  mongod --replSet mongoreplset --bind_ip_all &amp;&amp; wait-for-it -t 0 mongo:27017 -- /root/resolve_replicaset_host. sh init_replicaset. js: rs. initiate({'_id':'mongoreplset', members: [{'_id':0, 'host':'127. 0. 0. 1:27017'}]});resolve_replicaset_host. sh: #!/bin/bashmongo --eval  rs. initiate({'_id':'mongoreplset','members':[{'_id':0,'host':'mongo:27017'}]}) 这里用到了 https://raw. githubusercontent. com/vishnubob/wait-for-it/master/wait-for-it. sh 这个工具，用来不断测试连接mongo:27017是否就绪，如果可以正确连接，就会自动运行/root/resolve_replicaset_host. sh这个脚本，设置 MongoDB 以 Replica 模式运行起来。 当然，还有最后和最重要的一步： 修改/etc/hosts，添加如下一行 127. 0. 0. 1 mongo最终测试结果一切工作正常。完美！ "
    }, {
    "id": 29,
    "url": "/2020/10/squash-n-plus-one-queries-in-rails/",
    "title": "在Rails中尽早碾碎N+1查询",
    "body": "2020/10/31 - 本文已获得原作者（Vladimir Dementyev）和 Evil Martians 授权许可进行翻译。原文介绍了 对于 Rails 中经典的 N+1 问题，我们通常使用的 Bullet 的局限性，以及如何运用 n_plus_one_control 以测试的方式来尽早发现 N+1 查询。  原文链接：Squash N+1 queries early with n_plus_one_control test matchers for Ruby and Rails 作者：Vladimir Dementyev 站点：Evil Martians ——位于纽约和俄罗斯的 Ruby on Rails 开发人员博客。 它发布了许多优秀的文章，并且是不少 gem 的赞助商。【正文如下】 引言: 发现一种面向测试的替代方案，在开发中的冗余数据库调用之前，就检测你的 Rails 和纯 Ruby 应用程序中的 N+1 查询问题。n_plus_one_control gem 的工作方式与 Bullet 等众所周知的工具不同，能确保额外的 SQL 查询永远不被忽略，且与你对 ORM 工具的选择无关。 我假定本文读者是了解 N+1 查询问题的。如果不是，可从我们的一篇介绍文章开始，该文展示了 Evil Martians 更早开发的另一个测试工具。 每个使用 ORM（如 Active Record）与数据库进行工作的后端 Ruby 开发者都知道：需要小心你的数据库查询，以免生成太多 Query。即使 N+1 问题众所周知，但我们仍然不可能每次都比 ORM 聪明：即我们需要一种自动化方式来指明问题。 在过去三年，我一直使用自己的小工具尽可能在更早的场合下就检测到 N+1 问题：在运行 RSpec 或 Minitest 的时候。我的 n_plus_one_control gem 一直在缓慢地逐步完善，最近来到了 0. 5. 0 版本，因此我觉得是时候向各位展示它，以及我的解决方案与其他常见工具的不同之处了。 Biting the bullet: 你可能想知道已经有了 Bullet gem 为什么我还要创建另一个库来检测 N+1 问题呢？让我来分享一个小故事。 我已使用 Bullet 许多年了：首先，是在开发环境和 staging 环境中，像我们大多数人一样。但在某个时候，我意识到测试环境更适合用来检测和防止 N+1 问题。通过一些小修小补，让 Bullet 在测试环境下工作起来是可行的。如下代码示例创建了一个可用于 spec 的bulletify帮助方法。 # spec/shared_contexts/bulletify. rbRSpec. shared_context  bullet  do before(:each) do  Bullet. enable = true  Bullet. bullet_logger = true  Bullet. raise = true # raise an error if N+1 query occurs  Bullet. start_request end after(:each) do  Bullet. perform_out_of_channel_notifications if Bullet. notification?  Bullet. end_request  Bullet. enable = false  Bullet. bullet_logger = false  Bullet. raise = false endendRSpec. configure do |config| config. include_context  bullet , bullet: true config. alias_example_to :bulletify, bullet: trueend# spec/controllers/my_controller_spec. rbcontext  N+1  do bulletify { get :index }end然而在多个成熟的 Rails 应用程序的工作中，我发现当数据库交互超越 Active Record 及其关联关系时，Bullet 并不能发现问题。False positives 也是相当的常见。 这有一个范例，使用了上面的bulletify方法： # user. rbclass User &lt; ApplicationRecord has_many :teams def supports?(team_name)  teams. where(name: team_name). exists? endend&lt;!-- users/index. html. erb --&gt;&lt;% @users. each do |user| %&gt; &lt;li class= &lt;%= user. supports?( FC Spartak Moscow ) &amp;&amp;  red-white  %&gt; &gt;  &lt;%= user. name %&gt; &lt;/li&gt;&lt;% end %&gt;# users_controller_spec. rbdescribe  GET #index  do render_views let!(:users) { create_pair(:user, :with_teams) } bulletify { get :index }end即使我们在这儿明显有了 N+1 查询（来自于#supports?方法），测试却仍然是绿色通过的。 Running and counting: 我就开始考虑一种检测 N+1 查询的更健壮的方式，与 Active Record 内部无关的方式。 而最终我找到了这样的思路：如果我们运行同样的代码两次，以不同数量的数据库记录，并比较所执行 SQL 查询的次数呢？ 如果这个数字不依赖于测试集的大小，我们便没有（N+1）问题。否则，看起来就是有一个 XN+Y 了。 这个想法在 n_plus_one_control gem 中得以实现，它为你所选择的 Ruby 测试框架添加 N+1 检测的 DSL。下面是 RSpec 的示例： context  N+1 , :n_plus_one do # Populate block is called multiple times # with different values of n (2 and 3 by default) populate { |n| create_list(:user, n, :with_teams) } specify do  # The example body is executed  #after each `populate` call  expect { get :index }. to perform_constant_number_of_queries endend而测试将以如下消息报失败： Expected to make the same number of queries, but got: 3 for N=2 4 for N=3Search and destroy N+1 violations: 最新版本中，我把重点放在了开发体验上，帮助开发者不仅检测 N+1 问题，还能方便地在大型应用程序中找到惹出问题的“元凶”。 当处理优化工作时，我认识到仅仅拥有测试因 N+1 而报失败是不够的（能知道这问题的存在要感谢性能监测系统）。所以，我已经开始为这个 gem 添加新的特性，即以如下计划发现 N+1 问题： Step 1: Write a test that fails: 创建一个好的性能测试可不容易。我们需要所涉及的数据，各种设置，来覆盖多数代码路径，这样才不会遗失任何不期望的数据库交互。比如： # This example uses Minitestclass PerformanceTest &lt; ApplicationIntegrationTestCase def populate(scale_factor)  scale_factor. times do   # Here, I'm not using create_list but introducing some   # randomness instead.    # That would make our setup less deterministic   # (and, thus, test more valuable)   create(:resource, :with_tags, tags_num: [0, 1, 2]. sample)  end  create_list(:document, scale_factor) end test  should not produce N+1 queries  do  assert_perform_constant_number_of_queries do   get :index  end endend输出看起来会是这样： Expected to make the same number of queries, but got: 10 for N=2 11 for N=3Unmatched query numbers by tables: resources (SELECT): 2 != 3 permissions (SELECT): 4 != 6注意：即使我们没有看到上面的documents 列表，也不要移除设置中的这个部分：N+1 问题可能会在将来遇到，所以我们应该做好准备。 Step 2: Localize the problem: 根据上面的错误信息，我们可以看到有两个影响到的表。让我们通过使用查询筛选的特性，来对每个测试单独重新运行。我们也可启用明细输出模式以获得所收集到的查询列表及其来自哪里： $ NPLUSONE_VERBOSE=1 \ NPLUSONE_FILTER=resources \ bundle exec rails testExpected to make the same number of queries, but got: 2 for N=2 3 for N=3Unmatched query numbers by tables: resources (SELECT): 2 != 3Queries for N=2  SELECT  resources . * FROM  resources  WHERE  resources .  deleted_at  IS NULL  ↳ app/controllers/resources_controller. rb:32:in `index'  . . . Queries for N=3  . . . 再重复第二个表的： $ NPLUSONE_VERBOSE=1 \ NPLUSONE_FILTER=permissions \ bundle exec rails testExpected to make the same number of queries, but got: 4 for N=2 6 for N=3Unmatched query numbers by tables: permissions (SELECT): 4 != 6Queries for N=2 SELECT  permissions . * FROM  permissions  WHERE  permissions. user_id  = 42 AND  permissions. resource_id  = 15 AND  permissions. grants  @&gt; '{manage}' ↳ app/policies/resource_policy. rb:41:in `update?' SELECT  permissions . * FROM  permissions  WHERE  permissions. user_id  = 42 AND  permissions. resource_id  = 15 AND  permissions. grants  @&gt; '{invite}' ↳ app/policies/resource_policy. rb:56:in `invite?' . . . Queries for N=3 . . . 有时，查询会太长，让我们的输出难以阅读。那么我们可以通过设置NPLUSONE_TRUNCATE环境变量只显示查询的前 N 个字符。 还有，仅显示堆栈的单独一行可能没那么有用。不用担心！你可以通过NPLUSONE_BACKTRACE变量来增加堆栈行数显示的数量。 因此，最终的命令看起来会是这样： NPLUSONE_VERBOSE=1 \NPLUSONE_FILTER=permissions \NPLUSONE_TRUNCATE=100 \NPLUSONE_BACKTRACE=5 \bundle exec rails test总结一下，通过n_plus_one_control，你可以通过编写测试并以不同参数运行多次来快速识别出任何一种 N+1 查询的根源。而在你修复所有这些问题后，这种检测就成为一种回归测试——阻止你的代码在将来再出问题的测试！ "
    }, {
    "id": 30,
    "url": "/2020/10/build-docker-with-private-ssh-key/",
    "title": "Build Docker时使用SSH Private Key的新玩法",
    "body": "2020/10/17 - 在实际工作中，Build Docker 镜像时，经常碰上需要在 Docker 镜像内用到 SSH Private Key 的场景。比如 Docker 镜像内要从 GitHub、GitLab 的私有库 Clone 代码，或者要安装私有库的Gem、NPM Package等。而如果直接把自己的 SSH Private Key 打包到 Docker 镜像中的话，是存在很大安全风险的。如何解决这个问题？ 在之前，我们往往是通过 Squash 或者 Docker Multi Stage 等方式来处理，但都有各自不足之处。而 Docker 在 18. 09 版本后，推出了 BuildKit 的 SSH mount type。我们就可以用这个特性来解决了。 首先，需要在 Dockerfile 的顶部添加这样一行来开启该实验特性： # syntax=docker/dockerfile:1. 0. 0-experimental然后，在 Dockerfile 中需要使用 SSH Private Key 的地方都加上 --mount=type=ssh比如，Rails 项目安装有私有库的 Gem 包时，就写成这样： RUN --mount=type=ssh bundle install以此类推。Dockerfile 准备好之后，再设置一下环境变量以启用 BuildKit： export DOCKER_BUILDKIT=1最后，即可使用如下命令来 Build Docker 镜像了： docker build --ssh default=/Users/xxxxx/. ssh/id_rsa -f Dockerfile -t rails-demo-dev:1. 0. 0 . 搞定。 这种方式，既能正常使用上 SSH Private Key，又能使其在镜像中不留痕迹。完美！ 参考阅读：  https://medium. com/@tonistiigi/build-secrets-and-ssh-forwarding-in-docker-18-09-ae8161d066 https://medium. com/@amimahloof/securely-build-small-python-docker-image-from-private-git-repos-c3e6d5da4626"
    }, {
    "id": 31,
    "url": "/2020/09/auto-run-mongodb-docker-in-replica-mode/",
    "title": "自动运行MongoDB Docker为Replica模式",
    "body": "2020/09/22 - 最近工作中碰到一个有意思的问题：需要让 MongoDB 的 Docker 镜像以 Replica 模式运行起来。而官方 MongoDB 的 Docker 镜像默认是单机而非 Replica 模式。当然，如果你启动 Docker 后再进入容器内，手动修改配置生效，肯定能搞定。但毫无疑问这是效率很低的做法。如何让这一切自动实现，而不是每次重新运行 MongoDB 都手工去改？ 其实，官方 MongoDB Docker 页面已经指出了解决方向。如下图： 意思是说在启动时，位于/docker-entrypoint-initdb. d目录下的. sh, . js文件都会被自动作为配置读取生效。 所以，我们需要本地 Build 一下 MongoDB，并先写好所需要的配置文件，在 Build 时把该配置文件放入上述目录内，最后生成的镜像就能实现自动 Replica 模式的启动。 直接上最终的Dockerfile： ARG MONGODB_VERSIONFROM mongo:$MONGODB_VERSIONADD . /dockerize/mongo/init_replicaset. js /docker-entrypoint-initdb. d/init_replicaset. jsCMD [ mongod ,  --replSet ,  mongoreplset ,  --bind_ip_all ]和配置文件init_replicaset. js： rs. initiate({'_id':'mongoreplset', members: [{'_id':0, 'host':'127. 0. 0. 1:27017'}]});就是这样了。 "
    }, {
    "id": 32,
    "url": "/2020/08/testprof-chinese-doc-is-online/",
    "title": "TestProf文档中文版翻译完成上线",
    "body": "2020/08/18 - 我在前几篇博客中翻译推荐了关于 TestProf 的一些使用方法和技巧，这个 Evil Martins 出品的 Ruby 测试工具 Gem 的强大和有趣，从中可窥一斑。要想了解和使用 TestProf 的全部功能，当然还是需要去看它的官方文档（地址：https://test-prof. evilmartians. io）。顺便说一句，连这个 Gem 文档的网站都秉承了 Evil Martins 的一贯风格，同样的精致，同样的讲究设计感。 为了更好地推荐给国内的 Ruby 开发者，于是我有了一个大胆的想法。我给 TestProf 作者 Vladimir Dementyev 发了邮件，向他询问关于把 TestProf 的文档进行中文化的意向。Vladimir 技术很厉害，没想到人也非常好沟通。作为作者，当然他也是希望把文档翻译成更多的本地化语言，好推广给更多的 Ruby 开发者了。不过他同时回复说文档网站目前还不支持多语言，需要先调研一下，尽快告诉我结果。 而他只用了两个晚上，就搞定了把原本只支持英文的文档网站，扩展升级为支持多语言的版本，并且添加了全部文档的俄语版本进行验证。既然文档的网站一切准备就绪，那么接下来就等我的中文化翻译了。 翻译工作从 2020. 07. 24 开始，到 2020. 08. 12 完成最后一篇，共耗时 12 天。 中文版本来可以那时就上线的，不巧的是，正好碰上是 Vladimir 的假期，只能等他回来再处理。 今天，TestProf 文档的中文版终于正式放出，可供所有 Ruby 开发者查阅了。效果如下： Vladimir 也在 Twitter 上发布了中文版文档上线的消息:) 衷心希望 TestProf 的中文文档能够帮助所有国内的 Ruby 开发者把测试写得更加高效，也让编写测试成为一种乐趣，从而让 Ruby/Rails 程序更加完美！ 附注TestProf 中文文档的 GitHub 地址是：https://github. com/test-prof/docs-zh-cn， 欢迎各位小伙伴针对文档中翻译不准确甚至有误的地方提 issue 给予反馈。 "
    }, {
    "id": 33,
    "url": "/2020/08/ports-and-adapters-pattern-part3/",
    "title": "“端口—适配器”模式的概念(3)",
    "body": "2020/08/13 - 这是“端口—适配器”模式的概念梳理第三部分。 6. - PROS AND CONS: 模块化和应用与技术的解耦，是六边形架构两个重要的特征。这些特征是优点和缺点的来源。下面是我发现的一些关于六边形架构的利与弊。 6. 1 - PROS: 6. 1. 1 - TESTABILITY IMPROVEMENT这个架构所提供的主要收益就是能够独立于其依赖的外部设备来测试应用程序。这通过两件事来实现：  对于每个驱动者端口，开发一个测试适配器以在该端口上测试其用例。 对于每个从动者端口，开发一个 mock 适配器。对六边形做隔离性测试对于这些很有用：  运行回归测试。当源代码因某种原因变更时（添加了新功能，修复了一个 bug，等等），这些测试运行来确保如上改动没有在任何已存在功能上的边际效应。要运行这些测试，驱动者适配器使用一个自动化测试框架。 做 BDD (行为驱动开发)。对于每个驱动者端口的功能，一套验收标准是由用户定义的。当所有验收标准都吻合时，该功能就被看作“完成了”。这些验收标准被称为 scenarios，将被测试用例通过测试适配器来运行。要运行这些验收测试，适配器可以使用如 Cucumber 的工具。Nat Pryce（Growing Object-Oriented Software, Guided by Tests 一书的合著者）在他的文章 Visualising Test Terminology 中定义了有关六边形架构各种不同的测试：  单元测试（Unit Tests）：测试六边形内的单个对象。 集成测试（Integration Tests）：测试适配器。它们确保在端口和外部世界之间的转换通过适配器正确地完成了。 验收测试（Acceptance Tests）：测试驱动者端口，比如，隔离中的六边形。它们检查应用程序有如用户预期那样的行为，符合他/她在之前用例中所定义的验收标准。 系统测试（System Tests）：一起测试整个系统，适配器和六边形。它们也测试系统的部署和启动。6. 1. 2 - MAINTAINABILITY IMPROVEMENT可维护的系统是那种易于修改的系统。六边形架构提升了可维护性，因为它提供了关注点跟业务逻辑解耦的分离，使得查找我们要更改的代码更加容易。 应用程序的可维护性是与技术债相关的长期概念。 可维护性越高，技术债就越少。因此，六边形架构减少了技术债。 6. 1. 3 - FLEXIBILITY在不同的技术之间切换是容易的。对一个给定的端口，你可以有多个适配器，每个使用特定的技术。要选择使用其中一个，你只用配置好哪个适配器用于该端口即可。这种配置可以如同调整一个外部配置参数文件一样容易。不用修改源代码，不用重新编译，不用重新构建。 同样地，把一个新的特定技术的适配器添加到一个端口可以不用触及现有代码即可做到。这个适配器开发和编译它自己的。运行时，它会被检查并插入到该端口。 6. 1. 4 - APPLICATION IMMUNE TO TECHNOLOGY EVOLUTION技术远比业务逻辑更频繁地进化着。在业务逻辑跟技术绑在一起的应用程序中，你不能进行技术变更而不触碰业务逻辑。这并不好，因为业务不应改变。 使用六边形架构，你想要升级的技术是位于应用程序外部的适配器中。你只用更改适配器就好。应用程序本身保持不变性，因为它不依赖于适配器。 6. 1. 5 - DELAY TECHNOLOGICAL DECISIONS当你开始开发写代码时，会只专注在业务逻辑上，而推迟决断将要使用哪个框架和技术。你可以晚一点来选择一种技术，并为其编写适配器。 6. 2 - CONS: 6. 2. 1 - COMPLEXITY一个实现了六边形架构的软件项目有复杂的结构，使用了很多的模块和定义在它们之间的明确依赖。我所说的模块是指源代码子项目（比如，Maven 模块），用于物理上分离架构的不同元素。 至少，会有一个用于六边形的模块，每个适配器一个模块，以及一个模块用于启动整个项目。你也必须定义模块之间的依赖：六边形不依赖任何东西，适配器依赖于六边形，而启动是依赖所有其他东西的。 如果编程语言不允许六边形只暴露端口，那么就会有甚至更多的模块了。你不得不把六边形拆分为端口及实现。六边形的实现和适配器将依赖于端口，而端口不依赖任何东西。 6. 2. 2 - BUILD PROCESS PERFORMANCE由于我们已经看到的复杂性，如果项目太大，有太多适配器，那么编译、运行测试、把所有模块构建到一起，以及启动整个项目的过程将会花费许多许多的时间。 6. 2. 3 - INDIRECTION AND MAPPINGS通过端口和适配器把应用程序跟技术解耦会增加间接性，比如，当适配器在端口和特定技术接口之间转换时对方法的额外调用。除此之外，可能还需要在应用程序跟外部世界对象之间进行映射。 7. - WHEN TO “HEXAGONAL THIS!”: 或者你会这样说：“什么时候应该在项目上应用六边形架构？”好吧，答案恐怕令人不快：“得看情况”：  对于小项目，也许“治愈胜过疾病”，所以解决琐碎的问题不值得使用该架构来增加额外的复杂性。 对于中等/大型项目，有很长的生命周期，在其生涯中会进行多次修改，使用六边形架构就非常值得。有些人可能说如果他们确认项目中要用的技术或框架不会变更（比如，由于某些原因被绑定到了特定技术），就不需要六边形架构了。好吧，即使在这种情况下，“端口-适配器”模式也很有用，因为你可以添加 mock 适配器，以便在应用程序所依赖的设备/服务不可用时使用，或者你可以为不同的运行时环境（开发、测试、生产）添加适配器。 8. - IMPLEMENTATION STEPS: 起点是应用程序作为一个黑盒，所定义的端口接口围绕其四周，无论是在驱动者侧还是从动者侧，以此与外部世界进行交互。 起初可能你无法完整定义每个从动者端口，因为你仍然不完全了解应用程序对端口用途的所有需求。或者你可能错过某些从动者端口。但这些需求将会随着六边形内的开发而产生，比如，驱动者端口的实现。 所以，对于从头开发一个六边形应用程序，下面是适配器在驱动者与从动者两侧被构造和添加的顺序，直到完成所有工作： 8. 1 - TEST DRIVER ADAPTERS / MOCK DRIVEN ADAPTERS:  驱动者侧（Driver side）：对于每个驱动者端口，构造一个测试适配器，并由测试来驱动实现该驱动者端口。BDD 可被用于此处以实现驱动者端口，而测试用例会是 GWT scenarios。 从动者侧（Driven side）：当在实现一个驱动者端口时，你可能需要使用从动者端口。这样的情况下，为其构造 mock 适配器。一旦你实现了所有的驱动者端口和 mock 从动者端口，这一步就完成了。 此时，六边形是完整的，有测试在驱动者侧，mock 在从动者侧。应用程序是可以被隔离性测试的。 下一步是为每个端口添加所需的“真实”的驱动者适配器和从动者适配器，这依赖于外部世界的通信需求。例如，你可能需要 Web UI 和 REST API 适配器在驱动者侧，而 SQL 数据库和 app-to-app 适配器在从动者侧。 8. 2 - REAL DRIVER ADAPTERS / MOCK DRIVEN ADAPTERS:  驱动者侧（Driver side）：对于每个驱动者端口，构造并添加你需要的“真实”的驱动者适配器。例如，Web UI, REST API，等等。 从动者侧（Driven side）：保留你在步骤一中构造的 mock 适配器。这样你就可以测试新的驱动者适配器了。 8. 3 - TEST DRIVER ADAPTERS / REAL DRIVEN ADAPTERS:  驱动者侧（Driver side）：配置每个驱动者端口以使用在步骤一中构造的测试驱动者适配器。 从动者侧（Driven side）：对于每个从动者端口，构造并添加你需要的“真实”的从动者适配器。例如，数据库适配器，邮件提醒适配器，等等。这样你就可以测试新的从动者适配器了。 8. 4 - REAL DRIVER ADAPTERS / REAL DRIVEN ADAPTERS:  驱动者侧（Driver side）：配置每个驱动者端口使用在步骤二中构造的“真实”的驱动者适配器。 从动者侧（Driven side）：配置每个从动者端口使用在步骤三中构造的“真实”的从动者适配器。这样你就可以对应用程序进行端到端测试了，在驱动者侧和从动者侧都包含“真实”的适配器。 到这一步你就全部完成了。你可以用期望的适配器来配置每个端口，并以端口与适配器配置的任意组合来运行应用程序。 "
    }, {
    "id": 34,
    "url": "/2020/08/ports-and-adapters-pattern-part2/",
    "title": "“端口—适配器”模式的概念(2)",
    "body": "2020/08/12 - 这是“端口—适配器”模式的概念梳理第二部分。 3. - CONFIGURABLE DEPENDENCY PATTERN: 配置依赖 是 依赖注入 的一种概括，也被称为控制反转。 它是 Gerard Meszaros 起的一个新名称。Alistair Cockburn 认为它是该模式的更好名称，因为它是一种特性，一种特征。“依赖注入”不是属性，它是一个操作，它是你实现配置依赖的一种东西。“控制反转”是一个双重否定，类似于先把事情做错然后反转它们。 “配置依赖”符合模式的定义： “事物的模式是事物良好范例的共同特征”。 我们可以说配置依赖是对象在接口之上的一种依赖。这个接口将是对象构造器的参数。然后，运行时，在实例化对象的时候，该接口的一个特定实现被传给该构造器。 六边形架构把配置依赖模式应用于驱动者和从动者两侧。在“参与者-应用程序”的交互中，开始对话的一方必须知道另一方，以便了解要跟谁通话，比如，它必须在另一方实现的接口上具有一个配置依赖：  驱动者侧：对话由驱动者（主角参与者）开始，所以驱动者适配器在驱动者端口上有一个配置依赖，其是由应用程序实现的一个接口。 从动者侧：对话由应用程序开始，所以应用程序在从动者端口上有一个配置依赖，其是由配角参与者的从动者适配器实现的一个接口。所以依赖如下图这样：  一个驱动者适配器依赖于六边形（它使用一个驱动者端口的接口）。 一个从动者适配器依赖于六边形（它实现一个从动者端口的接口）。 六边形不依赖任何东西，可能只跟编程语言工具有关。配置依赖是“端口—适配器”架构所基于的最重要模式，因为它使得六边形与任何技术解耦。而这种解耦使得该架构的主要目标成为可能，就是说，拥有了一个由多个驱动者运行，并能与 recipients/repositories 进行隔离性测试的应用程序。 4. - FROM “SYMMETRICAL ASYMMETRY” TO “ASYMMETRICAL SYMMETRY”（从“对称的不对称”到“不对称的对称”）: 听起来相当令人困惑吧？如果是，那就对了，这正是我想要的效果。让我来解释一下。 当该模式被写于 2005 年时，作者想要展示的是传统分层架构的不对称（用户侧 vs 数据侧），实际上是对称的。他通过画出一个六边形并把 UI 和数据库置于外侧来表示这种对称。数据库如同 UI 一样，只是技术，应用程序不了解它，正如同不了解 UI。 通过为其创建一个从动者端口，然后通过创建一个实现该端口的从动者适配器来应用配置依赖模式，把应用程序与数据库解耦。这样数据库依赖于应用程序，而不是其他方式。 然后，当实现六边形架构范例时，作者认识到这种对称实际上是不对称的，因为配置依赖模式在从动者侧和驱动者侧是不同的。 在从动者侧，应用程序必须了解从动者适配器，因为是应用程序开启对话。它必须了解要跟谁对话。 而在另一侧，驱动者侧，是必须了解应用程序的驱动者适配器来开启对话。应用程序不知道是哪一个驱动者来驱动它的。 所以结论是什么？“端口-适配器”对称还是不对称？ 其必然结果就是“对称”和“不对称”共存：  对称：所有的适配器，包括驱动者和从动者，都依赖于六边形。在两侧，应用程序都是跟技术无关的。 不对称：配置依赖的实现在每一侧是不同的。在驱动者侧，应用程序不了解是哪个适配器在驱动它。但在从动者侧，应用程序必须了解它要跟哪一个从动者适配器对话。5. - MISCONCEPTIONS: 5. 1 - LAYERED ARCHITECTURE: 很多我读过的关于该架构的文章都说其是分层的。它们说到了三个层：领域（domain）、端口（ports）、适配器（adapters）。我不知道它们为什么说这些，这个模式没有说到任何分层。该模式只说我们有一个应用程序（六边形），带有给定数量的端口，而对每个端口我们可以有不同的适配器，每个使用一种技术。一个字都没有说到分层。 5. 2 - WHY A HEXAGON?: 一些人想知道为什么是一个六边形，可能认为边的数量很重要。答案是否定的，根本不用在意这个。六这个数字完全不重要。无论如何，如果你好奇的话，这里有一些 Alistair Cockburn 关于为什么选择六边形的原因：  你有了足够的空间来描绘所需要的端口和适配器。 形状应指示内部/外部的不对称性，而不是顶部/底部或左侧/右侧的。 那么正方形不合适。 五边形，七边形，八边形，……都很难绘制。 因此，六边形赢了。5. 3 - PORTS OUTSIDE ADAPTERS: 我看到很多图把端口置于适配器之外，这样参与者就直接与端口进行交互，而适配器就是端口跟六边形之间的中间件了。这种方式： 参与者 ==&gt; 端口 ==&gt; 适配器 ==&gt; 六边形 根本是不正确的。 端口是六边形的边界。实际上，它们属于六边形，是其一部分，是六边形的接口。参与者通过适配器与六边形（的端口）进行交互。适配器是参与者跟端口之间的中间件。正确的描绘是： 参与者 ==&gt; 适配器 ==&gt; （端口）六边形 对于驱动者侧和从动者侧都是如此。这就是“端口-适配器”模式的对称性。 "
    }, {
    "id": 35,
    "url": "/2020/08/ports-and-adapters-pattern-part1/",
    "title": "“端口—适配器”模式的概念(1)",
    "body": "2020/08/10 - 最近看了下 DDD（领域驱动设计）和在 Go 中的应用。目前看来，整洁架构（Clean Architecture）和 “端口-适配器架构”（Ports and Adapters Pattern，又叫六边形架构 Hexagonal Architecture）是相对比较成熟的方案了。而后者的概念比较复杂一些，于是在概念上进行一些梳理和澄清。本文是第一部分。 1. - INTRODUCTION: “端口—适配器”是一个对象结构模型，由 Dr. Alistair Cockburn 在 2005 年所写的一篇文章中提出。 如果你在想……“这文章不会太老了点吧？在软件开发持续进化，新技术或新框架每天都层出不穷，干掉我们昨天还在用着的这个时代，它还有什么价值？”很好，答案就在问题之中。“端口—适配器”是一个促进从技术和框架中解耦的模式。所以答案是不，它并不过时。好东西是永恒的。就像酒一样，愈陈愈香。 “端口—适配器”的主要思想是定义了应用程序的架构，这样它就能通过不同类型的客户端（人、测试用例、其他应用，等等）来运行，而且它能被从应用程序所依赖的真实世界的外部设备（数据库、服务端、其他应用等）来隔离测试。 让我们来看看是如何做到的。 2. - THE ARCHITECTURE: 这一节我们会看到“端口—适配器”模式的要素和它们之间的关系。 2. 1 - THE HEXAGON: “端口—适配器”模式把应用程序描述为一个封闭区域。 该封闭区域在由 Alistair Cockburn 来描绘应用程序时选择的是一个六边形，这也是该模式被称为“六边形架构”的原因。 我个人更喜欢“端口—适配器”的名称，因为它指出了架构的关键要素，如同我们将要看到的。另一方面，用来描绘应用程序的图形并不是那么重要。然而，看起来“六边形架构”的名称更加广为人知。 六边形是应用程序自身。说“六边形”跟说“应用程序”是一个东西，从现在起二者会被任意使用。 在六边形内，我们只放置那些应用程序尝试解决的重要业务问题。 六边形包含业务逻辑，而不关心任何技术，框架或者真实世界中的设备。因此应用程序是与技术无关的。 “端口—适配器”模式并未说明六边形内部结构的任何东西。你可以使用分层，你可以使用特性组件，你可以使用意大利面条式的代码，你可以使用“大泥球”，你可以使用 DDD 战术模式，你可以使用简单的 CRUD。一切取决于你。 2. 2 - ACTORS: 在六边形外，是任何应用程序与之交互的真实世界的东西。这些东西包括人，其他应用程序，或者任何硬件/软件设备。它们是“参与者”。我们可以说这些参与者是应用程序的环境。 参与者被环绕分布在六边形外，根据“谁”触发了应用程序和参与者之间的交互：  位于左/上一侧的参与者是“驱动者”（Drivers），或者主角（Primary Actors）。交互由该参与者触发。驱动者就是与应用程序交互以达成一个目标的参与者。驱动者是应用程序的使用者（无论人或设备）。 位于右/下一侧的参与者是“从动者”（Driven Actors），或者配角（Secondary Actors）。交互由应用程序来触发。从动者提供应用程序所需的某些功能以实现业务逻辑。有两种类型的从动者：  Repository：应用程序除了向其发送信息外，也可以从其获取信息。例如，数据库或任何其他存储设备。 Recipient：应用程序只向其发送信息，然后就忘掉它了。例如，SMTP 服务器发送邮件。下图展示了在驱动和从动两侧的一些参与者的示例： 这些关于主（驱动者）和次（从动者）参与者的概念涉及到了使用场景。 所以，要知道在应用程序跟参与者交互中，该参与者是哪种类型，问问你自己，“谁”触发了对话。如果答案是参与者，它就是驱动者。如果答案是应用程序，那么该参与者就是从动者。 2. 3 - PORTS: 参与者与应用程序之间的交互根据它们与应用程序交互的原因而组织在六边形的边界处。 具有给定目的/意图的每组交互就是一个端口。 端口应根据其用途而不是任何技术来命名。 因此，为了命名端口，我们应使用以“ing”结尾的动词，并应说“此端口用于…ing某物”。例如：  这个驱动者端口是用于“添加（adding）商品到购物车”。 这个从动者端口（repository）是用于“获取（obtaining）订单信息”。 这个从动者端口（recipient）是用于“发送（sending）提醒”。端口是应用程序的边界，上面的图例中端口就是六边形的边。从外部世界，参与者只能跟六边形的端口交互，它们不应该访问到六边形的内部。端口是应用程序提供给外部世界的接口，允许参与者与应用程序交互。所以，应用程序应该遵循Information Hiding Principle。一件值得注意的重要事情是，端口属于应用程序。 驱动者端口为外部世界的驱动者提供了应用程序的功能。因此，驱动者端口被认为是应用程序的用例边界。 它们是应用程序的 API。 依赖于功能分组时所采用的粒度，我们可以有一个端口作为很多用例的接口，或只作为少数用例的接口。如果我们期望遵循Single Responsibility Principle，那么将会有很多端口，每个针对一个用例。这种场景下一个更好的选择是为端口使用 command bus 设计模式，对每个用例使用一个 command handler。同样的思想可以被用在查询上，这样我们将也会喜欢 CQRS 模式。我们会有一个端口负责执行命令，另一个端口负责执行查询。 从动者端口是功能的接口，该功能是应用程序实现业务逻辑所需的。这些功能是由从动参与者提供的。所以从动者端口是应用程序所需的 SPI（Service Provider Interface）。从动者端口会类似于 Required Interface。 2. 4 - ADAPTERS: 参与者通过使用特定技术的适配器与六边形的端口进行交互。一个适配器就是一个软件组件，允许用一种技术与六边形的端口来交互。给定一个端口，对于我们想要使用的每种技术都可以有一个适配器。适配器是在应用程序外部的。 驱动者适配器使用驱动者端口的接口，把一种特定技术的请求转换为对驱动者端口技术无关的请求。 下图展示了某些驱动者适配器的示例：  一个自动化测试框架：把测试用例转换为对驱动者端口的请求。 一个 CLI：把输入文本转换到 console 中。 一个桌面应用程序 GUI：通过图形组件转换被触发的事件。 一个 MVC Web 应用程序：Controller 接收从 View 中由用户请求的行为，并把其转换为对驱动者端口的请求。 一个 REST API controller：转换 REST API 请求。 一个事件订阅者：把来自消息队列的消息（事件）转换到所订阅的应用程序。对于每个驱动者端口，都应该至少有两个端口：一个用于真实驱动者的执行，另一个用于测试该端口的行为。 从动者适配器实现一个从动者端口的接口，把该端口与技术无关的方法转换为特定技术的方法。 下图展示了从动适配器的某些示例：  一个 mock 适配器：模仿真实配角的行为。例如，内存数据库。 一个 SQL 适配器：实现一个从动者端口以通过 SQL 数据库持久化数据。 一个 email 适配器：实现一个从动者端口以通过发送邮件提醒人们。 一个 App-To-App 适配器：实现一个从动者端口以通过向远程应用程序请求来获取某些数据。 一个事件发布者：实现一个从动者端口以通过发送事件到消息队列来发布它们，这样它们对订阅者就可用了。对于每个从动者端口，我们都应该编写至少两个适配器：一个用于真实世界的设备，另一个是 mock，用于模拟真实行为。 适配器最终所做的就是把一个接口转换到另一个，所以我们可以使用 Adapter Design Pattern 来实现它。 每个端口要使用哪种适配器是在应用程序启动时要配置的。这赋予了该模式以灵活性，因此我们能够在每次运行应用程序时从一种技术切换到另一种。如果针对从动者端口我们选择了一个测试驱动者和 mocks 适配器，应用程序就可以进行隔离性测试。 2. 5 - SUMMARY: 正如我们看到的，该架构的要素有这些：  六边形 ==&gt; 应用程序     驱动者端口 ==&gt; 应用程序提供的 API   从动者端口 ==&gt; 应用程序所需的 SPI    参与者 ==&gt; 与应用程序交互的环境设备     驱动者 ==&gt; 应用程序的用户（无论人还是硬件/软件 设备）   从动者 ==&gt; 提供应用程序所需的服务    适配器 ==&gt; 调整特定技术以适配应用程序     驱动者适配器 ==&gt; 使用驱动者端口   从动者适配器 ==&gt; 实现从动者端口   除了这些要素之外，还有一个 Composition Root（也被 Robert C. Martin 称为“Main Component”，在他的 Clean Architecture: A Craftsman’s Guide to Software Structure and Design 书中）。这个组件会在启动时运行并构建整个系统，会做如下事情：  它初始化并配置环境（数据库、服务，等等） 对于每个从动者端口，它选择一个实现了该端口的从动者适配器，并创建一个该适配器的实例。 它创建一个应用程序的实例，将从动者适配器的实例注入到应用程序构造函数中。 对于每个驱动者端口：     它选择一个使用该端口的驱动者适配器，并创建其实例，把应用程序的实例注入到该适配器构造函数中。   它运行该驱动者适配器的实例。   2. 6 - EXAMPLE: 一个 Web 接口的简单应用程序，由公司雇员使用来相互分配任务。当一个雇员被分配一个任务时，应用程序向他/她发送一封邮件。 "
    }, {
    "id": 36,
    "url": "/2020/07/github-special-readme-appear-on-profile/",
    "title": "GitHub个人页面的新玩法",
    "body": "2020/07/27 - 最近 GitHub 推出了一个新功能，让你可以展现更加个性的 Profile 页面。简单来说，就是可以创建一个与自己 GitHub 账号同名、且包含README. md的 Repo，那么这个README. md的内容将会自动显示在你的个人 Profile 页面上，而且是在顶部展示。这个新功能一下子引发了大量开发者的极大兴趣，各自八仙过海各显神通，把这个东西玩出了“花”。 比如我自己的 GitHub 账号是xfyuan，就可以在自己账号下创建一个名称也叫xfyuan的 Repo，并且包含一个README. md。这时可以在页面上看到 GitHub 贴心地告诉你它的特别用途： 再结合别的开发者提供的一些 GitHub Actions 等工具，就可以做到让其定时自动获取我的最新博客列表并显示出来。还可以展示我的 GitHub 当前的各种统计数字，甚至可以列出我账号下所有 Repo 用到的各种开发语言占比，等等。最后，打开我的 GitHub Profile 页面，就变成了这样： 效果非常不错！ 有兴趣的朋友可以直接看我的 README. md 是怎么做的。 要是还想把 GitHub 个人页面弄得再酷炫一些的话，自己到下面这些资源列里翻找折腾吧：  abhisheknaiidu/awesome-github-profile-readme：各种新 README 收集大全，特别是 GitHub Actions 那部分，最有技术参考含量，能玩出的花样最多。 GitHub 隐藏新功能！个人页还能这么玩？ 你在 GitHub 上看到过的最有意思的项目是什么？"
    }, {
    "id": 37,
    "url": "/2020/07/became-github-arctic-code-vault-contributor/",
    "title": "成为2020 GitHub北极源代码保险库计划的贡献者",
    "body": "2020/07/24 - GitHub 最近公布了一项“北极源代码保险库计划”，要把开源软件代码埋藏在北极的数百米地底，至少保存 1000 年。从 GitHub 的官方博客公告看，目前已经有数百万开发者的开源代码被保存到这个“保险库”中了。可以在每个人的 GitHub Profile 页面中查看到自己有没有成为其中之一。 幸运的是，我入选了，自己 GitHub 上的部分源码已经被封存到北极的地下:)。看到下面截图中左下角的“Highlight”么？如果你有跟我一样的“Arctic Code Vault Contributor”这一行，那就是了！把鼠标 Hover 上去会出现一个浮动窗口，显示你入选的源码项目。 看来，可以给自己的简历加上这样一栏了，哈哈😄！ “成为 2020 GitHub 北极源代码保险库计划贡献者之一” "
    }, {
    "id": 38,
    "url": "/2020/07/testprof-factory-therapy-for-ruby-tests/",
    "title": "Ruby测试的“工厂疗法”",
    "body": "2020/07/23 - 本文已获得原作者（Vladimir Dementyev）和 Evil Martians 授权许可进行翻译。原文是 TestProf 这个 Evil Martians 出品的 Gem 介绍文章系列的第二篇。作者介绍了造成 Ruby 慢测试的一个主要元凶——Factory Cascade，以及如何使用 TestProf 来消灭这个元凶。文中也提到了众所周知的 Factories vs. Fixtures 问题，而 TestProf 可以做到让你鱼和熊掌兼得。  原文链接：TestProf II: Factory therapy for your Ruby tests — Martian Chronicles, Evil Martians’ team blog 作者：Vladimir Dementyev 站点：Evil Martians ——位于纽约和俄罗斯的 Ruby on Rails 开发人员博客。 它发布了许多优秀的文章，并且是不少 gem 的赞助商。【下面是正文】 概述: 学习如何把你的 Ruby 测试套件带回到健康满满、速度满满的路上，通过使用 TestProf——一个强大的工具包来诊断所有跟测试有关的问题。这一次，我们来聊聊 factories：它们如何拖慢你的测试，如何来估量那些负面影响，如何避免，以及如何让你的 factories 跟 fixtures 一样快。 前言: TestProf，用于很多 Evil Martians 的项目，以缩短 TDD 的反馈环，对任何其测试运行时间超过一分钟的 Rails（或其他基于 Ruby 的）应用而言都是一个必备工具。它通过扩展有关功能而对 RSpec 和 minitest 均可适用。 在我们展示该开源项目的介绍文章里，当时承诺会有一篇专门文章来讲述在测试 Ruby Web 应用时被经常忽视的一个问题：factory cascades。本文就是我们所承诺的东西。 通过在你的实际测试中运行一下 TestProf 有个概览是比较好的。所以如果你手边恰好有一个 RSpec 覆盖的 Rails 项目，且使用了 factory_bot（之前闻名的名字叫 factory_girl）的 factories——我们建议你阅读下去之前安装该 gem，那么这将会是一个互动式演练！ 安装 TestProf 很简单，把下面这行添加到你Gemfile的:test组： group :test do gem 'test-prof'endCrumbling factories: 无论何时要测试自己的应用时，我们都需要生成测试数据——两种常见的方式就是 factories 和 fixtures。 factory 是一个可以根据预定义 schema 生成其他对象（可以被持久化，也可以不）并动态实现的对象。 Fixtures 相当于一个不同的方案：它们声明数据的静态状态，其会被立即加载到测试数据库中，且通常在测试运行之间保持不变。 在 Rails 世界中，我们有内置 fixtures 和广受欢迎的第三方 factory 工具（比如 factory_bot, Fabrication, 及其他）。 尽管关于 “factories vs. fixtures” 的辩论看起来永远不会停止，不过我们仍然把 factories 视为一种更加灵活也更易于维护的方式来处理测试数据。 然而，能力越大责任越大：factories 更容易让你搬起石头砸自己的脚，让你的测试陷入困境。 那么，我们如何判断自己是否滥用了这种能力并且该怎么办？首先，让我们来看看测试套件耗费在 factories 上的时间有多少。 对此，我们应该使用这个“良医”：TestProf。该 gem 是个完整的诊断工具包，EventProf 为其中工具之一。顾名思义：它是一个事件分析器，可以让它追踪factory. create事件，它会在你每次调用FactoryBot. create()时被触发，同时一个 factory 生成的对象被保存到数据库中。 EventProf 对 RSpec 和 minitest 都支持，有一个命令行界面，所以在任何 Rails 项目目录下启动终端（当然，它必须得有测试和 factories，且本文中所有测试用例都假设用的是 RSpec）并运行如下命令： $ EVENT_PROF= factory. create  bundle exec rspec在输出中，你可以看到从 factories 创建数据所花费的总时间，以及前五个最慢的测试： [TEST PROF INFO] EventProf results for factory. createTotal time: 03:07. 353Total events: 7459Top 5 slowest suites (by time):UsersController (users_controller_spec. rb:3) – 00:10. 119 (581 / 248)DocumentsController (documents_controller_spec. rb:3) – 00:07. 494 (71 / 24)RolesController (roles_controller_spec. rb:3) – 00:04. 972 (181 / 76)Finished in 6 minutes 36 seconds (files took 32. 79 seconds to load)3158 examples, 0 failures, 7 pending从我们项目之一取出的一个真实范例中（重构之前），在六分半的测试运行时间里，超过三分钟是用于生成测试数据，其几乎占据了 50%。这并不令人惊讶：我曾经工作过的一些代码库上，从 factories 生成数据花费了多达 80% 的测试时间。 平静一下，请继续阅读，我们明白如何修复这个问题。 The name of the game is “cascade”: 根据多年以来的观测和对 TestProf 的开发，以及对所有与测试有关东西的分析，慢测试的原因里最大的一个是——factory cascade。 让我们来做一个演示： factory :comment do sequence(:body) { |n|  Awesome comment ##{n}  } author answerendfactory :answer do sequence(:body) { |n|  Awesome answer ##{n}  } author questionendfactory :question do sequence(:title) { |n|  Awesome question ##{n}  } author account # suppose it's our tenant in SaaS applicationendfactory :author do sequence(:name) { |n|  Awesome author ##{n}  } accountendfactory :account do sequence(:name) { |n|  Awesome account ##{n}  }end现在，试着猜一下，一旦你调用create(:comment)，会有多少条记录被创建到数据库中？如果你已经有了答案，请继续往下看。  首先，我们对于comment生成了一个body。但还没有记录被创建，所以这时总数是 0。 接下来，对于comment我们需要一个author。author应该属于account，因此我们创建了两条记录。总数：2。 每个 comment 需要一个可注释的对象，对吧？我们这里是answer。answer自身需要一个author跟account。这就多了三条记录。总数：2 + 2 = 4。【译者注：原文这里有点描述不清晰。前面说“三条记录”，是把 answer、author、account 一起算上的，后面总数计算的数字又按 2 来加，应该没计入 answer。结合后文看，answer 是在最后一步才计入的，所以这里按 2 算是对的。】 answer也需要一个question，其有自己的author跟后者自身的account。而且，我们的:question factory 也包含一个account关联。总数：4 + 4 = 8 现在我们可以创建answer以及comment本身了。总数： 8 + 2 = 10。就是如此！使用create(:comment)创建一个 comment 产生了十条数据库记录。 我们需要多个 account 和不同的 author 来测试一个单独的 comment 吗？不太可能吧。 你可以想象出当我们创建多个 comment 时，比如create_list(:comment, 10)，会发生什么。休斯敦，我们碰上麻烦了。【译者注：电影《阿波罗13》的经典台词】 遭遇 factory cascade——一个通过嵌套 factory 调用生成过量数据的无法控制的过程。 我们可以把 cascade 用一颗树来描绘： comment||-- author|  ||  |-- account||-- answer   |   |-- author   |  |   |  |-- account   |   |-- question   |  |   |  |-- author   |  |  |   |  |  |-- account   |  |   |  |--account让我们把这种呈现形式称为 factory 树。稍后会用在我们的分析中。 Fire walk with me: EventProf 仅为我们展示了花费在 factories 上的总时间，也因此能够识别出某些东西不对。然而，我们仍然不知道它们在哪儿，除非去挖掘代码做猜谜游戏。使用 TestProf 医疗包中的另一个工具，就不用这么麻烦了。 第二个分析器登场：FactoryProf。你可以这样来运行它： $ FPROF=1 bundle exec rspec报告结果列出了所有 factories 及其使用情况统计： [TEST PROF INFO] Factories usage total   top-level              name 1298       2             account 1275       69              city  524      516              room  551      549              user  396      117           membership524 examples, 0 failures这里的total和top-level结果有什么区别？total值是一个 factory 被使用生成数据记录的次数，不管显式使用（通过create调用），还是在另一个 factory 内的隐式使用（通过关联关系和回调）；而top-level值仅考虑显式调用。 因此，top-level值和total值之间的明显不同就可能指示了 factory cascade：告知我们一个 factory 更经常从其他 factories 被引用，而非直接调用其自身。 如何准确找出这个“其他 factories”？就用前面讨论过的 factory 树来帮忙！我们来把这颗树平铺展开（使用 pre-order traversal）并把结果列表称为 factory 堆栈： // factory stack built from the factory tree above[:comment, :author, :account, :answer, :author, :account, :question, :author, :account, :account]下面是如何以编程方法构建 factory 堆栈的方式：  每次FactoryBot. create(:thing)被调用时，一个新堆栈就被初始化（使用:smth作为首元素）。 每次另一个 factory 在一个:thing内被使用时，我们把其压入堆栈。为什么堆栈很棒？正如跟堆栈调用那样，我们可以绘制火焰图！而有什么比火焰图更酷的呢？ FactoryProf 了解如何生成交互的 HTML 火焰图报告，开箱即用。下面是另一个命令行调用： $ FPROF=flamegraph bundle exec rspec输出结果包含一个 HTML 报告的路径： [TEST PROF INFO] FactoryFlame report generated: tmp/test_prof/factory-flame. html在你浏览器中打开，可以看到类似这样： 我们怎样来阅读它？ 每一栏代表一个 factory 堆栈。该栏越宽，该堆栈在测试中耗费的时间越多。root那栏展示了 top-level create调用的总数。 如果你的 FactoryFlame 报告看起来象纽约的大厦轮廓线那样参差不齐，这就是有很多 factory cascade 了（每个“摩天大楼”代表一个 cascade）： 尽管看起来景色很美，但这并非你理想的无 cascade 报告应有的模样。相反，你的目标应该是象荷兰乡村那样平坦的东西： Doctor, am I going to live?: 知道如何发现 cascade 是不够的——我们还需要消灭它们。让我们来考虑有关的几种技术吧。 Explicit associations: 涌上心头的第一个做法就是从我们的 factories 移除所有（或几乎所有）关联关系： factory :comment do sequence(:body) { |n|  Awesome comment ##{n}  } # do not declare associations # author # answerend采用这个方案，你在使用一个 factory 时必须明确指定所有需要的关联关系： create(:comment, author: user, answer: answer)# But!create(:comment) # =&gt; raises ActiveRecord::InvalidRecord有人可能会问：我们使用 factories 不就是为了每次避免指定所需的参数么？对，没错。用这个方案，factories 变快了，但也更没用了。 Association inference: 有时候（通常是在处理 denormalization时）是可能从其他关系推断出关联关系的： factory :question do sequence(:title) { |n|  Awesome question ##{n}  } author account do  # infer account from author  author&amp;. account endend现在，我们可以编写create(:question)或create(:question, author: user)而不创建一个单独的 account。 我们也能够使用生命周期回调： factory :question do sequence(:title) { |n|  Awesome question ##{n}  } transient do  author :undef  account :undef end after(:build) do |question, _evaluator|  # if only author is specified, set account to author's account  question. account ||= author. account unless author == :undef  # if only account is specified, set author to account's owner  question. author ||= account. owner unless account == :undef endend这个方案会很有效，但需要很多重构（而且，坦率地讲，让 factories 更难阅读）。 Factory default: 而 TestProf 提供了另一种方式来消除 cascade——FactoryDefault。它是一个 factory_bot 的扩展，通过允许你隐式重用 factory 内的记录，来启用更简洁和更不易出错的 DSL，以创建有关联关系的默认值。考虑如下示例： describe 'PATCH #update' do let!(:account) { create_default(:account) } let!(:author) { create_deafult(:author) } # implicitly uses account defined above let!(:question) { create_default(:question) } # implicitly uses account and author defined above let(:answer) { create(:answer) } # implicitly uses question and author defined above let(:another_question) { create(:question) } # uses the same account and author let(:another_answer) { create(:answer) } # uses the same question and author # . . . end这个方案的主要优势在于你不必修改自己的 factories。你所有需要做的就是把测试中的一些create(…)调用替换为create_default(…)。 另一方面，这个特性也为你的测试带来了一些魔法，所以请谨慎使用，因为测试应该尽可能保持可读性。仅针对 top-level 的实体（比如多租户 App 中的租户）使用是个不错的主意。 Bonus: AnyFixture: 到目前为止我们只讨论了 factory cascade。从 TestProf 报告中我们还能看出其他什么呢？ 让我们再来看一眼 FactoryProf 报告： [TEST PROF INFO] Factories usage total   top-level              name 1298       2             account 1275       69              city  524      516              room  551      549              user524 examples, 0 failures注意到room和user factories 被使用了跟测试用例总数大约相同的次数。因此，在每个用例中两者可能都需要。对于所有用例一次性创建这些记录呢？那么，我们可以使用 fixtures。 由于我们已经有了 factories，重用它们来生成 fixtures 会是很棒的。有请 AnyFixture 登场。 你可以为数据生成使用任何代码块，而 AnyFixture 会在运行结束时负责清理数据库。 AnyFixture 可以很好地跟 RSpec 的 shared contexts 一起工作： # Activate AnyFixture DSL (fixture) through refinementsusing TestProf::AnyFixture::DSLshared_context  shared user , user: true do # You should call AnyFixture outside of transaction to re-use the same # data between examples before(:all) do  fixture(:user) { create(:user, room: room) } end let(:user) { fixture(:user) }end然后激活该 shared context： describe CitiesController, :user do before { sign_in user } # . . . end随着 AnyFixture 的启用，FactoryProf 报告会看起来这样： total   top-level              name 1298       2             account 1275       69              city 8        1              room 2        1              user524 examples, 0 failures看起来完美，不是么？ 小孩子才对 factories 和 fixtures 做选择，我们全都要！ 附记: 感谢阅读！ Factories 为你的测试数据生成带来了简单和灵活，但它们非常脆弱——cascade 如幽灵般出现，而重复地数据创建会消费大量的时间。 照顾好你的 factories 吧，定期带它们去看看医生（TestProf）。让测试更快速，开发者就更快乐！ 请阅读 TestProf 介绍文章 以了解更多该项目和其他使用场景背后的初衷。 "
    }, {
    "id": 39,
    "url": "/2020/07/testprof-doctor-for-slow-ruby-tests/",
    "title": "Ruby慢测试的“良医圣手”",
    "body": "2020/07/21 - 本文已获得原作者（Vladimir Dementyev）和 Evil Martians 授权许可进行翻译。原文介绍了 TestProf 这个 Evil Martians 出品的强大 Gem。作者通过详细的范例场景和代码演示，说明了 TestProf 怎样对 Ruby 测试进行性能分析，找出慢测试的痛点，以及如何使用其提供的工具箱对慢测试改进，缩短测试运行时间，进行令人愉悦的 Ruby 开发。  原文链接：TestProf: a good doctor for slow Ruby tests — Martian Chronicles, Evil Martians’ team blog 作者：Vladimir Dementyev 站点：Evil Martians ——位于纽约和俄罗斯的 Ruby on Rails 开发人员博客。 它发布了许多优秀的文章，并且是不少 gem 的赞助商。【下面是正文】 概述: 编写测试是开发过程的重要部分，尤其是在 Ruby 和 Rails 社区。我们平常不会关注测试用例套件的性能，直到发现自己在对测试“绿点”的等待中已经耗费了太多时间为止。 前言: 我已经花费了许多时间用在分析测试套件的性能上，也开发了一些很有用的技术和工具来让测试跑得更快。我把所有这些集合到了一个称为 TestProf 的 Gem 中，这是一个 Ruby 测试的分析工具箱。 The Motivation: 慢测试浪费你的时间，降低你的效率。 你可能在扪心自问：“为什么测试性能很重要？”。在给出任何建议之前，让我向你展示一些统计数据吧。 今年早些时候我做过一个小调查，询问了 Ruby 开发者关于其测试偏好的内容。 首先——各位，好消息是——事实证明大多数 Ruby 开发者都确实会编写测试（坦率地说，我并不感到惊讶）。顺便说一句，这就是我之所以如此喜欢 Ruby 社区的原因。 根据这个调查，所有测试套件只有四分之一其运行时间超过 10 分钟——同时只有一半是运行少于 5 分钟。 看起来情况还不坏，对吧？那让我们只来看拥有超过 1000 个测试用例的情况。 现在看起来就差多了：将近一半的测试套件运行都超过了 10 分钟，而几乎 30%——更是超过了 20 分钟。 顺便说一句，我一直工作着的一个典型 Rails 应用有着 6000 ～ 10000 个测试用例。 当然，你不是每次进行一个改动后都必须运行整个测试套件。通常，当我做一个中等大小的功能时，每次提交之前会运行大约 100 个测试用例，而这只花费一分钟左右。但即使这样的“一分钟”也影响到了我的反馈环（参看 Justin Searls 的演讲）从而浪费了我的时间。 尽管如此，在部署周期之内，我们还是必须在使用 CI 服务时运行所有测试。你是否愿意等待好几十分钟（如果队列中有大量的构建，甚至要数小时）来部署一个 hotfix？ 我很怀疑。 并发地构建会有所帮助，但它们是有代价的。看看下面的柱状图： 例如，我当前的项目上，我们有 5 个并行构建，而平均（每个 job）RSpec 耗时是 2 分 30 秒于 1250 个测试用例上。这意味着我们的 EPM（examples per minute）等于 500. 在未优化之前，800 个测试用例耗时 4 分钟——这仅仅 200 EPM！现在每次构建我们节省了 3～4 分钟。 所以，毫无疑问，慢测试浪费你的时间，拉低你的工作效率。 The Toolbox: 好，你已经认识到了自己的测试套件很慢。如何找出它们慢的原因？ 让我略过这个介绍视频的所有说辞直接向你介绍 TestProf——一个 Ruby 测试分析工具箱。 TestProf 旨在帮你识别测试套件的瓶颈，并为你提供修复它们的“配方”。 我来给你展示下自己是如何使用它来分析并且改进测试的。 General Profiling: 在深入挖掘整个测试套件之前，收集一些常规信息通常很有用。 尝试回答如下问题：  在哪些地方你的测试花费了更多的时间：controllers、models、services 或者 jobs？ 最耗时间的 module/method 是什么？并非那么容易，对吧？ 要回答第一个问题，你可以使用 TestProf 的 Tag Profiler，它让你可以收集到根据特别的 RSpec tag 值进行分组的统计信息。RSpec 为测试用例自动添加了type的 tag，所以我们可以这样使用它： TAG_PROF=type rspec[TEST PROF INFO] TagProf report for type     type     time  total %total  %time      avg  controller   08:02. 721  2822  39. 04  34. 29   00:00. 171    service   05:56. 686  1363  18. 86  25. 34   00:00. 261     model   04:26. 176  1711  23. 67  18. 91   00:00. 155      job   01:58. 766   327  4. 52  8. 44   00:00. 363    request   01:06. 716   227  3. 14  4. 74   00:00. 293     form   00:37. 212   218  3. 02  2. 64   00:00. 170     query   00:19. 186   75  1. 04  1. 36   00:00. 255    facade   00:18. 415   95  1. 31  1. 31   00:00. 193  serializer   00:10. 201   19  0. 26  0. 72   00:00. 536    policy   00:06. 023   65  0. 90  0. 43   00:00. 092   presenter   00:05. 593   42  0. 58  0. 40   00:00. 133    mailer   00:04. 974   41  0. 57  0. 35   00:00. 121    . . . 现在要查找瓶颈即可只关注某些测试类型就够了。 你可能已经了解了常规的 Ruby 分析器，例如 RubyProf 和 StackProf。 TestProf 帮助你在测试套件上无需任何调整就能运行它们： TEST_RUBY_PROF=1 bundle exec rake test# orTEST_STACK_PROF=1 rspec这些分析器生成的报告可以帮你识别最热的堆栈路径，从而回答第二个问题。 不幸的是，这种类型的分析需要大量资源，让你本来就不那么快的测试套件愈加迟缓。你不得不在测试的一小部分上来运行它，但如何选择是哪一小部分？好吧，只能随机了！ TestProf 包含一个特别的补丁，让你可以运行随机的 RSpec 用例组（或 Minitest 的）： SAMPLE=10 bundle exec rspec现在尝试在你 controller 测试的一个样例上运行 StackProf（因为根据上面 TestProf 它们是最慢的）看看输出结果。当我在自己的一个项目上这样做之后，看到了如下内容： %self   calls name20. 85    721  &lt;Class::BCrypt::Engine&gt;#__bc_crypt 2. 31   4690 *ActiveSupport::Notifications::Instrumenter#instrument 1. 12   47489  Arel::Visitors::Visitor#dispatch 1. 04  205208  String#to_s 0. 87  531377  Module#=== 0. 87  117109 *Class#new事实证明我们 Sorcery 的 encryption 配置在测试环境跟在生产环境中一样严格。 一个典型的 Rails 应用中，关于时间你会在报告内看到类似这样的一些内容： TOTAL  (pct)   SAMPLES  (pct)   FRAME  205 (48. 6%)     96 (22. 7%)   ActiveRecord::PostgreSQLAdapter#exec_no_cache  41  (9. 7%)     22  (5. 2%)   ActiveModel::AttributeMethods::#define_proxy_call  20  (4. 7%)     14  (3. 3%)   ActiveRecord::LazyAttributeHash#[]大量ActiveRecord的东西——意味着大量的数据库操作。想知道如何处理？继续往下看。 Database Interactions: 知道你的测试套件在数据库上花费了多少时间吗？先猜猜看，然后使用 TestProf 来计算一下。 我们已经扩展了 Rails 中的 instrumentation（ActiveSupport 的 Notification 和 Instrumentation 功能），所以让我们略过基础来介绍 Event Profiler。 EventProf 在你的测试套件运行中收集检测指标，并提供包含常规信息的报告以及与指定指标有关的前 N 个最慢的组和用例。目前，它自带的仅支持ActiveSupport::Notifications，但其很容易跟你自己的解决方案集成。 要获取有关数据库使用情况的信息，我们可以用sql. active_record事件。然后报告看起来会是这样（很类似于rspec --profile）： EVENT_PROF=sql. active_record rspec . . . [TEST PROF INFO] EventProf results for sql. active_recordTotal time: 00:05. 045Total events: 6322Top 5 slowest suites (by time):MessagesController (. /spec/controllers/messages_controller_spec. rb:3)–00:03. 253 (4058 / 100)UsersController (. /spec/controllers/users_controller_spec. rb:3)–00:01. 508 (1574 / 58)Confirm (. /spec/services/confirm_spec. rb:3)–00:01. 255 (1530 / 8)RequestJob (. /spec/jobs/request_job_spec. rb:3)–00:00. 311 (437 / 3)ApplyForm (. /spec/forms/apply_form_spec. rb:3)–00:00. 118 (153 / 5)对于我目前的项目，耗费在 DB 上的时间量大约是 20%——而这已经是在对其进行了大量优化之后！起初，其耗费时间超过了 30%。 这个指标对于每个项目没有一个单一的最优数字。它高度依赖于你的测试风格：编写更多的单元测试还是集成测试。 我们主要编写集成测试，顺便说一句——20%并不差（但还能更好）。 什么是数据库耗时偏高的典型原因呢？一言难尽，我来捋一捋其中的部分：  无用的数据生成 过重的测试准备（before/setup hooks） Factory cascades第一个是著名的Model. new vs. Model. create问题（或者build_stubbed vs. create在 FactoryBot 中的区别）——你在对 model 的单元测试中可能不需要写入数据库。所以别那样做，好吧？ 但如果已经那样做了怎么办？如何找出哪些测试不需要持久化数据？这就该 Factory Doctor 登场了。 当你创建不必要的数据时，FactoryDoctor 会通知你： FDOC=1 rspec[TEST PROF INFO] FactoryDoctor reportTotal (potentially) bad examples: 2Total wasted time: 00:13. 165User (. /spec/models/user_spec. rb:3) validates name (. /spec/user_spec. rb:8)–1 record created, 00:00. 114 validates email (. /spec/user_spec. rb:8)–2 records created, 00:00. 514不幸的是，FactoryDoctor 不是魔术师（它还在学习中），有时它也会发生“误诊”的事情。 第二个问题比较棘手。考虑这个例子： describe BeatleSearchQuery do # We want to test our searching feature, # so we need some data for every example let!(:paul) { create(:beatle, name: 'Paul') } let!(:ringo) { create(:beatle, name: 'Ringo') } let!(:george) { create(:beatle, name: 'George') } let!(:john) { create(:beatle, name: 'John') } # and about 15 examples hereend你可能会想：“这里用 fixture 就行了呗”。这貌似个好主意，不过当你正在做一个每天都要更改数十个 model 的大型项目时，就不是那么回事儿了。 另外一个考虑是用before(:all) hook 仅生成数据一次。但这里要提请注意——我们不得不手动清理数据库，因为before(:all)是在事务外运行的。 或者，我们可以把整个组都手动包在一个事务里！这正是 TestProf 的 before_all helper 所做的： describe BeatleSearchQuery do before_all do  @paul = create(:beatle, name: 'Paul')  @ringo = create(:beatle, name: 'Ringo')  @george = create(:beatle, name: 'George')  @john = create(:beatle, name: 'John') end # and about 15 examples hereend如果你想要在不同的组（文件）之间共享 context，考虑使用 Any Fixture，其让你能从代码生成 fixture（比如，使用 factories）。 Factory Cascades: Factory cascade 是一个非常普遍但很少解决的问题，它会让你的整个测试套件陷入困境。 简而言之，它是通过嵌套 factory 调用而生成过度数据的不可控进程。TestProf 知道如何处理它，我们已经写了一篇专栏文章来单独讨论这个主题——你值得看一下。 Background Jobs: 除去数据库瓶颈之外，当然还有很多其他瓶颈。我们来说说其中一个。 在测试中有一个 inline 后台任务的普遍做法（例如，Sidekiq::Testing. inline!）。 通常，我们把一些繁重的事情丢进后台任务中，因此无条件地运行所有任务会拖慢运行时间。 TestProf 支持对后台任务耗费时间的分析（目前，仅对 Sidekiq）。只需告诉它要分析sidekiq. inline： EVENT_PROF=sidekiq. inline bundle exec rspec现在当你知道了所耗费的准确时间之后，接下来要做什么？简单地关闭 inline 模式很可能会破坏很多测试用例——太多太多以至于无法快速修复。 解决方案就是全局关闭 inline 模式，仅在必要时才使用它。如果你在用 RSpec，则可以这样做： # Add a shared contextshared_context  sidekiq:inline , sidekiq: :inline do around(:each) { |ex| Sidekiq::Testing. inline!(&amp;ex) }end# And use it when necessaryit  do some bg , sidekiq: :inline do # . . . end那还是得必须把这些 tag 添加到每个失败的用例上，不是么？抱歉，有 TestProf 在，你确实不必。 在 TestProf 的工具箱中，有一个称为 RSpec Stamp 的特殊工具。它可以自动地添加指定的 tag： RSTAMP=sidekiq:inline rspec顺便说一句，RSpec Stamp 在其之下是用了 Ripper 来解析源文件并准确插入 tag 的。 在我们的指南里可以阅读到关于如何从inline!迁移到fake!的完整说明。 附记: TestProf 已经发布在 GitHub 和 rubygems. org，可随时用在你的应用中，帮助你提升测试套件的性能。 本文只是一个 TestProf 的简介，并未涵盖其所有特性。可以跳转到该系列的下一篇：TestProf II: Factory therapy for your Ruby tests 来学习更多关于 TestProf “医生”的工具包，它们能使你的测试更加漂亮优雅，你的 TDD 反馈环更短更快，从而让你成为一个快乐的 Ruby 开发者。 这里是一些额外的资源列表：  TestProf 文档 2017 年 RailsClub Moscow 的“Faster Tests” 演讲（视频[俄语]，slides） 2017 年 RubyConfBy 的“Run Test Run” 演讲（视频，slides） Benoit Tigeot 发表的 “Tips to improve speed of your test suite”"
    }, {
    "id": 40,
    "url": "/2020/07/programming-elixir-1-6-chapter-5/",
    "title": "《Programming Elixir >= 1.6》第五章：匿名函数",
    "body": "2020/07/20 - 函数是 Elixir 的数据转换基石的重要引擎之一。Elixir 函数又分为匿名函数和具名函数。《Programming Elixir &gt;= 1. 6》用整个第五章专门讲述了“匿名函数”的概念，可见其重要性。如果是写过 JavaScript 的朋友，对“匿名函数”的概念一定不陌生。而 Elixir 的“匿名函数”到底怎样，看这一章就能完全了解。 【下面是正文】 5. Anonymous Functions: Elixir 是函数式语言，所以毫不意外函数是一个基础类型。 一个匿名函数使用关键字fn来创建。 fn parameter-list -&gt; body parameter-list -&gt; bodyend可以把fn. . . end想像成有点像包裹字符串字面量的双引号，只是这里把一个函数而非字符串作为返回值。我们可以把这个返回的函数传递给其他函数，也可以向它传参来运行。 最简单的是，函数有一个参数列表和一个函数体，用-&gt;分隔。 例如，下面定义了一个函数，并把其绑定到变量sum，然后调用它： iex&gt; sum = fn (a, b) -&gt; a + b end#Function&lt;12. 17052888 in :erl_eval. expr/5&gt;iex&gt; sum. (1, 2)3第一行代码创建了一个函数，带两个参数（名为 a 和 b）。函数的实现方法位于-&gt;之后（本例中只是简单把两个参数相加），到关键字end为止。我们把该函数存到变量sum上。 第二行代码使用语句sum. (1, 2)执行了这个函数。点. 语法表示调用函数，且传递的参数放在括号中。（你可能注意到了，我们在调用具名函数时并没有使用点语法——这是一个匿名函数和具名函数的差别。） 如果函数不带任何参数，仍需要使用括号来调用它： iex&gt; greet = fn -&gt; IO. puts  Hello  end#Function&lt;12. 17052888 in :erl_eval. expr/5&gt;iex&gt; greet. ()Hello:ok然而，可以在定义函数时不要括号： iex&gt; f1 = fn a, b -&gt; a * b end#Function&lt;12. 17052888 in :erl_eval. expr/5&gt;iex&gt; f1. (5,6)30iex&gt; f2 = fn -&gt; 99 end#Function&lt;12. 17052888 in :erl_eval. expr/5&gt;iex&gt; f2. ()99Functions and Pattern Matching: 当我们调用sum. (2, 3)时，很自然会认为是简单地把 2 赋予参数 a、3 赋予参数 b。但是这个词，赋予，应当给我们敲响警钟。Elixir 没有赋值，而是试图把值和模式进行匹配。（这些我们已经在前面的《Pattern Matching》一章中讲过了。） 当我们写下 a = 2Elixir 会通过把 a 绑定到 2 来进行模式匹配。这才是刚才的sum函数被调用时发生的事。如果我们传递 2 和 3作为参数时，Elixir 会试图把传入的参数和定义的参数 a 和 b 进行匹配（这样就绑定 a 为 2以及 b 为 3）。这等同于： {a, b} = {2, 3}这意味着我们在调用一个函数时可以进行更复杂的模式匹配。例如，下面的函数把一个二元元组中的两个元素颠倒了顺序： iex&gt; swap = fn { a, b } -&gt; { b, a } end#Function&lt;12. 17052888 in :erl_eval. expr/5&gt;iex&gt; swap. ( { 6, 8 } ){8, 6}下一节中，我们会看到通过利用模式匹配的威力来达到函数的多种实现方式。 One Function, Multiple Bodies: 单个函数定义允许你定义不同的实现，具体取决于传递的参数的类型和内容。（你无法根据参数的数量进行选择 - 函数定义中每个子句必须具有相同数量的参数。） 最简单的，我们能够使用模式匹配来选择哪个子句被执行。下面的例子中，由于我们知道File. open在成功打开文件时会返回一个首元素为 :ok 的元组，所以我们可以定义一个函数，要么显示打开文件的第一行，要么在文件无法打开时显示一个简单的错误信息。 iex&gt; handle_open = fn. . . &gt; {:ok, file} -&gt;  Read data: #{IO. read(file, :line)} . . . &gt; {_, error} -&gt;  Error: #{:file. format_error(error)} . . . &gt; end#Function&lt;12. 17052888 in :erl_eval. expr/5&gt;iex&gt; handle_open. (File. open( code/intro/hello. exs )) Read data: IO. puts \ Hello, World!\ \n iex&gt; handle_open. (File. open( nonexistent )) Error: no such file or directory 来看一下函数定义的内部。2、3行我们定义了两个单独的函数体，每个都用一个元组作为参数。第一个需要元组的首元素为 :ok，第二个使用特别的变量_（下划线）来匹配任何其他的值作为首元素。 再看第6行，我们调用了该函数，且把使用File. open打开一个已存在文件的结果传递给它。这意味着它接收到的是元组{:ok, file}，这正好匹配了第2行的函数字句。相应地调用IO. read读取该文件的第一行。 接着我们再次调用handle_oepn，这次尝试打开一个不存在文件。返回的元组（{:error, :enoent}）被传递给它，并寻找一个可匹配的子句。第2行会由于首元素不是 :ok 而匹配失败，但下一行能匹配成功。error 恰好能完美契合该子句的代码格式。 留意一下代码的其他部分。第3行我们调用了:file. format_error，:file部分表示了底层 Erlang 的File模块，因此我们能调用其format_error函数。与第6行的File. open调用对比一下，那里的File部分对应的是 Elixir 的内建模块。这是一个 Elixir 代码中使用底层环境（函数）的好例子。很棒的是，你能使用全部现有的 Erlang 库——有成千上万经过了时间检验的代码任你取用。但这也有点复杂了，你在调用时将不得不在 Erlang 的函数和 Elixir 的函数中进行区分。 最后，上例中展示了 Elixir 的字符串插值。在一个字符串中，#{. . . }的内容会被解析并使用其结果来代替。 Functions Can Return Functions: 这儿有一些奇怪的代码： iex&gt; fun1 = fn -&gt; fn -&gt;  Hello  end end#Function&lt;12. 17052888 in :erl_eval. expr/5&gt;iex&gt; fun1. ()#Function&lt;12. 17052888 in :erl_eval. expr/5&gt;iex&gt; fun1. (). () Hello 奇怪的地方在于第一行。它很难理解，所以我们把它展开来看。 fun1 = fn -&gt;					fn -&gt;						 Hello 					end				end变量fun1被绑定到一个函数。这个函数不带参数，其函数体是第二个函数定义。第二个函数也不带参数，返回字符串“Hello”。 当我们调用外层函数（使用fun1. ()），它返回内层函数。这个返回值当我们再调用（fun1. (). ()）时内层函数即被执行，“Hello”被返回。 一般来说我们不会写出类似fun1. (). ()这样的代码。然而我们可以调用外层函数并绑定结果给一个变量。还可以用括号把内层函数括起来使其不易混淆。 iex&gt; fun1 = fn -&gt; (fn -&gt;  Hello  end) end#Function&lt;12. 17052888 in :erl_eval. expr/5&gt;iex&gt; other = fun1. ()#Function&lt;12. 17052888 in :erl_eval. expr/5&gt;iex&gt; other. () Hello Functions Remember Their Original Environment: 让我们更深入地看下嵌套函数。 iex&gt; greeter = fn name -&gt; (fn -&gt;  Hello #{name}  end) end #Function&lt;12. 17052888 in :erl_eval. expr/5&gt;iex&gt; dave_greeter = greeter. ( Dave )#Function&lt;12. 17052888 in :erl_eval. expr/5&gt;iex&gt; dave_greeter. () Hello Dave 当我们调用外层函数，其返回内层的函数定义。这时并没有把 name 替换成字符串。然而当我们调用内层函数（dave_greeter. ()）时，替换发生了，结果显示出来。 有些奇怪的事情发生了。内层函数使用外层函数的name参数。在greeter. ( Dave )执行并返回时，外层函数已经结束，参数已不在定义域了。但是当我们运行内层函数，它又使用了这个参数的值。 这之所以能正常运行是由于在 Elixir 中函数会自动携带变量的绑定，包括其在定义时的域。上面例子中，变量name是在外层函数的域中被绑定。当内层函数定义时，它继承了这个域，且把name相关的绑定一起带上了。这就是闭包——它的域会包含其变量的绑定，并把这些绑定打包成可以保存并在以后使用的东西。 来看看更多的玩意儿。 Parameterized Functions: 上一个示例中，外层函数带一个参数，内层函数则没有。现在试试都带参数的情况。 iex&gt; add_n = fn n -&gt; (fn other -&gt; n + other end) end #Function&lt;12. 17052888 in :erl_eval. expr/5&gt;iex&gt; add_two = add_n. (2)#Function&lt;12. 17052888 in :erl_eval. expr/5&gt;iex&gt; add_five = add_n. (5)#Function&lt;12. 17052888 in :erl_eval. expr/5&gt;iex&gt; add_two. (3)5iex&gt; add_five. (7)12这里内层函数把其参数other加到外层函数的参数n上。每次调用外层函数，我们传一个值给其参数n，它返回一个函数，这个函数把n和其自己的参数相加。 Passing Functions as Arguments: 函数就是值，所以可以把它们传给其他函数。 iex&gt; times_2 = fn n -&gt; n * 2 end#Function&lt;12. 17052888 in :erl_eval. expr/5&gt;iex&gt; apply = fn (fun, value) -&gt; fun. (value) end #Function&lt;12. 17052888 in :erl_eval. expr/5&gt;iex&gt; apply. (times_2, 6)12这里，apply是一个带第二个函数和一个值的函数。它返回的是第二个函数以那个值作为参数运行的结果。 在 Elixir 中，这种函数传递的能力几乎在到处都被漂亮地使用着。例如，内置的Enum模块有一个map函数，使用两个参数：一个集合和一个函数。它返回一个列表，是在那个集合的每个元素上都调用那个函数后的结果。 iex&gt; list = [1, 3, 5, 7, 9][1, 3, 5, 7, 9]iex&gt; Enum. map list, fn elem -&gt; elem * 2 end[2, 6, 10, 14, 18]iex&gt; Enum. map list, fn elem -&gt; elem * elem end[1, 9, 25, 49, 81]iex&gt; Enum. map list, fn elem -&gt; elem &gt; 6 end[false, false, false, true, true]Pinned Values and Function Parameters: 我们之前看模式匹配时，看到过 pin 操作符（^）允许在模式中使用一个变量的当前值。这种方式也能用在函数参数上。 defmodule Greeter do def for(name, greeting) do  fn   (^name) -&gt;  #{greeting} #{name}    (_) -&gt;  I don't know you   end endendmr_valim = Greeter. for( José ,  Oi! )IO. puts mr_valim. ( José ) # =&gt; Oi! JoséIO. puts mr_valim. ( Dave ) # =&gt; I don't know you这里，Greeter. for函数返回一个带两个 head 的函数（还记得列表的头和尾吗？）。当第一个参数为传给for的 name 的值时，第一个 head 就能匹配上。 The &amp; Notation: 创建短小帮助函数的策略是如此普遍，所以 Elixir 提供了一种快捷方式。我们先来看一下。 iex&gt; add_one = &amp;(&amp;1 + 1) # same as add_one = fn (n) -&gt; n + 1 end#Function&lt;6. 17052888 in :erl_eval. expr/5&gt;iex&gt; add_one. (44)45iex&gt; square = &amp;(&amp;1 * &amp;1)#Function&lt;6. 17052888 in :erl_eval. expr/5&gt;iex&gt; square. (8)64iex&gt; speak = &amp;(IO. puts(&amp;1))&amp;IO. puts/1iex&gt; speak. ( Hello )Hello:ok&amp;操作符把其后的表达式转换为一个函数。在表达式内，&amp;1, &amp;2等类似的占位符依次对应第一个、第二个等函数的参数。所以&amp;(&amp;1 + &amp;2)会被转换为fn p1, p2 -&gt; p1 + p2 end。 如果你觉得这种做法很聪明，那我们再来看看上面代码中有speak的那一行。一般来说 Elixir 会生成一个匿名函数，所以&amp;(IO. puts(&amp;1))会变成fn x -&gt; IO. puts(x) end。然而 Elixir 注意到匿名函数的函数体是一个简单具名函数（IO 模块的 puts 函数）的调用，且其参数按正确的顺序对应（意思是，匿名函数的第一个参数就是具名函数的第一个参数，以此类推）。因此 Elixir 就会优化这个匿名函数，用具名函数（IO. puts/1）的一个直接引用来代替它。 要使其正常工作，参数必须保持正确的顺序： iex&gt; rnd = &amp;(Float. round(&amp;1, &amp;2))&amp;Float. round/2iex&gt; rnd = &amp;(Float. round(&amp;2, &amp;1))#Function&lt;12. 17052888 in :erl_eval. expr/5&gt;当用这种方式定义函数时，你可能会看到对 Erlang 的引用蹦出来，这是因为 Elixir 运行在 Erlang VM 上的缘故。当你尝试更多如&amp;abs(&amp;1)的东西时，可以看到这种行为的更多体现。这里 Elixir 把对于 abs 函数的使用直接映射到底层的 Erlang 库，返回&amp;:erlang. abs/1。 因为[]和{}在 Elixir 中都是操作符，列表和元组的字面量也能被转换为函数。下面这个函数用来返回一个元组，其元素包含两个整数相除后的商和余数： iex&gt; divrem = &amp;{ div(&amp;1,&amp;2), rem(&amp;1,&amp;2) } #Function&lt;12. 17052888 in :erl_eval. expr/5&gt;iex&gt; divrem. (13, 5){2, 3}最后，&amp;操作符也可用于字符串（或类似字符串）的字面量： iex&gt; s = &amp; bacon and #{&amp;1} #Function&lt;6. 99386804/1 in :erl_eval. expr/5&gt;iex&gt; s. ( custard ) bacon and custard iex&gt; match_end = &amp;~r/. *#{&amp;1}$/#Function&lt;6. 99386804/1 in :erl_eval. expr/5&gt;iex&gt;  cat  =~ match_end. ( t )trueiex&gt;  cat  =~ match_end. ( ! )false还有第二种&amp;函数捕获操作符的使用方式。你能传给它一个已有函数的名称和元数（参数的个数），它返回的匿名函数会调用这个函数。传递给匿名函数的参数会依次传给这个具名函数。我们已经看到过了：当你在 iex 中输入&amp;(IO. puts(&amp;1))，看到显示的结果是&amp;IO. puts/1。这里puts是 IO 模块的函数，带一个参数。在 Elixir 中对其的命名方式为IO. puts/1。当把&amp;放在它的前面时，我们是把它封装为了一个函数。再看看其他的例子： iex&gt; l = &amp;length/1&amp;:erlang. length/1iex&gt; l. ([1,3,5,7])4iex&gt; len = &amp;Enum. count/1&amp;Enum. count/1iex&gt; len. ([1,2,3,4])4iex&gt; m = &amp;Kernel. min/2 # This is an alias for the Erlang function&amp;:erlang. min/2iex&gt; m. (99,88)88这种方式对于我们自己写的具名函数也是适用的（尽管我们还没有讲怎样写具名函数）。 &amp;快捷方式为我们提供了一种绝妙的方式来把函数传递给其他函数。 iex&gt; Enum. map [1,2,3,4], &amp;(&amp;1 + 1)[2, 3, 4, 5]iex&gt; Enum. map [1,2,3,4], &amp;(&amp;1 * &amp;1)[1, 4, 9, 16]iex&gt; Enum. map [1,2,3,4], &amp;(&amp;1 &lt; 3)[true, true, false, false]Functions Are the Core: 本书开头，我们说过编程的基石是数据的转换。函数是 提供这种转换的微小引擎。它们居于 Elixir 的最中心。 至此我们已经领略了匿名函数——尽管我们能把它们和变量绑定，但这些函数自己并没有名称。Elixir 也有具名函数。下一章就会讲到它们。 "
    }, {
    "id": 41,
    "url": "/2020/07/proper-browser-testing-in-rails/",
    "title": "2020时代的Rails系统测试",
    "body": "2020/07/16 - 本文已获得原作者（Vladimir Dementyev）和 Evil Martians 授权许可进行翻译。原文介绍了在新的 2020 时代，摒弃了基于 Java 的笨重 Selenium 之后，如何在 Rails 下构建基于浏览器的高效系统测试。作者对于系统测试概念进行了详细阐述，演示了具体配置的范例和运行效果，对 Docker 开发环境也有专业级别的涵盖。非常推荐。  原文链接：System of a test:Proper browser testing in Ruby on Rails — Martian Chronicles, Evil Martians’ team blog 作者：Vladimir Dementyev 站点：Evil Martians ——位于纽约和俄罗斯的 Ruby on Rails 开发人员博客。 它发布了许多优秀的文章，并且是不少 gem 的赞助商。【下面是正文】 发现 Ruby on Rails 应用中端到端浏览器测试的最佳实践集合，并在你的项目上采用它们。了解如何摒弃基于 Java 的 Selenium，转而使用更加精简的 Ferrum-Cuprite 组合，它们能直接通过纯 Ruby 方式来使用 Chrome DevTools 的协议。如果你在使用 Docker 开发环境——本文也有涵盖。 Ruby 社区对于测试饱含激情。我们有数不胜数的测试库，有成千上万篇关于测试主题的博客文章，甚至为此有一个专门的播客。更可怕的是，下载量排在前三名的 Gem 也都是有关 RSpec 测试框架的！ 我相信，Rails，是 Ruby 测试兴盛背后的原因之一。这个框架让测试的编写尽可能地成为一种享受了。多数情况下，跟随着 Rails 测试指南的教导就已足够（起码在刚开始的时候）。但事情总有例外，而在我们这里，就是系统测试。 对 Rail 应用而言，编写和维护系统测试很难被称为“惬意的”。我在处理这个问题上，自 2013 年的第一次 Cucumber 驱动测试算起，已经逐步改进了太多。而今天，到了 2020 年，我终于可以来跟大家分享一下自己当前（关于测试）的设置了。本文中，我将会讨论以下几点：  系统测试概述 使用 Cuprite 进行现代系统测试 配置范例 Docker 化的系统测试系统测试概述: “系统测试”是 Rails 世界中对于自动化端到端测试的一种通常称谓。在 Rails 采用这个名称之前，我们使用诸如功能测试、浏览器测试、甚至验收测试等各种叫法（尽管后者在意思上有所不同）。 如果我们回忆下测试金字塔（或者就金字塔），系统测试是处于非常靠上的位置：它们把整个程序视为一个黑盒，通常模拟终端用户的行为和预期。而这就是在 Web 应用程序情况下，为什么我们需要浏览器来运行这些测试的原因（或者至少是类似 Rack Test 的模拟器）。 我们来看下典型的系统测试架构： 我们需要管理至少三个“进程”（它们有些可以是 Ruby 线程）：一个 运行我们应用的 Web 服务端，一个浏览器，和一个测试运行器。这是最低要求。实际情况下，我们通常还需要另一个工具提供 API 来控制浏览器（比如，ChromeDriver）。一些工具尝试通过构建特定浏览器（例如 capybara-webkit 和 PhantomJS）来简化此设置，并提供开箱即用的此类 API，但它们在对真正浏览器的兼容性“竞争”中都败下阵来，没能幸免于难。 当然，我们需要添加少量 Ruby Gem 测试依赖库——来把所有碎片都粘合起来。更多的依赖库会带来更多的问题。例如，Database Cleaner 在很长时间内都是一个必备的库：我们不能使用事务来自动回滚数据库状态，因为每个线程都是用自己独立的连接。我们不得不针对每张表使用TRUNCATE . . . 或DELETE FROM . . . 来代替，这会相当慢。我们通过在所有线程中使用一个共享连接解决了这个问题（使用 TestProf extension）。Rails 5. 1 也发布了一个现成的类似功能。 这样，通过添加系统测试，我们增加了开发环境和 CI 环境的维护成本，以及引入了潜在的故障或不稳定因素：由于复杂的设置，flakiness 是端到端测试中最常见的问题。而大多数这些 flakiness 来自于跟浏览器的通信。 尽管通过在 Rails 5. 1 中引入系统测试简化了浏览器测试，它们仍然需要一些配置才能平滑运行：  你需要处理 Web drivers（Rails 假设你使用 Selenium）。 你可以自己在容器化环境中配置系统测试（例如，象这篇文章中的做法）。 配置不够灵活（例如，屏幕快照的路径）让我们来转到代码层面吧，看看在 2020 时代如何让你的系统测试更有乐趣！ 使用 Cuprite 进行现代系统测试: 默认情况下，Rails 假设你会使用 Selenium 来跑系统测试。Selenium 是一个实战验证过的 Web 浏览器自动化软件。它旨在为所有浏览器提供一个通用 API 以及最真实的体验。在模拟用户浏览器交互方面，唯有有血有肉的真人才比它做得更好些。 不过，这种能力不是没有代价的：你需要安装特定浏览器的驱动，现实交互的开销在规模上是显而易见的（例如，Selenium 的测试通常都相当慢）。 Selenium 已经是很久以前发布的了，那时浏览器还不提供任何内置自动化测试兼容性。这么多年过去了，现在的情况已经大不相同，Chrome 引入了 CDP 协议。使用 CDP，你可以直接操作浏览器的 Session，不再需要中间的抽象层和工具。 从那以后，涌现了好多利用 CDP 的项目，包括最著名的——Puppeteer，一个 Node. js 的浏览器自动化库。那么 Ruby 世界呢？是 Ferrum，一个 Ruby 的 CDP 库，尽管还相当年轻，也提供了不逊色于 Puppeteer 的体验。而对我们更重要的是，它带来了一个称为 Cuprite 的伙伴项目——使用 CDP 的纯 Ruby Capybara 驱动。 我从 2020 年初才开始积极使用 Cuprite 的（一年前我尝试过，但在 Docker 环境下有些问题），从未让我后悔过。设置系统测试变得异常简单（全部所需仅 Chrome 而已），而且执行是如此之快，以至于在从 Selenium 迁移过来之后我的一些测试都失败了：它们缺乏合适的异步期望断言，在 Selenium 中能通过仅仅是因为 Selenium 太慢。 让我们来看下我最近工作上所用到 Cuprite 的系统测试配置。 注释过的配置范例: 这个范例来自于我最近的开源 Ruby on Rails 项目——AnyCable Rails Demo。它旨在演示如何跟 Rails 应用一起使用刚刚发布的 AnyCable 1. 0，但我们也可用于本文——它有很好的系统测试覆盖着。 这个项目使用 RSpec 及其系统测试封装器。其大部分也是可以用在 Minitest 上的。 让我们从一个足以在本地机器上运行的最小示例开始。其代码放在 AnyCable Rails Demo 的 demo/dockerless 分支上。 首先来快速看一眼 Gemfile： group :test do gem 'capybara' gem 'selenium-webdriver' gem 'cuprite'end什么？为什么需要selenium-webdriver，如果我们根本不用 Selenium 的话？事实证明，Rails 要求这个 gem 独立于你所使用的驱动而存在。好消息是，这已经被修复了，我们有望在 Rails 6. 1 中移除这个 gem。 我把系统测试的配置放在多个文件内，位于spec/system/support目录，使用专门的system_helper. rb来加载它们： spec/ system/  support/   better_rails_system_tests. rb   capybara_setup. rb   cuprite_setup. rb   precompile_assets. rb   . . .  system_helper. rb我们来看下上面这个列表中的每个文件都是干嘛的。 system_helper. rb: system_helper. rb文件包含一些针对系统测试的通用 RSpec 配置，不过，通常而言，都如下面这样简单： # Load general RSpec Rails configurationrequire  rails_helper. rb # Load configuration files and helpersDir[File. join(__dir__,  system/support/**/*. rb )]. sort. each { |file| require file }然后，在你的系统测试中，使用require  system_helper 来激活该配置。 我们为系统测试使用了一个单独的 helper 文件和一个 support 文件夹，以避免在我们只需运行一个单元测试时加载所有多余的配置。 capybara_setup. rb: 这个文件包含针对 Capybara 框架的配置： # spec/system/support/capybara_setup. rb# Usually, especially when using Selenium, developers tend to increase the max wait time. # With Cuprite, there is no need for that. # We use a Capybara default value here explicitly. Capybara. default_max_wait_time = 2# Normalize whitespaces when using `has_text?` and similar matchers,# i. e. , ignore newlines, trailing spaces, etc. # That makes tests less dependent on slightly UI changes. Capybara. default_normalize_ws = true# Where to store system tests artifacts (e. g. screenshots, downloaded files, etc. ). # It could be useful to be able to configure this path from the outside (e. g. , on CI). Capybara. save_path = ENV. fetch( CAPYBARA_ARTIFACTS ,  . /tmp/capybara )该文件也包含一个对于 Capybara 很有用的补丁，其目的我们稍后揭示： # spec/system/support/capybara_setup. rbCapybara. singleton_class. prepend(Module. new do attr_accessor :last_used_session def using_session(name, &amp;block)  self. last_used_session = name  super ensure  self. last_used_session = nil endend)Capybara. using_session让你能够操作不同的浏览器 session，从而在单个测试场景内操作多个独立 session。这在测试实时功能时尤其有用，例如使用 WebSocket 的功能。 该补丁跟踪上次使用的 session 名称。我们会用这个信息来支持在多 session 测试中获取失败情况下的屏幕快照。 cuprite_setup. rb: 这个文件负责配置 Cuprite： # spec/system/support/cuprite_setup. rb# First, load Cuprite Capybara integrationrequire  capybara/cuprite # Then, we need to register our driver to be able to use it later# with #driven_by method. Capybara. register_driver(:cuprite) do |app| Capybara::Cuprite::Driver. new(  app,  **{   window_size: [1200, 800],   # See additional options for Dockerized environment in the respective section of this article   browser_options: {},   # Increase Chrome startup wait time (required for stable CI builds)   process_timeout: 10,   # Enable debugging capabilities   inspector: true,   # Allow running Chrome in a headful mode by setting HEADLESS env   # var to a falsey value   headless: !ENV[ HEADLESS ]. in?(%w[n 0 no false])  } )end# Configure Capybara to use :cuprite driver by defaultCapybara. default_driver = Capybara. javascript_driver = :cuprite我们也为常用 Cuprite API 方法定义了一些快捷方式： module CupriteHelpers # Drop #pause anywhere in a test to stop the execution.  # Useful when you want to checkout the contents of a web page in the middle of a test # running in a headful mode.  def pause  page. driver. pause end # Drop #debug anywhere in a test to open a Chrome inspector and pause the execution def debug(*args)  page. driver. debug(*args) endendRSpec. configure do |config| config. include CupriteHelpers, type: :systemend下面你可以看到一个#debug帮助方法如何工作的演示： better_rails_system_tests. rb: 这个文件包含一些有关 Rails 系统测试内部的补丁以及通用配置（代码注释有详细解释）： # spec/system/support/better_rails_system_tests. rbmodule BetterRailsSystemTests # Use our `Capybara. save_path` to store screenshots with other capybara artifacts # (Rails screenshots path is not configurable https://github. com/rails/rails/blob/49baf092439fc74fc3377b12e3334c3dd9d0752f/actionpack/lib/action_dispatch/system_testing/test_helpers/screenshot_helper. rb#L79) def absolute_image_path  Rails. root. join( #{Capybara. save_path}/screenshots/#{image_name}. png ) end # Make failure screenshots compatible with multi-session setup.  # That's where we use Capybara. last_used_session introduced before.  def take_screenshot  return super unless Capybara. last_used_session  Capybara. using_session(Capybara. last_used_session) { super } endendRSpec. configure do |config| config. include BetterRailsSystemTests, type: :system # Make urls in mailers contain the correct server host.  # It's required for testing links in emails (e. g. , via capybara-email).  config. around(:each, type: :system) do |ex|  was_host = Rails. application. default_url_options[:host]  Rails. application. default_url_options[:host] = Capybara. server_host  ex. run  Rails. application. default_url_options[:host] = was_host end # Make sure this hook runs before others config. prepend_before(:each, type: :system) do  # Use JS driver always  driven_by Capybara. javascript_driver endendprecompile_assets. rb: 这个文件负责在运行系统测试之前预编译 assets（我不在这儿粘贴它的完整代码，只给出最有趣的部分）： RSpec. configure do |config| # Skip assets precompilcation if we exclude system specs.  # For example, you can run all non-system tests via the following command: # #  rspec --tag ~type:system # # In this case, we don't need to precompile assets.  next if config. filter. opposite. rules[:type] ==  system  || config. exclude_pattern. match?(%r{spec/system}) config. before(:suite) do  # We can use webpack-dev-server for tests, too!  # Useful if you working on a frontend code fixes and want to verify them via system tests.   if Webpacker. dev_server. running?   $stdout. puts  \n⚙️ Webpack dev server is running! Skip assets compilation. \n    next  else   $stdout. puts  \n🐢 Precompiling assets. \n    # The code to run webpacker:compile Rake task   # . . .   end endend为什么要手动预编译 assets 呢，如果 Rails 能够自动为你做这事的话？问题在于 Rails 预编译 assets 是惰性的（比如，在你首次请求一个 asset 的时候），这会使你的第一个测试用例非常非常慢，甚至碰到随机超时的异常。 另一个我想提请注意的是使用 Webpack dev server 进行系统测试的能力。这在当你进行艰苦的前端代码重构时相当有用：你可以暂停一个测试，打开浏览器，编辑前端代码并看到它被热加载了！ Docker 化的系统测试: 让我们把自己的配置提升到更高的层次，使其兼容我们的 Docker 开发环境。Docker 化版本的测试设置在 AnyCable Rails Demo 代码库的默认分支上，可随意查看，不过下面我们打算涵盖那些有意思的内容。 Docker 下设置的主要区别是我们在一个单独的容器中运行浏览器实例。可以把 Chrome 添加到你的基础 Rails 镜像上，或者可能的话，甚至从容器内使用主机的浏览器（这可以用 Selenium and ChromeDriver 做到）。但是，在我看来，为docker-compose. yml定义一个专用的浏览器 service 是一种更正确的 Docker 式方式来干这个。 目前，我用的是来自 browserless. io 的 Chrome Docker 镜像。它带有一个好用的 Debug 查看器，让你能够调试 headless 的浏览器 session（本文最后有一个简短的视频演示）： services: # . . .  chrome:  image: browserless/chrome:1. 31-chrome-stable  ports:   -  3333:3333   environment:   # By default, it uses 3000, which is typically used by Rails.    PORT: 3333   # Set connection timeout to avoid timeout exception during debugging   # https://docs. browserless. io/docs/docker. html#connection-timeout   CONNECTION_TIMEOUT: 600000把CHROME_URL: http://chrome:3333添加到你的 Rails service 环境变量，以后台方式运行 Chrome： docker-compose up -d chrome现在，如果提供了 URL，我们就需要配置 Cuprite 以和远程浏览器一起工作： # cuprite_setup. rb# Parse URL# NOTE: REMOTE_CHROME_HOST should be added to Webmock/VCR allowlist if you use any of those. REMOTE_CHROME_URL = ENV[ CHROME_URL ]REMOTE_CHROME_HOST, REMOTE_CHROME_PORT = if REMOTE_CHROME_URL  URI. parse(REMOTE_CHROME_URL). yield_self do |uri|   [uri. host, uri. port]  end end# Check whether the remote chrome is running. remote_chrome = begin  if REMOTE_CHROME_URL. nil?   false  else   Socket. tcp(REMOTE_CHROME_HOST, REMOTE_CHROME_PORT, connect_timeout: 1). close   true  end rescue Errno::ECONNREFUSED, Errno::EHOSTUNREACH, SocketError  false endremote_options = remote_chrome ? { url: REMOTE_CHROME_URL } : {}上面的配置假设当CHROME_URL未被设置或者浏览器未响应时，使用者想使用本地安装的 Chrome。 我们这样做以便让配置向下兼容本地的配置（我们一般不强迫每个人都使用 Docker 作为开发环境；让拒绝使用 Docker 者为其独特的本地设置而受苦吧😈）。 我们的驱动注册现在看起来是这样的： # spec/system/support/cuprite_setup. rbCapybara. register_driver(:cuprite) do |app| Capybara::Cuprite::Driver. new(  app,  **{   window_size: [1200, 800],   browser_options: remote_chrome ? {  no-sandbox  =&gt; nil } : {},   inspector: true  }. merge(remote_options) )end我们也需要更新自己的#debug帮助方法以打印 Debug 查看器的 URL，而不是尝试去打开浏览器： module CupriteHelpers # . . .  def debug(binding = nil)  $stdout. puts  🔎 Open Chrome inspector at http://localhost:3333   return binding. pry if binding  page. driver. pause endend由于浏览器是运行在一个不同的“机器”上，因此它应该知道如何到达测试服务端（其不再是localhost）。 为此，我们需要配置 Capybara 服务端的 host： # spec/system/support/capybara_setup. rb# Make server accessible from the outside worldCapybara. server_host =  0. 0. 0. 0 # Use a hostname that could be resolved in the internal Docker network# NOTE: Rails overrides Capybara. app_host in Rails &lt;6. 1, so we have# to store it differentlyCAPYBARA_APP_HOST = `hostname`. strip&amp;. downcase ||  0. 0. 0. 0 # In Rails 6. 1+ the following line should be enough# Capybara. app_host =  http://#{`hostname`. strip&amp;. downcase ||  0. 0. 0. 0 } 最后，让我们对better_rails_system_tests. rb做一些调整。 首先，我们来让 VS Code 中的屏幕快照通知变得可点击🙂（Docker 绝对路径与主机系统是不同的）： # spec/system/support/better_rails_system_tests. rbmodule BetterRailsSystemTests # . . .  # Use relative path in screenshot message def image_path  absolute_image_path. relative_path_from(Rails. root). to_s endend其次，确保测试都使用了正确的服务端 host（这在 Rails 6. 1 中已经被修复了）： # spec/system/support/better_rails_system_tests. rbconfig. prepend_before(:each, type: :system) do # Rails sets host to `127. 0. 0. 1` for every test by default.  # That won't work with a remote browser.  host! CAPYBARA_APP_HOST # Use JS driver always driven_by Capybara. javascript_driverendIn too Dip: 如果你使用 Dip 来管理 Docker 开发环境（我强烈建议你这么做，它使你获得容器的强大能力，而无需记忆所有 Docker CLI 命令的成本付出），那么你可以通过在dip. yml中添加自定义命令和在docker-compose. yml中添加一个额外 service 定义，来避免手动加载chrome service： # docker-compose. yml# Separate definition for system tests to add Chrome as a dependencyrspec_system: &lt;&lt;: *backend depends_on:  &lt;&lt;: *backend_depends_on  chrome:   condition: service_started# dip. ymlrspec: description: Run Rails unit tests service: rails environment:  RAILS_ENV: test command: bundle exec rspec --exclude-pattern spec/system/**/*_spec. rb subcommands:  system:   description: Run Rails system tests   service: rspec_system   command: bundle exec rspec --pattern spec/system/**/*_spec. rb现在，我使用如下命令来运行系统测试： dip rspec system就是这样了！ 最后，让我向你展示一下如何使用 Browserless. io 的 Docker 镜像的 Debug 查看器进行调试： "
    }, {
    "id": 42,
    "url": "/2020/07/vim-mac-dictionary-plugin/",
    "title": "实现在Vim内直接查询macOS词典的Plugin",
    "body": "2020/07/15 - 我每天都会使用 Vim。用 Vim 不论编写自己的代码还是阅读别人的代码，这中间自然都会碰到一些不认识的英文词汇，自己就总会习惯性地想随手查一下单词的中文释义。而如何最高效地解决这个问题便成了一个有趣的事情。 最早的时候，基本上就是用鼠标点开 macOS 的词典 App，输入要查询的单词，查看结果。如果一天中只不过偶尔为之，那当然不是问题。但有时难免会有频繁碰到陌生词汇的时候，每个要查的词都得手动输入，这样繁琐的做法自然就不可取了。 然后就用上了 macOS 上大名鼎鼎的 Alfred 这个 App。GitHub 上有一个给它写的 workflow，让你可以在 Alfred 的弹出窗口中输入要查的单词，下方自动显示查询结果。这在效率上已经是前进了一大步，基本足够应对绝大多数的场景。但用的时间长了，还是发现它有不足的地方。因为 Alfred 显示查询结果时只能在一行中，而很多时候一个单词的释义内容很长，一行根本显示不完。这时就只能还是再敲一下回车键打开词典查看。比如下图中的 Service 释义，实际内容可远远不止截图中这么一点： 前几天，又碰到了这样的场景，多次不断打开又关闭 macOS 自带词典之后，我就在考虑能否换个思路完全解决这个问题？ 既然自己主要都是在 Vim 内才有这个“需求”，平时一般的场景用 Alfred 便足够对付了，那么就重点想想在 Vim 内的解决方案好了。这么一想，自然首先考虑 Vim 的 Plugin 了。Vim 的 Plugin 生态圈足够大，说不定有人早就碰到类似问题，已经写了 Plugin 来实现呢。 上 GitHub 一搜，还真被我找到这么一个：vim-mac-dictionary。不过看提交历史，最近一次还是在 2018 年，貌似作者已经没管了。本地安装试了下，倒是能工作。但我不满意的一点是，它把查询结果都显示在下方的弹出 quickfix 窗口内，而且默认高度不够，释义文本稍微多一些就看不完，还得把光标移过去，手动翻页查看。使用体验并不好。 现在已经是 9012 年了，我早已用上了 NeoVim，其很棒的一个特性就是支持 Floating Window。那么用 Floating Window 来显示词典释义，显示面积足够大，看完释义后再快捷键快速关闭弹窗，继续在原来窗口内工作，这样的工作流既顺畅又自然，岂不美哉？ 于是就按这个思路来吧。具体实现过程并没啥太特别的地方，这里不赘述了。反正最后上述想法得以完美实现，我的这个 Vim Plugin 已经放到了 GitHub 上，目前而言，实际体验自己感觉很满意。 附一个动态效果图： "
    }, {
    "id": 43,
    "url": "/2020/07/dip-introduction/",
    "title": "让Docker-Compose如虎添翼的DIP",
    "body": "2020/07/13 - 前一篇博客“骑鲸之路——Docker模式下的Rails开发环境构筑（翻译）”的文章末尾，作者提到了一个叫 Dip 的工具，引起了我的兴趣。作为 Evil Martions 的开源作品，品质应该是有保证的，值得一试。我经过几天的试用后，感觉很是“惊艳”，觉得完全把它看作 Docker 本地开发环境的两大杀手级生产力工具：Docker-Compose + Dip，称为“帝国双璧”亦不为过。这篇博客就来简单介绍下 Dip 的使用。 要说 Dip，得先说 Docker-Compose 这个 Docker 容器的编排工具。一般来说，一个项目都会由多个 Docker 容器组成（几乎不可能只有单个容器），比如“Rails + DB + Redis + ElasticSearch”这样。而 Docker-Compose 则通过设定一个称为docker-compose. yml的配置文件，提供了把多个容器“串联”起来的能力，让我们不用手工输入繁琐的 Docker 命令去一个个单独启动容器，这已经大幅度提高了我们使用 Docker 的方便性。下面是个docker-compose. yml的例子： version: '2'services: app:  image: ruby:2. 4-buster  environment:   - GEM_HOME=/bundle   - BUNDLE_PATH=/bundle   - HISTFILE=/app/tmp/. bash_history  working_dir: /app  volumes:   - . :/app   - bundle:/bundlevolumes: bundle:但是，当使用 Docker-Compose 稍微长一点时间后，你可能就会发现依然存在一些让你难受的地方。 比如，当每次要使用它时，都得敲下面这样的命令： docker-compose run --rm web bundle exec rails cdocker-compose这个命令本身就太长，不便于输入。当然，你可以给它设置一个 alias（例如我本地就设为dp）。然而后面那一长串又怎么办呢？想想每天的日常开发，可能这种类似的命令要输入好几十次，无论对手指还是心理，都是一种折磨-_- 再比如，使用了 Docker 容器作为开发环境，由于代码的运行、数据库的存储等就都是在容器中了，那么要运行测试或者查看数据库内表中数据时，就都得先使用如下命令进入容器： docker-compose run --rm app bash然后在容器内才能再运行所需要的命令： 8523c2:/# bundle exec rspec把一件事非得分成好几步来操作，这当然显得过于繁琐累赘了，一点也不简洁高效。 Evil Martins 的开发者们也发现了这一点。而他们则是想办法来尝试解决这个问题。所以，Dip 就应运而生。 Dip 的安装很简单，有三种方式，任选其一即可：    Homebrew 安装：Homebrew install dip     Gem 安装：gem install dip     直接下载已编译版本：   curl -L https://github. com/bibendi/dip/releases/download/v6. 0. 0/dip-`uname -s`-`uname -m` &gt; /usr/local/bin/dip chmod +x /usr/local/bin/dip 安装好之后，就可以针对你项目下的docker-compose. yml，在同级目录下添加 Dip 的配置文件dip. yml。 比如对于上面的docker-compose. yml例子，对应的dip. yml可以是这样的： version: '4'compose: files:  - docker-compose. ymlinteraction: bash:  description: Open the Bash shell in app's container  service: app  command: /bin/bash pry:  description: Open Pry console  service: app  command: . /bin/console bundle:  description: Run Bundler commands  service: app  command: bundle rspec:  description: Run Rspec commands  service: app  command: bundle exec rspec rubocop:  description: Run Rubocop commands  service: app  command: bundle exec rubocopprovision: - rm -rf Gemfile. lock - dip bundle installdip. yml配置妥当后，就可以开始“愉快”地使用 Dip 了。 首先，你能随时使用dip ls，显示配置好的可用命令，而无需每次都去打开文件查找有哪些。显示效果类似这样： bash   # Open the Bash shell in app's containerpry   # Open Pry consolebundle  # Run Bundler commandsrspec  # Run Rspec commandsrubocop # Run Rubocop commands而象上文提到的测试场景，有了 Dip，只需输入这个命令： dip rspec即可随时运行所有测试。 重要的是，Dip 会在运行命令时，首先自动找到docker-compose. yml中相应的 Service，自动启动其容器（这里是app，如果有依赖的容器也会自动启动），然后在容器内运行设定好的命令（bundle exec rspec）。在命令运行结束之后，Dip 还会贴心地自动关闭刚才启动的所有容器，减少系统资源的占用。 由此可以看到，Dip 针对上面提到的“痛点”，让我们不用登入容器就可以进行各种工作，也不用再担心容器的启动状态，把那些繁琐的操作步骤进行了完美的精简，貌似已经少到“减无可减”的地步了。 但作者认为这还不够，并不满足于此，所以他给 Dip 添加了一个更加“黑科技”的技能点。当你在自己的. bashrc或. zshrc中只要再加上这一行： eval  $(dip console) 甚至就可以把上面那些命令中的dip都省掉！比如dip rspec直接敲rspec就可以了。这样在项目下，你可以随时直接运行这些命令： bundle installrails srake routes | grep adminrspec spec/models/user_spec. rb:16而只看这些命令，别人还会以为你完全就是在原生环境而非 Docker 环境下开发呢！让你使用时根本感觉不到是在 Docker 环境下，这算是真正的大道至简了。 除此之外，Dip 还提供了诸如dip ssh、dip nginx、dip dns之类的高级功能，本文这里就暂不讨论了，读者可以参看它的官方文档自行尝试。 总而言之，就像本文标题所说的那样，Dip 让 Docker-Compose 更加如虎添翼，也让本地以 Docker 作为开发环境的体验更加高效流畅、丝般顺滑。作为我个人而言，已经把 Dip 作为必备工具加入自己的日常开发工具箱了。 "
    }, {
    "id": 44,
    "url": "/2020/07/dockeerizing-rails-development/",
    "title": "骑鲸之路——Docker模式下的Rails开发环境构筑（翻译）",
    "body": "2020/07/10 - 本文已获得原作者（Vladimir Dementyev）和 Evil Martians 授权许可进行翻译。原文介绍了一套成熟的 Rails 项目的 Docker 化开发环境如何搭建，怎样通过一些配置技巧让其高效运行。也谈到了这种模式下的开发流程和诀窍等问题。关于 Docker 作为开发环境的做法早已不是新鲜事，相关文章更是有非常多了。但本文是一名卓越的 Ruby 开发者在两年开发实践中提炼出的真正经验之谈，解答了之前自己对 Docker 开发环境模式的种种疑惑，因此觉得很有价值，遂成此文。  原文链接：Ruby on Whales: Dockerizing Ruby and Rails development — Martian Chronicles, Evil Martians’ team blog 作者：Vladimir Dementyev 站点：Evil Martians ——位于纽约和俄罗斯的 Ruby on Rails 开发人员博客。 它发布了许多优秀的文章，并且是不少 gem 的赞助商。【下面是正文】 这篇文章是对我在 RailsConf 2019 上演讲“Terraforming legacy Rails applications”的整理。今天我不是要说服你把应用程序开发切换到 Docker（不过你可以去看一下当时 RailsConf 的视频了解更多）。我的目的是分享目前自己在 Rails 项目上使用的配置，其诞生于 Evil Martians 的开发工作中。尽情享用它吧！ 声明：本文会根据最佳实践进行定期更新，参看最后的 Changelog 部分。 我从三年前就开始在自己的开发环境中使用 Docker 了（代替了 Vagrant，后者对于我的 4GB 内存笔记本来说实在太重了）。一开始，这当然并非那么令人愉悦——我花了两年时间来尝试为自己更为团队找到一个足够好的配置。 让我在这里来为你展示这个配置，并解释其（几乎）每一行吧，因为我们已经看到太多那些关于 Docker 的晦涩难懂的教程了。 相关源代码可以在 GitHub 的 evilmartians/terraforming-rails 上找到。 我们在下面范例中使用了这些技术栈：  Ruby 2. 6. 3 PostgreSQL 11 NodeJS 11 &amp; Yarn (for Webpacker-backed assets compilation)Dockerfile: Dockerfile 定义了我们 Ruby 应用的环境：这是我们作为开发者运行 servers、console（rails c）、tests、Rake tasks 以及与代码进行交互的地方。 ARG RUBY_VERSION# See explanation belowFROM ruby:$RUBY_VERSION-slim-busterARG PG_MAJORARG NODE_MAJORARG BUNDLER_VERSIONARG YARN_VERSION# Common dependenciesRUN apt-get update -qq \ &amp;&amp; DEBIAN_FRONTEND=noninteractive apt-get install -yq --no-install-recommends \  build-essential \  gnupg2 \  curl \  less \  git \ &amp;&amp; apt-get clean \ &amp;&amp; rm -rf /var/cache/apt/archives/* \ &amp;&amp; rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* \ &amp;&amp; truncate -s 0 /var/log/*log# Add PostgreSQL to sources listRUN curl -sSL https://www. postgresql. org/media/keys/ACCC4CF8. asc | apt-key add - \ &amp;&amp; echo 'deb http://apt. postgresql. org/pub/repos/apt/ buster-pgdg main' $PG_MAJOR &gt; /etc/apt/sources. list. d/pgdg. list# Add NodeJS to sources listRUN curl -sL https://deb. nodesource. com/setup_$NODE_MAJOR. x | bash -# Add Yarn to the sources listRUN curl -sS https://dl. yarnpkg. com/debian/pubkey. gpg | apt-key add - \ &amp;&amp; echo 'deb http://dl. yarnpkg. com/debian/ stable main' &gt; /etc/apt/sources. list. d/yarn. list# Application dependencies# We use an external Aptfile for that, stay tunedCOPY . dockerdev/Aptfile /tmp/AptfileRUN apt-get update -qq &amp;&amp; DEBIAN_FRONTEND=noninteractive apt-get -yq dist-upgrade &amp;&amp; \ DEBIAN_FRONTEND=noninteractive apt-get install -yq --no-install-recommends \  libpq-dev \  postgresql-client-$PG_MAJOR \  nodejs \  yarn=$YARN_VERSION-1 \  $(cat /tmp/Aptfile | xargs) &amp;&amp; \  apt-get clean &amp;&amp; \  rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* &amp;&amp; \  truncate -s 0 /var/log/*log# Configure bundlerENV LANG=C. UTF-8 \ BUNDLE_JOBS=4 \ BUNDLE_RETRY=3# Uncomment this line if you store Bundler settings in the project's root# ENV BUNDLE_APP_CONFIG=. bundle# Uncomment this line if you want to run binstubs without prefixing with `bin/` or `bundle exec`# ENV PATH /app/bin:$PATH# Upgrade RubyGems and install required Bundler versionRUN gem update --system &amp;&amp; \  gem install bundler:$BUNDLER_VERSION# Create a directory for the app codeRUN mkdir -p /appWORKDIR /app这个配置仅包含了必不可少的部分，可以被用作一个起点。我来说明下这里做了什么。 头两行看起来有点奇怪： ARG RUBY_VERSIONFROM ruby:$RUBY_VERSION-slim-buster为什么不使用FROM ruby:2. 6. 3或者任何 Ruby 的稳定版本呢？因为我们想使环境是可从外部进行配置的，以便把 Dockerfile 作为一种模板：  明确的运行时版本倚赖会在docker-compose. yml中指定（见后面）； apt的安装依赖列表存放于一个单独的文件里（见后面）我们也明确指定了 Debian 发行版（buster）以确保为其他依赖（比如 PostgreSQL）添加正确的源。 下面四行定义了 PostgreSQL、NodeJS、Yarn 和 Bundler 的版本： ARG PG_MAJORARG NODE_MAJORARG BUNDLER_VERSIONARG YARN_VERSION因为我们不期望任何人不通过 Docker Compose 来使用该 Dockerfile，所以就不提供默认值了。 然后就是实际的镜像 build 过程。首先，我们需要手动安装一些通用的系统依赖库（Git，cURL等），因为使用了 slim 基础镜像以缩小体积： # Common dependenciesRUN apt-get update -qq \ &amp;&amp; DEBIAN_FRONTEND=noninteractive apt-get install -yq --no-install-recommends \  build-essential \  gnupg2 \  curl \  less \  git \ &amp;&amp; apt-get clean \ &amp;&amp; rm -rf /var/cache/apt/archives/* \ &amp;&amp; rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* \ &amp;&amp; truncate -s 0 /var/log/*log我们会在下面解释所有这些安装的系统依赖库的细节，当谈到应用程序特定部分的时候。 通过apt安装 PostgreSQL、NodeJS、Yarn 需要添加它们 deb 包的 repos 到源的列表中。 对于 PostgreSQL（基于官方文档）： RUN curl -sSL https://www. postgresql. org/media/keys/ACCC4CF8. asc | apt-key add - \ &amp;&amp; echo 'deb http://apt. postgresql. org/pub/repos/apt/ buster-pgdg main' $PG_MAJOR &gt; /etc/apt/sources. list. d/pgdg. list注意：这就是我们使用操作系统发行版为 buster 的原因。 对于 NodeJS（根据 NodeSource repo）： RUN curl -sL https://deb. nodesource. com/setup_$NODE_MAJOR. x | bash -对于 Yarn（根据官方网页）： RUN curl -sS https://dl. yarnpkg. com/debian/pubkey. gpg | apt-key add - \ &amp;&amp; echo 'deb http://dl. yarnpkg. com/debian/ stable main' &gt; /etc/apt/sources. list. d/yarn. list现在该来安装那些依赖库了，例如，运行apt-get install： COPY . dockerdev/Aptfile /tmp/AptfileRUN apt-get update -qq &amp;&amp; DEBIAN_FRONTEND=noninteractive apt-get -yq dist-upgrade &amp;&amp; \ DEBIAN_FRONTEND=noninteractive apt-get install -yq --no-install-recommends \  libpq-dev \  postgresql-client-$PG_MAJOR \  nodejs \  yarn \  $(cat /tmp/Aptfile | xargs) &amp;&amp; \  apt-get clean &amp;&amp; \  rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* &amp;&amp; \  truncate -s 0 /var/log/*log首先，我们来说下关于 Aptfile 的技巧： COPY . dockerdev/Aptfile /tmp/AptfileRUN apt-get install \  $(cat /tmp/Aptfile | xargs)这是我从 heroku-buildpack-apt 借鉴到的，它允许在 Heroku 上安装其他包。如果你使用这个 buildpack，甚至能为本地和线上环境重用相同的 Aptfile（尽管 buildpack 提供了更多的功能）。 我们的默认 Aptfile 仅包含了一个单独的包（因为要使用 Vim 来编辑 Rails 的 Credentials）： vim在我工作过的前一个项目中，我们使用 LaTeX 和 TexLive 来生成 PDF。我们的 Aptfile 看起来是这样的（那时我还没有运用这个技巧）： vimtexlivetexlive-latex-recommendedtexlive-fonts-recommendedtexlive-lang-cyrillic通过这种方式，我们把所需要的依赖库放在一个单独文件中，使得自己的 Dockerfile 更加通用。 对于DEBIAN_FRONTEND=noninteractive，我建议去看下 answer on Ask Ubuntu。 而--no-install-recommends则通过不安装推荐的包来帮助我们节省一些空间（让镜像更加苗条）。可以参看这儿。 最后的部分RUN (apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* &amp;&amp; truncate -s 0 /var/log/*log)也为了同样的目的——清理所接收到的包文件（全部都安装好了，这些不再需要了），以及在安装期间所创建的临时文件和日志。我们需要在同一个RUN语句中做这些清理，以确保这个Docker layer不包含任何垃圾。 最后的部分几乎都是用于 Bundler： # Configure bundlerENV LANG=C. UTF-8 \ BUNDLE_JOBS=4 \ BUNDLE_RETRY=3 \# Uncomment this line if you store Bundler settings in the project's root# ENV BUNDLE_APP_CONFIG=. bundle# Uncomment this line if you want to run binstubs without prefixing with `bin/` or `bundle exec`# ENV PATH /app/bin:$PATH# Upgrade RubyGems and install required Bundler versionRUN gem update --system &amp;&amp; \  gem install bundler:$BUNDLER_VERSIONLANG=C. UTF-8设置默认为 UTF-8，否则 Ruby 会对字符串使用 US-ASCII 编码，你再也不能使用那些可爱的 emojis 👋 如果你使用&lt;root&gt;/. bundle目录来存放项目特定的 Bundler 设置（例如，针对私有 gems 的 credentials），那么就需要设置BUNDLE_APP_CONFIG。Ruby 的默认镜像就定义了这个变量以使得 Bundler 不回退到本地配置。 你可以选择性地把&lt;root&gt;/bin目录添加到PATH，以便运行命令时无需带上bundle exec前缀。我们这里默认没有加上，因为在多项目环境下它可能无法正常工作（比如，当你在 Rails 应用中有本地 gems 或 engines 的时候）。 docker-compose. yml: Docker Compose 是一个编排容器环境的工具。它使我们能够相互链接容器、定义持久化 volumes 和 services。 下面是一个典型 Rails 应用程序开发环境的 compose 文件，使用 PostgreSQL 作为数据库，Sidekiq 作为后台任务处理器： version: '2. 4'services: app: &amp;app  build:   context: .    dockerfile: . /. dockerdev/Dockerfile   args:    RUBY_VERSION: '2. 6. 3'    PG_MAJOR: '11'    NODE_MAJOR: '11'    YARN_VERSION: '1. 13. 0'    BUNDLER_VERSION: '2. 0. 2'  image: example-dev:1. 0. 0  tmpfs:   - /tmp backend: &amp;backend  &lt;&lt;: *app  stdin_open: true  tty: true  volumes:   - . :/app:cached   - rails_cache:/app/tmp/cache   - bundle:/usr/local/bundle   - node_modules:/app/node_modules   - packs:/app/public/packs   - . dockerdev/. psqlrc:/root/. psqlrc:ro  environment:   - NODE_ENV=development   - RAILS_ENV=${RAILS_ENV:-development}   - REDIS_URL=redis://redis:6379/   - DATABASE_URL=postgres://postgres:postgres@postgres:5432   - BOOTSNAP_CACHE_DIR=/usr/local/bundle/_bootsnap   - WEBPACKER_DEV_SERVER_HOST=webpacker   - WEB_CONCURRENCY=1   - HISTFILE=/app/log/. bash_history   - PSQL_HISTFILE=/app/log/. psql_history   - EDITOR=vi  depends_on:   postgres:    condition: service_healthy   redis:    condition: service_healthy runner:  &lt;&lt;: *backend  command: /bin/bash  ports:   - '3000:3000'   - '3002:3002' rails:  &lt;&lt;: *backend  command: bundle exec rails server -b 0. 0. 0. 0  ports:   - '3000:3000' sidekiq:  &lt;&lt;: *backend  command: bundle exec sidekiq -C config/sidekiq. yml postgres:  image: postgres:11. 1  volumes:   - . psqlrc:/root/. psqlrc:ro   - postgres:/var/lib/postgresql/data   - . /log:/root/log:cached  environment:   - PSQL_HISTFILE=/root/log/. psql_history  ports:   - 5432  healthcheck:   test: pg_isready -U postgres -h 127. 0. 0. 1   interval: 5s redis:  image: redis:3. 2-alpine  volumes:   - redis:/data  ports:   - 6379  healthcheck:   test: redis-cli ping   interval: 1s   timeout: 3s   retries: 30 webpacker:  &lt;&lt;: *app  command: . /bin/webpack-dev-server  ports:   - '3035:3035'  volumes:   - . :/app:cached   - bundle:/usr/local/bundle   - node_modules:/app/node_modules   - packs:/app/public/packs  environment:   - NODE_ENV=${NODE_ENV:-development}   - RAILS_ENV=${RAILS_ENV:-development}   - WEBPACKER_DEV_SERVER_HOST=0. 0. 0. 0volumes: postgres: redis: bundle: node_modules: rails_cache: packs:我们定义了八个 service。为什么要这么多？其中一些只是用来定义共享配置给别的 service 用而已（抽象 service，例如，app和backend），其他则用于使用应用程序容器（例如，runner）的特定命令。 使用这种方案，我们就不必使用docker-compose up命令来运行应用程序了，而是总能指定自己期望运行的确定 service 来运行（比如，docker-compose up rails）。这在开发时很有用，你很少需要所有 service 同时启动和运行（Webpacker、Sidekiq 等等）。 让我们来看看每个 service。 app: 该 service 的主要目的是提供所有需要的信息以构建我们的应用程序容器（定义在 Dockerfile 上方的部分）： build: context: .  dockerfile: . /. dockerdev/Dockerfile args:  RUBY_VERSION: '2. 6. 3'  PG_MAJOR: '11'  NODE_MAJOR: '11'  YARN_VERSION: '1. 13. 0'  BUNDLER_VERSION: '2. 0. 2'context为 Docker 定义了 build context：类似于 build 进程的工作目录，例如，它会被COPY命令用到。 我们明确指定了 Dockerfile 的路径，因为并没有把它放在项目根目录下，而是把所有 Docker 有关的文件一起放在一个隐藏目录. dockerdev内。 并且，如前所述，我们指定了那些依赖库的确切版本，它们在 Dockerfile 中用args所声明的。 一个我们应该注意的地方是我们对镜像打上 tag 的方式： image: example-dev:1. 0. 0在开发中使用 Docker 的一个好处就是在团队里自动同步这些配置的变动的能力。你只需要每次对本地镜像进行改动时升级其版本（或者在 arguments 或者在所依赖的文件）即可。最糟糕的则是使用example-dev:latest作为你的 tag。 保留镜像版本还有助于在两种不同的环境下工作，而不会带来任何其他麻烦。比如，当你在一个长期的“chore/upgrade-to-ruby-3”分支上工作时，能够很容易就切换到master分支并使用带旧 Ruby 版本的旧镜像，而无需重新构建任何东西。 最糟糕的就是在你的docker-compose. yml中对镜像使用latest的 tag。 我们也告知 Docker 在一个容器内为/tmp目录使用 tmpfs 以加快速度： tmpfs: - /tmpbackend: 我们来到了这篇博客中最有趣的部分。 该 service 定义了关于所有 Ruby service 的共享行为。 我们来先看下 volumes： volumes: - . :/app:cached - bundle:/usr/local/bundle - rails_cache:/app/tmp/cache - node_modules:/app/node_modules - packs:/app/public/packs - . dockerdev/. psqlrc:/root/. psqlrc:ro该 volumes 列表的第一条是挂载当前工作目录（项目根目录）到容器内的/app目录，并使用了cached策略。该/cached修饰器是在 macOS 上提高 Docker 开发效率的关键。本文中我们不深入讨论这个（我们正在就此主题写另一篇文章😉），但你可以看看这个文档。 下一行告诉容器使用一个名为bundle的 volume 来存储/usr/local/bundle的内容（这是默认存放 gems 的地方）。通过这种方式我们把 gems 数据在运行期间进行持久化：所有定义在docker-compose. yml中的 volumes 将保持 put 状态，直到我们运行docker-compose down --volumes为止。 下面三行也是为了摆脱“Docker 在 Mac 上慢”的诅咒。我们把所有生成的文件放置到 Docker 的 volumes 中以避免在 host 机器一方的繁重磁盘操作： - rails_cache:/app/tmp/cache- node_modules:/app/node_modules- packs:/app/public/packs要让 Docker 在 macOS 上足够快，遵循下面两条规则：使用:cached来挂载源文件和针对生成的内容（assets、bundle 等等）使用 volumes。 最后一行添加了一个特定的psql配置到容器。我们经常需要保存其历史命令，这里存储到了 app 的log/. psql_history文件。为什么要在 Ruby 容器内执行psql？当你运行rails dbconsole时在内部就会用到。 我们的. psqlrc文件包含下面的技巧来使其通过环境变量指定历史文件路径成为可能（允许通过PSQL_HISTFILE环境变量指定历史文件路径，否则就为默认的$HOME/. psql_history）： \set HISTFILE `[[ -z $PSQL_HISTFILE ]] &amp;&amp; echo $HOME/. psql_history || echo $PSQL_HISTFILE`让我们来谈下环境变量： environment: - NODE_ENV=${NODE_ENV:-development} - RAILS_ENV=${RAILS_ENV:-development} - REDIS_URL=redis://redis:6379/ - DATABASE_URL=postgres://postgres:postgres@postgres:5432 - WEBPACKER_DEV_SERVER_HOST=webpacker - BOOTSNAP_CACHE_DIR=/usr/local/bundle/_bootsnap - HISTFILE=/app/log/. bash_history - PSQL_HISTFILE=/app/log/. psql_history - EDITOR=vi - MALLOC_ARENA_MAX=2 - WEB_CONCURRENCY=${WEB_CONCURRENCY:-1}这里有好些东西，我重点谈一个。 首先是X=${X:-smth}的语法。其可以被翻译为“对于容器内的 X 变量，使用 host 机器的 X 环境变量值如果其存在的话，否则就用另一个值”。这样，我们让 service 运行在通过命令所提供的不同环境中就成为可能，例如，RAILS_ENV=test docker-compose up rails。 变量DATABASE_URL、REDIS_URL和WEBPACKER_DEV_SERVER_HOST把我们的 Ruby 应用连接到了其他 service。DATABASE_URL和WEBPACKER_DEV_SERVER_HOST变量是 Rails 原生所支持的（分别是 ActiveRecord 和 Webpacker）。某些库也支持REDIS_URL（Sidekiq），但并非所有（比如，Action Cable 必须明确配置才行）。 我们使用了bootsnap来提升应用程序的加载时间，把其缓存存储于跟 Bundler 数据相同的 volume 中，因为这个缓存主要是包含 gems 数据；因此，我们在做另一次 Ruby 版本升级时，就应该一起删除所有内容。 HISTFILE=/app/log/. bash_history从开发者的 UX 角度看是很重要的设置：告诉 Bash 在指定位置存储其历史命令以使其持久化。 EDITOR=vi被用在，例如，rails credentials:edit命令管理 credentials 文件时。 最后，末尾两个设置，MALLOC_ARENA_MAX和WEB_CONCURRENCY，帮助你检查 Rails 内存的处理情况。 该 service 中尚未涵盖的是： stdin_open: truetty: true它们使得该 service 是可交互的，例如，提供 TTY。我们需要这个，比如，来运行 Rails console 或者在容器内的 Bash。 这跟使用-it选项来运行 Docker 容器是相同的。 webpacker: 这里我只想强调一点就是WEBPACKER_DEV_SERVER_HOST=0. 0. 0. 0的配置：它让 Webpack dev server 从外部是可访问的（默认运行在localhost上）. runner: 要解释该 service 的目的，让我来分享一下自己在开发时使用 Docker 的方式：  启动一个 Docker daemon，运行一个自定义的docker-start脚本：#!/bin/shif ! $(docker info &gt; /dev/null 2&gt;&amp;1); then echo  Opening Docker for Mac. . .   open -a /Applications/Docker. app while ! docker system info &gt; /dev/null 2&gt;&amp;1; do sleep 1; done echo  Docker is ready to rock! else echo  Docker is up and running.  fi 然后我在项目目录下运行dcr runner（dcr是docker-compose run的 alias）以进入容器的 shell 中；这是下面命令的 alias：$ docker-compose run --rm runner 我在这个容器内运行（几乎）一切：tests、migrations、Rake tasks，等等。如你所见，我不是在需要运行一个任务时去开新容器，而是总使用同一个。 这样，我就如同多年前使用vagrant ssh那样来使用dcr runner了。 我之所以把它称为runner而不是shell，是因为它还可在容器内用来运行任意命令。 注意：runner这个 service 是一个品味问题，跟web service 相比，除了默认 command（/bin/bash）外，它并没有带来任何新东西；因此，docker-compose run runner跟docker-compose run web /bin/bash是一样的（除了短一些😉）。 Health checks: 当运行诸如db:migrate的常规 Rails 命令时，我们期望确认数据库已经启动并准备好连接了。如何告知 Docker Compose 等待所依赖的 service 直至就绪？我们可以使用健康检查！ 你可能已经注意到我们的depends_on定义并非 services 列表： backend: # . . .  depends_on:  postgres:   condition: service_healthy  redis:   condition: service_healthypostgres: # . . .  healthcheck:  test: pg_isready -U postgres -h 127. 0. 0. 1  interval: 5sredis: # . . .  healthcheck:  test: redis-cli ping  interval: 1s  timeout: 3s  retries: 30注意：健康检查仅被 Docker Compose 文件格式在 v2. 1 及更高版本以上支持；这就是我们用在开发中的原因了。 题外话：dip. yml: 如果你仍然觉得 Docker Compose 的方式过于复杂，有一个叫做 Dip 的工具，是我在 Evil Martians 的同事开发的，目的是让开发体验更加顺滑。 如果你有多个 compose 文件或平台配置，那么它尤其有用，因为它可以把这些配置粘合到一起，并提供一个通用界面来管理 Docker 开发环境。 我们将在未来为你介绍有关它的更多内容，敬请关注！ 注：特别感谢 Sergey Ponomarev 和 Mikhail Merkushin 分享了有关该主题的技巧🤘 封面图片：© NASA/JPL-Caltech, 2009 Changelog: 1. 1. 0 (2019-12-10):  Change base Ruby image to slim.  Specify Debian release for Ruby version explicitly and upgrade to buster.  Use standard Bundler path (/usr/local/bundle) instead of /bundle.  Use Docker Compose file format v2. 4.  Add health checking to postgres and redis services. "
    }, {
    "id": 45,
    "url": "/2020/07/patch-sf-mono-to-nerd-font/",
    "title": "把 SF Mono 字体 Patch 为 Nerd Font",
    "body": "2020/07/08 - 作为一名开发者，在编程中使用等宽字体是很重要的。我最近几年一直使用的是 Adobe 公司的 Source Code Pro 字体。这款等宽字体的字形设计优美，间距适中，阅读时眼睛不易疲劳，完全可以排到等宽字体的 Top 3。不过再好的东西，时间长了也有审美疲劳。正好最近看到了这一篇文章《从 DejaVu Sans Mono 换成 Hack 字体了》，作者的审美甚合我心，于是打换一款试试。 上面引用的文章中的几款字体当然都很不错，我也一一在本地尝试了，但还是差一点那种令自己心动的“感觉”。正在犹豫不决之际，无意中看到了苹果公司今年的 WWDC 大会，其中正好谈到了字体设计的主题。于是眼前一亮，因为我对于 Apple 在设计上的品味一向是非常信服的，在使用 Source Code Pro 之前，恰恰就是以 macOS 自带的 Menlo 等宽字体作为常用编程字体。而现在，Apple 的新字体已经换成了 SF（旧金山） 字体家族，其中自然也包括等宽系列，SF Mono。 因为自己的 macOS 已经升级到了 Catalina，所以实际上系统已经安装了 SF Mono 字体。只是位置比较隐秘，在字体册中是看不到的。其实际位置在 /Applications/Utilities/Terminal. app/Contents/Resources/Fonts接下来就是需要把 SF Mono 字体 Patch 上 Nerd Font 了。关于 Nerd Font 的介绍，可以参考其官网。Nerd Font 其实已经预先 Patch 好了不少常用等宽字体，在其 Release 页面可以直接下载。但 SF Mono 由于 Apple 版权原因，Nerd Font 肯定不能公然发布的，所以只能自己使用 Nerd Font 提供的 Script 来手动 Patch。 过程并不复杂，只是需要先 Homebrew 安装好 fontforge brew install fontforge然后把 Nerd Font 的 Repo 使用 git clone 到本地（Repo 非常大，所以使用 –depth=1 选项） git clone --depth=1 https://github. com/ryanoasis/nerd-fonts. git再把上面目录位置下的 SF Mono 字体复制到某个临时目录，比如叫 tmpfont。 最后，在临时目录下运行命令 fontforge -script nerd-font/font-patcher -s -c tmpfont/SFMono-Medium. otf -out . /tmpfont/patched即可看到 Patch 的过程在飞快进行。稍等片刻后，在 patched 目录下就能看到生成好的字体了。 安装好字体，把 Terminal 和 Vim 的字体都设置为 SF Mono，随便打开一个文件看看效果吧： "
    }, {
    "id": 46,
    "url": "/2020/07/integrate-stimulus-and-tailwindcss-with-rails6/",
    "title": "在 Rails 6 中整合 Stimulus 和 Tailwind CSS",
    "body": "2020/07/05 - 上一篇博客提到了 Stimulus。Stimulus 也是 Basecamp 开源发布的一个前端 JS 方案（我个人认为，跟 React、Vuejs这些 JS 框架相比，Stimulus 应该还称不上是一个框架）。Stimulus 的文档很简单，主要就 Handbook 和 Reference 两部分，基本两个小时就能看完。 Tailwind CSS 则是目前很火的一个 CSS 库，我个人很看好。它有点类似著名的 Bootstrap 框架，但又有显著不同。简单地说，Tailwind CSS 是一个让你能不写 CSS 就能实现 CSS 效果的一个 CSS 工具库。Tailwind CSS 的具体优点，我准备以后用专门的文章进行介绍，本文不做过多展开。 所以，本文就来看下在 Rails 6 中如何集成 Stimulus 和 Tailwind CSS 并进行简单地使用。 首先，当然是使用 rails new stw_demo -d postgresql来创建一个新 Rails 项目，并进行常规的database. yml、Gemfile等配置，这里不做赘述了。 集成 Stimulus 并创建一个小的 Demo: 然后，运行如下命令来安装 Stimulus rails webpacker:install:stimulus修改config/routes. rb为 root to: 'home#index'修改app/controllers/home_controller. rb为 class HomeController &lt; ApplicationController def index endend修改app/views/home/index. html. erb为 &lt;div data-controller= hello &gt; &lt;h3&gt; Default text when connect stimulus: &lt;/h3&gt; &lt;span data-target= hello. output &gt;&lt;/span&gt; &lt;h3&gt; disply text when click button: &lt;/h3&gt; &lt;div&gt; &lt;button data-action= click-&gt;hello#sayHello &gt;Say Hello!&lt;/button&gt; &lt;span data-target= hello. sayResult &gt;123&lt;/span&gt; &lt;/div&gt;&lt;/div&gt;修改app/javascript/controllers/hello_controller. js为 import { Controller } from  stimulus export default class extends Controller { static targets = [  output ,  sayResult  ] connect() {  this. outputTarget. textContent = 'Welcome, Stimulus!' } sayHello() {  this. sayResultTarget. textContent = 'Hello, Stimulus!' }}修改app/javascript/controllers/index. js为 import { Application } from  stimulus import { definitionsFromContext } from  stimulus/webpack-helpers const application = Application. start()const context = require. context( controllers , true, /_controller\. js$/)application. load(definitionsFromContext(context))然后运行rails s，浏览器打开http://localhost:3000，即可体验 Stimulus 的实际效果了。 集成 Tailwind CSS: 使用 yarn 进行安装 yarn add tailwindcssyarn add @tailwindcss/ui生成 tailwind 的配置文件 npx tailwindcss init这会在项目根目录下生成一个tailwind. config. js，加入下面内容 module. exports = { purge: [], theme: {  extend: {}, }, variants: {}, plugins: [],}然后，对根目录下的postcss. config. js进行修改，加入 require('tailwindcss'),require('autoprefixer'),postcss. config. js最终是这样 module. exports = { plugins: [  require('postcss-import'),  require('tailwindcss'),  require('autoprefixer'),  require('postcss-flexbugs-fixes'),  require('postcss-preset-env')({   autoprefixer: {    flexbox: 'no-2009'   },   stage: 3  }) ]}再在app/javascript下创建一个名为stylesheets的目录，并在其下创建一个application. scss文件 @import  tailwindcss/base ;@import  tailwindcss/components ;@import  tailwindcss/utilities ;然后在app/javascript/packs/application. js内添加如下几行 require('stylesheets/application. scss')app/javascript/packs/application. js最终看起来是这样 require( @rails/ujs ). start()require( turbolinks ). start()require( channels )import '. . /stylesheets/application. scss'至此，Tailwind CSS 就可以在 Rails 中使用了。来试试把前面的 Home index 页面应用上，把其 erb 文件改为如下 &lt;div class= py-6 mx-auto max-w-7xl sm:px-6 lg:px-8  data-controller= hello &gt; &lt;!-- Replace with your content --&gt; &lt;div class= px-4 py-6 sm:px-0 &gt;  &lt;div class= border-4 border-gray-200 border-dashed rounded-lg h-96 &gt;   &lt;div&gt;    Default text when connect stimulus:    &lt;span data-target= hello. output &gt;&lt;/span&gt;   &lt;/div&gt;   &lt;div&gt;    disply text when click button:    &lt;button class= px-4 py-2 font-bold text-white bg-blue-500 rounded hover:bg-blue-700  data-action= click-&gt;hello#sayHello &gt;Say Hello!&lt;/button&gt;    &lt;span data-target= hello. sayResult &gt;123&lt;/span&gt;   &lt;/div&gt;  &lt;/div&gt; &lt;/div&gt; &lt;!-- /End replace --&gt;&lt;/div&gt;运行rails s，访问http://localhost:3000，应该就可以看到效果了。 加入 Alpine. js: Tailwind CSS 只是一个工具库，并未提供现成的页面模块可以参考。不过不要紧，他们已经推出了 Tailwind-UI 的库，已经包含了这些内容，比如常见的页面布局，Header 设计，以及 Navigation 菜单等。因为这些需要一些简单 JS 的配合，Tailwind 官方文档里推荐了 Alpine. js 的库作为辅助。我们来把这些加入到项目中，以便快速实现一个不错的页面布局。 安装 Alpine. js yarn add alpinejs在app/javascript/packs/application. js内加入这行 require( alpinejs )然后，修改app/views/layouts/application. html. erb为如下： &lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt;  &lt;title&gt;STW Demo&lt;/title&gt;  &lt;%= csrf_meta_tags %&gt;  &lt;%= csp_meta_tag %&gt;  &lt;%= stylesheet_link_tag 'application', media: 'all', 'data-turbolinks-track': 'reload' %&gt;  &lt;%= javascript_pack_tag 'application', 'data-turbolinks-track': 'reload' %&gt; &lt;/head&gt; &lt;body&gt;  &lt;div&gt;   &lt;nav x-data= { open: false }  @keydown. window. escape= open = false  class= bg-gray-800 &gt;    &lt;div class= px-4 mx-auto max-w-7xl sm:px-6 lg:px-8 &gt;     &lt;div class= flex items-center justify-between h-16 &gt;      &lt;div class= flex items-center &gt;       &lt;div class= flex-shrink-0 &gt;        &lt;h3 class= text-lg text-gray-300  &gt;MyApp&lt;/h3&gt;       &lt;/div&gt;       &lt;div class= hidden md:block &gt;        &lt;div class= flex items-baseline ml-10 &gt;         &lt;a href= #  class= px-3 py-2 text-sm font-medium text-white bg-gray-900 rounded-md focus:outline-none focus:text-white focus:bg-gray-700 &gt;Dashboard&lt;/a&gt;         &lt;a href= #  class= px-3 py-2 ml-4 text-sm font-medium text-gray-300 rounded-md hover:text-white hover:bg-gray-700 focus:outline-none focus:text-white focus:bg-gray-700 &gt;Team&lt;/a&gt;         &lt;a href= #  class= px-3 py-2 ml-4 text-sm font-medium text-gray-300 rounded-md hover:text-white hover:bg-gray-700 focus:outline-none focus:text-white focus:bg-gray-700 &gt;Projects&lt;/a&gt;         &lt;a href= #  class= px-3 py-2 ml-4 text-sm font-medium text-gray-300 rounded-md hover:text-white hover:bg-gray-700 focus:outline-none focus:text-white focus:bg-gray-700 &gt;Calendar&lt;/a&gt;         &lt;a href= #  class= px-3 py-2 ml-4 text-sm font-medium text-gray-300 rounded-md hover:text-white hover:bg-gray-700 focus:outline-none focus:text-white focus:bg-gray-700 &gt;Reports&lt;/a&gt;        &lt;/div&gt;       &lt;/div&gt;      &lt;/div&gt;      &lt;div class= hidden md:block &gt;       &lt;div class= flex items-center ml-4 md:ml-6 &gt;        &lt;button class= p-1 text-gray-400 border-2 border-transparent rounded-full hover:text-white focus:outline-none focus:text-white focus:bg-gray-700 &gt;         &lt;svg class= w-6 h-6  stroke= currentColor  fill= none  viewBox= 0 0 24 24 &gt;          &lt;path stroke-linecap= round  stroke-linejoin= round  stroke-width= 2  d= M15 17h5l-1. 405-1. 405A2. 032 2. 032 0 0118 14. 158V11a6. 002 6. 002 0 00-4-5. 659V5a2 2 0 10-4 0v. 341C7. 67 6. 165 6 8. 388 6 11v3. 159c0 . 538-. 214 1. 055-. 595 1. 436L4 17h5m6 0v1a3 3 0 11-6 0v-1m6 0H9  /&gt;         &lt;/svg&gt;        &lt;/button&gt;        &lt;div @click. away= open = false  class= relative ml-3  x-data= { open: false } &gt;         &lt;div&gt;          &lt;button @click= open = !open  class= flex items-center max-w-xs text-sm text-white rounded-full focus:outline-none focus:shadow-solid &gt;           &lt;img class= w-8 h-8 rounded-full  src= https://images. unsplash. com/photo-1472099645785-5658abf4ff4e?ixlib=rb-1. 2. 1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=facearea&amp;facepad=2&amp;w=256&amp;h=256&amp;q=80  alt=   /&gt;          &lt;/button&gt;         &lt;/div&gt;         &lt;div x-show= open  x-transition:enter= transition ease-out duration-100  x-transition:enter-start= transform opacity-0 scale-95  x-transition:enter-end= transform opacity-100 scale-100  x-transition:leave= transition ease-in duration-75  x-transition:leave-start= transform opacity-100 scale-100  x-transition:leave-end= transform opacity-0 scale-95  class= absolute right-0 w-48 mt-2 shadow-lg origin-top-right rounded-md &gt;          &lt;div class= py-1 bg-white rounded-md shadow-xs &gt;           &lt;a href= #  class= block px-4 py-2 text-sm text-gray-700 hover:bg-gray-100 &gt;Your Profile&lt;/a&gt;           &lt;a href= #  class= block px-4 py-2 text-sm text-gray-700 hover:bg-gray-100 &gt;Settings&lt;/a&gt;           &lt;a href= #  class= block px-4 py-2 text-sm text-gray-700 hover:bg-gray-100 &gt;Sign out&lt;/a&gt;          &lt;/div&gt;         &lt;/div&gt;        &lt;/div&gt;       &lt;/div&gt;      &lt;/div&gt;      &lt;div class= flex -mr-2 md:hidden &gt;       &lt;button @click= open = !open  class= inline-flex items-center justify-center p-2 text-gray-400 rounded-md hover:text-white hover:bg-gray-700 focus:outline-none focus:bg-gray-700 focus:text-white &gt;        &lt;svg class= w-6 h-6  stroke= currentColor  fill= none  viewBox= 0 0 24 24 &gt;         &lt;path :class= {'hidden': open, 'inline-flex': !open }  class= inline-flex  stroke-linecap= round  stroke-linejoin= round  stroke-width= 2  d= M4 6h16M4 12h16M4 18h16  /&gt;         &lt;path :class= {'hidden': !open, 'inline-flex': open }  class= hidden  stroke-linecap= round  stroke-linejoin= round  stroke-width= 2  d= M6 18L18 6M6 6l12 12  /&gt;        &lt;/svg&gt;       &lt;/button&gt;      &lt;/div&gt;     &lt;/div&gt;    &lt;/div&gt;    &lt;div :class= {'block': open, 'hidden': !open}  class= hidden md:hidden &gt;     &lt;div class= px-2 pt-2 pb-3 sm:px-3 &gt;      &lt;a href= #  class= block px-3 py-2 text-base font-medium text-white bg-gray-900 rounded-md focus:outline-none focus:text-white focus:bg-gray-700 &gt;Dashboard&lt;/a&gt;      &lt;a href= #  class= block px-3 py-2 mt-1 text-base font-medium text-gray-300 rounded-md hover:text-white hover:bg-gray-700 focus:outline-none focus:text-white focus:bg-gray-700 &gt;Team&lt;/a&gt;      &lt;a href= #  class= block px-3 py-2 mt-1 text-base font-medium text-gray-300 rounded-md hover:text-white hover:bg-gray-700 focus:outline-none focus:text-white focus:bg-gray-700 &gt;Projects&lt;/a&gt;      &lt;a href= #  class= block px-3 py-2 mt-1 text-base font-medium text-gray-300 rounded-md hover:text-white hover:bg-gray-700 focus:outline-none focus:text-white focus:bg-gray-700 &gt;Calendar&lt;/a&gt;      &lt;a href= #  class= block px-3 py-2 mt-1 text-base font-medium text-gray-300 rounded-md hover:text-white hover:bg-gray-700 focus:outline-none focus:text-white focus:bg-gray-700 &gt;Reports&lt;/a&gt;     &lt;/div&gt;     &lt;div class= pt-4 pb-3 border-t border-gray-700 &gt;      &lt;div class= flex items-center px-5 &gt;       &lt;div class= flex-shrink-0 &gt;        &lt;img class= w-10 h-10 rounded-full  src= https://images. unsplash. com/photo-1472099645785-5658abf4ff4e?ixlib=rb-1. 2. 1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=facearea&amp;facepad=2&amp;w=256&amp;h=256&amp;q=80  alt=   /&gt;       &lt;/div&gt;       &lt;div class= ml-3 &gt;        &lt;div class= text-base font-medium leading-none text-white &gt;Tom Cook&lt;/div&gt;        &lt;div class= mt-1 text-sm font-medium leading-none text-gray-400 &gt;tom@example. com&lt;/div&gt;       &lt;/div&gt;      &lt;/div&gt;      &lt;div class= px-2 mt-3 &gt;       &lt;a href= #  class= block px-3 py-2 text-base font-medium text-gray-400 rounded-md hover:text-white hover:bg-gray-700 focus:outline-none focus:text-white focus:bg-gray-700 &gt;Your Profile&lt;/a&gt;       &lt;a href= #  class= block px-3 py-2 mt-1 text-base font-medium text-gray-400 rounded-md hover:text-white hover:bg-gray-700 focus:outline-none focus:text-white focus:bg-gray-700 &gt;Settings&lt;/a&gt;       &lt;a href= #  class= block px-3 py-2 mt-1 text-base font-medium text-gray-400 rounded-md hover:text-white hover:bg-gray-700 focus:outline-none focus:text-white focus:bg-gray-700 &gt;Sign out&lt;/a&gt;      &lt;/div&gt;     &lt;/div&gt;    &lt;/div&gt;   &lt;/nav&gt;   &lt;header class= bg-white shadow &gt;    &lt;div class= px-4 py-6 mx-auto max-w-7xl sm:px-6 lg:px-8 &gt;     &lt;h2 class= text-3xl font-bold leading-tight text-gray-900 &gt;      Dashboard     &lt;/h2&gt;    &lt;/div&gt;   &lt;/header&gt;   &lt;main&gt;   &lt;%= yield %&gt;   &lt;/main&gt;  &lt;/div&gt; &lt;/body&gt;&lt;/html&gt;再次运行rails s，访问http://localhost:3000，可以看到出现的常规页面布局，顶部右侧的头像点击可以弹出菜单。 最后的优化设计（加入自定义字体）: 我们有时希望使用更独特优美的字体来作为默认字体，以此增强页面的显示效果。下面我们来加入目前有点流行的 Inter 字体。 安装字体 yarn add 'typeface-inter'然后需要告诉 Tailwind CSS 使用这个字体。首先，修改app/javascript/packs/application. js加入下面这行 require('typeface-inter')再修改tailwind. config. js为 module. exports = { theme: {  fontFamily: {   body: ['inter']  },  extend: {}, }, variants: {}, plugins: [],}最后，需要修改上面的application. scss为如下以应用新字体 @import  tailwindcss/base ;html { @apply font-body;}@import  tailwindcss/components ;@import  tailwindcss/utilities ;这样，我们就搭建好了一个集成 Stimulus + Tailwind CSS + Tailwind UI + Alpine. js 的 Rails 项目基础，而且应用了自定义的 Inter 字体。接下来，就可以在这个基础上进行开发了。 "
    }, {
    "id": 47,
    "url": "/2020/07/dhh-talk-about-heystack/",
    "title": "对 Hey.com 技术栈的期待",
    "body": "2020/07/04 - 近一周来，著名的 Basecamp 公司发布了新式的 Hey. com 的邮件服务，号称是针对当前诸如 GMail 等邮件服务的一次“革命”，在 Twitter 上引发了巨大的议论风暴。而作为公司创始人、Rails 创建者的 DHH 大神，在 Twitter 上也发了多个推，来说明 Hey. com 的卓越不凡。其中一篇更是列举了 Hey. com 当前使用的技术栈，他称之为“Heystack”（甚至分享了所用到的 Gemfile），如下： - Vanilla Ruby on Rails on the backend, running on edge- Stimulus, Turbolinks, Trix + NEW MAGIC on the front end- MySQL for DB (Vitess for sharding)- Redis for short-lived data + caching- ElasticSearch for indexing- AWS/K8S由于 Hey. com 的（网页端）使用体验相当流畅顺滑（提供 7 天免费试用，有兴趣的同学可以去感受下。但正式使用的话则是按 $99/年 收费^_^），这个列表中的一项引发了诸多人的强烈兴趣： Stimulus, Turbolinks, Trix + NEW MAGIC on the front end这一项明显是指 Hey. com 当前所用到的前端技术，而所谓的“NEW MAGIC”到底是什么，在 DHH 本条推特下有很多人猜测，甚至直接询问他。DHH 则热情洋溢地回复说不久之后 Basecamp 会开源发布有关代码。我个人猜测有可能是 Stimulus 2. 0 + WebSocket 等技术实现，类似于 Elixir 语言的 Phoenix 框架中的 LiveView。不管怎样，DHH 的技术品味和 Basecamp 的名头，都相当值得给以期盼。让我们拭目以待吧。 "
    }, {
    "id": 48,
    "url": "/2020/07/programming-elixir-1-6-chapter-4-part2/",
    "title": "《Programming Elixir >= 1.6》第四章：基本语法（节选二）",
    "body": "2020/07/03 - 这是《Programming Elixir &gt;= 1. 6》第四章的第二部分。不多说了，直接上正文吧。 【下面是正文】 （接第一部分） Binaries: 有时你需要以比特位（bit）和字节（byte）序列的形式访问数据。例如，JPEG 和 MP3 文件的头部就有一些字段，那里单个字节可以编码成两三个单独值。 Elixir 通过二进制数据类型来做到这些。二进制字面量被包含在&lt;&lt;和&gt;&gt;之间。 基本的语法是将连续的整数转成字节： iex&gt; bin = &lt;&lt; 1, 2 &gt;&gt;&lt;&lt;1, 2&gt;&gt;iex&gt; byte_size bin2你可以添加修饰符来控制每个字段的类型和大小。下面的例子是一个单字节包含三个字段，大小分别是2、4、2比特位。（示例中使用了一些内置库函数来显示二进制数据的结果） iex&gt; bin = &lt;&lt;3 :: size(2), 5 :: size(4), 1 :: size(2)&gt;&gt;&lt;&lt;213&gt;&gt;iex&gt; :io. format( ~-8. 2b~n , :binary. bin_to_list(bin))11010101:okiex&gt; byte_size bin1二进制数据既重要却又神秘。重要在于 Elixir 用它们来表示 UTF 字符串，神秘是因为至少在最初阶段你都不太可能直接使用它们。 Dates and Times: Elixir 1. 3 添加了一个日历（calendar）模块和四个新的日期与时间相关的类型。最初，它们只不过用来放置数据，但 Elixir 1. 5 开始为它们添加一些功能。 Calendar模块代表了操作日期的规则。当前仅仅实现了Calendar. ISO，即公历的 ISO-8601 表示。 Date类型处理年、月、日，以及对儒略历的引用。 iex&gt; d1 = Date. new(2018, 12, 25){:ok, ~D[2018-12-25]}iex&gt; {:ok, d1} = Date. new(2018, 12, 25){:ok, ~D[2018-12-25]}iex&gt; d2 = ~D[2018-12-25]~D[2018-12-25]iex&gt; d1 == d2trueiex&gt; Date. day_of_week(d1)2iex&gt; Date. add(d1, 7)~D[2019-01-01]iex&gt; inspect d1, structs: false %{__struct__: Date, calendar: Calendar. ISO, day: 25, month: 12, year: 2018} （~D[…]和~T[…]是 Elixir 中的魔符sigil用法。它们是一种构建值的字面量的方式。当我们读到字符串和二进制时会再遇见它们。） Elixir 也可以表示一个日期的范围： iex&gt; d1 = ~D[2018-01-01]~D[2018-01-01]iex&gt; d2 = ~D[2018-06-30]~D[2018-06-30]iex&gt; first_half = Date. range(d1, d2)#DateRange&lt;~D[2018-01-01], ~D[2018-06-30]&gt;iex&gt; Enum. count(first_half)181iex&gt; ~D[2018-03-15] in first_halftrue时间类型处理时、分、秒，以及几分之一秒。后者存储为一个包含微秒和有效位数的元组（tuple）。（时间值与秒中有效位数相关的事实意味着~T[12:34:56. 0]跟~T[12:34:56. 00]是不相等的。） iex&gt; {:ok, t1} = Time. new(12, 34, 56){:ok, ~T[12:34:56]}iex&gt; t2 = ~T[12:34:56. 78]~T[12:34:56. 78]iex&gt; t1 == t2falseiex&gt; Time. add(t1, 3600)~T[13:34:56. 000000]iex&gt; Time. add(t1, 3600, :millisecond)~T[12:34:59. 600000]一共有两种日期时间的类型：DateTime和NaiveDateTime。Naive 版本只包含日期和时间，前者则还能关联时区。~N[. . . ]的魔符方式可以创建NaiveDateTime的结构体。 如果你在代码中使用日期和时间，则可能需要使用第三方库，例如 Lau Taarnskov 的日历库，来扩充这些内置类型。 Names, Source Files, Conventions, Operators, and So On: Elixir 的标识符必须以字母和下划线开头，后面可跟字母、数字和下划线。这里的字母是指任何 UTF-8 字母的字符（可带组合标记），而数字是指 UTF-8 十进制数的字符。如果你使用 ASCII，那么毋需担心。标识符可用问号或感叹号结尾。 下面是一些合法命名的变量例子： name josé _age まつもと _42 адрес!不合法命名的变量例子如下： name• a±2 42模块（module）、记录（record）、协议（protocol）和行为（behavior）的名称都以大写字母开头，且是驼峰式的（如 BumpyCase）。其他标识符以小写字母或下划线开头，且惯常用下划线分隔单词。当变量首字符是下划线时，只要它在模式匹配或函数参数列表中不被使用，那么 Elixir 是允许的。 惯例上源码文件都使用两个字符的缩进——且用 space 而非 tab。 注释以#开头直至一行的结尾。 Elixir 自带了一个代码格式化器，用来把代码转化成”统一规范”的格式。后面会提到它。本书中的很多示例都会跟随该规范（除去个别我认为有点丑陋外）。 Truth: Elixir 有三种特别的关于布尔操作的值：true、false和nil。nil在布尔上下文中被视作 false。 （提一下：这三种值都是相同名称的原子的别名，所以true就是原子:true。） 大多上下文中，任何不是false或nil的值都被认为是 true。有时我们把这称为 truthy 而不叫 true。 Operators: Elixir 有丰富的操作符。这儿只列出本书中用到的一部分： 比较运算符 a === b # strict equality (so 1 === 1. 0 is false)a !== b # strict inequality (so 1 !== 1. 0 is true)a == b # value equality (so 1 == 1. 0 is true)a != b # value inequality (so 1 != 1. 0 is false)a &gt; b # normal comparisona &gt;= b # :a &lt; b #:a &lt;= b #Elixir中的排序比较不像许多语言那样严格，因为你可以比较不同类型的值。如果类型相同或者兼容（如3 &gt; 2或3. 0 &lt; 5），比较会使用自然排序。否则会基于如下规则来比较： number &lt; atom &lt; reference &lt; function &lt; port &lt; pid &lt; tuple &lt; map &lt; list &lt; binary布尔运算符 （操作符期望其第一个参数为 true 或者 false） a or b # true if a is true; otherwise ba and b # false if a is false; otherwise bnot a  # false if a is true; true otherwise短路布尔运算符 操作符可使用任何类型作为参数。任何不是false或nil的值都被认为是 true。 a || b # a if a is truthy; otherwise ba &amp;&amp; b # b if a is truthy; otherwise a!a # false if a is truthy; otherwise true算术运算符 + - * / div rem整数相除会得到一个浮点数结果。使用div(a, b)可得到整数。 rem是余数操作符，作为函数来调用（rem(11, 3) =&gt; 2）。它与普通模运算的不同之处在于结果与函数的第一个参数具有相同的符号。 连接运算符 binary1 &lt;&gt; binary2 # concatenates two binaries (Later we'll          # see that binaries include strings. )list1 ++ list2   # concatenates two listslist1 -- list2   # removes elements of list 2 from a copy of list 1in运算符 a in enum # tests if a is included in enum (for example,      # a list, a range, or a map). For maps, a should      # be a {key, value} tuple. Variable Scope: Elixir 基于词法域，域的基本单元是函数体。定义在函数内部的变量（包括函数参数）都是函数的局部变量。此外，模块也定义了一个局部变量域，但这些变量只能在模块顶层访问，而不能在模块中定义的函数内访问。 Do-block Scope: 很多语言允许你把多个代码语句放到一起作为单个代码块，通常都使用大括号包起来。下面是个 C 语言的例子： int line_no = 50;/* . . . . . */if (line_no == 50) { printf( new-page\f ); line_no = 0;}Elixir 没有类似这样的代码块，但它用一些其他的方式来实现。最常见的是do代码块： line_no = 50# . . . if (line_no == 50) do IO. puts  new-page\f  line_no = 0endIO. puts line_no然而，Elixir 中这是一种危险的代码写法。特别是，很容易忘记在代码块外面初始化line_no，且在代码块后又依赖于line_no其值。这时，你会看到一个警告提示： $ elixir back_block. exwarning: the variable  line_no  is unsafe as it has been set inside one of: case, cond, receive, if, and, or, &amp;&amp;, ||. Please explicitly return the variable value instead. Here's an example:  case integer do   1 -&gt; atom = :one   2 -&gt; atom = :two  endshould be written as  atom =   case integer do1 -&gt; :one2 -&gt; :two endUnsafe variable found at: t. ex:100The with Expression: with表达式有双重用途。首先，你可以用它定义一个局部变量域。当你计算某个东西需要一些临时变量，又不想让它们泄漏到外部域的时候，可以使用with。其次，它能让你掌控一些模式匹配失败情况的处理。例如，文件/etc/passwd包含这样的文本 _installassistant:*:25:25:Install Assistant:/var/empty:/usr/bin/false_lp:*:26:26:Printing Services:/var/spool/cups:/usr/bin/false_postfix:*:27:27:Postfix Mail Server:/var/spool/postfix:/usr/bin/false行中的两个数字是对应用户的用户 ID 和组 ID。 下面的代码是查找_lp用户的对应值。 content =  Now is the time lp = with {:ok, file} = File. open( /etc/passwd ),   content = IO. read(file, :all), # note: same name as above   :ok = File. close(file),   [_, uid, gid] = Regex. run(~r/^lp:. *?:(\d+):(\d+)/m, content)  do    Group: #{gid}, User: #{uid}   endIO. puts lp #=&gt; Group: 26, User: 26IO. puts content #=&gt; Now is the time格式化代码的比较 with语句正好是 Elixir 关于代码格式化上还未取得一致的例子。如果使用其内置代码格式化器，格式化的结果是这样的。 content =  Now is the time lp = with {:ok, file} = File. open( /etc/passwd ),    content = IO. read(file, :all),    :ok = File. close(file),    [_, uid, gid] = Regex. run(~r/^_lp:. *?:(\d+):(\d+)/m, content) do   Group: #{gid}, User: #{uid}  end# =&gt; Group: 26, User: 26IO. puts(lp)# =&gt; Now is the timeIO. puts(content)哪种更好我留给你来判断了。 with表达式让我们在打开文件、读取内容、关闭文件和查找某行时能更高效地使用临时变量。with中的变量被传递为后面do块的参数使用。 变量content是with的局部变量，不会被在外部访问到。 with and Pattern Matching: 上面的示例中，with表达式的头部使用=来进行基本的模式匹配。其中任何一个匹配失败，都会抛出一个MatchError异常。但也许我们以一种更优雅的方式来处理。这里&lt;-就能一展身手了。如果在with表达式中使用&lt;-代替=，它依然进行匹配，但匹配失败时会返回无法匹配的值。 iex&gt; with [a|_] &lt;- [1,2,3], do: a1iex&gt; with [a|_] &lt;- nil, do: anil我们来使用这种方法让上面示例的with语句在无法找到用户时返回nil而不是抛出一个异常。 result = with {:ok, file} = File. open( /etc/passwd ),     content = IO. read(file, :all),     :ok = File. close(file),     [_, uid, gid] &lt;- Regex. run(~r/^xxx:. *?:(\d+):(\d+)/, content)    do      Group: #{gid}, User: #{uid}     endIO. puts inspect(result) #=&gt; nil当我们试图匹配用户 xxx 时，Regex. run会返回nil。这使得匹配失败，nil成为with的返回值。 A Minor Gotcha: 在表面之下，with被 Elixir 视为是一个函数或宏（macro）的调用。这意味着你不能这样写： mean = with             # WRONG!     count = Enum. count(values),     sum  = Enum. sum(values)    do    sum/count    end相反，你可以把第一个参数和with写在同一行： mean = with count = Enum. count(values),      sum  = Enum. sum(values)    do    sum/count    end或者使用括号： mean = with (     count = Enum. count(values),     sum  = Enum. sum(values)    do    sum/count    end)和其他do语句一样，也有简写方式可用： mean = with count = Enum. count(values),      sum  = Enum. sum(values)    do: sum/countEnd of the Basics: 至此我们已经讲完了 Elixir 语言的底层部分。接下来两章里我们将会讨论如何创建匿名函数、模块和具名函数。 "
    }, {
    "id": 49,
    "url": "/2020/07/programming-elixir-1-6-chapter-4-part1/",
    "title": "《Programming Elixir >= 1.6》第四章：基本语法（节选一）",
    "body": "2020/07/02 - 其他编程语言中的常规语法介绍在《Programming Elixir &gt;= 1. 6》里直到第四章才终于姗姗来迟。这一章介绍的是 Elixir 中全部的各种内置类型。函数在 Elixir 中也是一种类型，但会用单独的一章来专门介绍而不在本章之内。出人意料的是，字符串和结构体也不在本章里，因为它们不是 Elixir 的基本类型，而是由基本类型构成的高级类型。 由于本章涵盖内容太多，因此跟其他语言类似的一些类型就不发出来了，比如数字、正则表达式等，而只节选了一些 Elixir 特有的类型，比如元组等几种集合类型等。想要了解 Elixir 所有这些基本类型的知识，可以在去查阅官方文档或者本书该章节的原文。 即使经过上述节选之后，本章的内容还是显得太长了，所以我不得不把其分成两部分发出。这是节选的第一部分。 【下面是正文】 4. Elixir Basics: 这一章我们将看看 Elixir 中的类型，以及开始时需要了解的其他一些内容。本章特意写得不长——作为一名开发者你当然知道什么是整数，所以我不会在这些小事上侮辱大家的智商。相反，我会聊一些 Elixir 中特别的东西。 Built-in Types: Elixir 的内置类型有：  Value types:     Arbitrary-sized integers【整数】   Floating-point numbers【浮点数】   Atoms【原子】   Ranges【范围】   Regular expressions【正则表达式】    System types:     PIDs and ports   References    Collection types:     Tuples【元组】   Lists【列表】   Maps【映射】   Binaries   函数也是一种类型。下一章会用专门的章节来讲它。 你可能很惊讶上面的列表中没有包含诸如字符串和结构体这样的东西。Elixir 是有的，但它们是由上述基本类型来构成。它们全都很重要。字符串同样有专门的章节讲，列表（list）和映射（map）有好几章来讲（包括其他类似字典的类型）。映射那一章也会讲到 Elixir 的结构体。 最后，关于正则表达式（Regular expression）和范围（Range）是否是值类型还有些争议。从技术上讲，它们不是——表象之下它们都只是结构体。但当前把它们看作是不同的类型会更方便。 Value Types: Elixir 的值类型指的是数字（number）、原子（atom）、范围（range）和正则表达式（regular expression）。 ……节略…… Atoms【原子】: 原子是代表某事物名称的常量。它以一个冒号:开头，后面跟一个原子单词或 Elixir 操作符。原子单词由一系列UTF-8字母（包括组合标记）、数字、下划线和符号（@）构成。它能以感叹号或问号结尾。你还可以通过把冒号后的字符用双引号括起来以创建包含任意字符的原子。下面这些都是合法的原子： :fred :is_binary? :var@2 :&lt;&gt; :=== : func/3  : long john silver  :эликсир :mötley_crüe原子的名称就是它的值。两个相同名称的原子在比较时始终会被看作相等的，即使它俩是由远隔重洋的两台电脑上的不同应用程序所创建。 我们会把原子用在非常多的标记值（tag value）上。 ……节略…… Collection Types: 到上面为止我们看到的类型在其他编程语言中很常见。 现在起会开始看到更多独特的类型，所以我们接下来要详细介绍。 Elixir 集合可以包含任何类型的值（包括使用其他集合）。 Tuples【元组】: 元组是有序的值的集合。跟所有 Elixir 的数据结构一样，元组一旦创建即不可改变。 元组写在大括号中，元素用逗号分隔。 { 1, 2 } { :ok, 42,  next  } { :error, :enoent }一般而言 Elixir 的元组包含二至四个元素——更多的话你可能应该去看看map或者struct。 可以把元组用在模式匹配中： iex&gt; {status, count, action} = {:ok, 42,  next }{:ok, 42,  next }iex&gt; status:okiex&gt; count42iex&gt; action next 在函数中，当没有错误时，返回一个首元素为原子:ok的元组是很常见的事。下面是一个例子（假设你当前目录中有一个名为mix. exs的文件）： iex&gt; {status, file} = File. open( mix. exs ){:ok, #PID&lt;0. 39. 0&gt;}因为文件打开成功，所以元组包含一个:ok的状态和一个可用于访问文件内容的 PID。 一个常见的做法是写一个预设成功的匹配： iex&gt; { :ok, file } = File. open( mix. exs ){:ok, #PID&lt;0. 39. 0&gt;}iex&gt; { :ok, file } = File. open( non-existent-file )** (MatchError) no match of right hand side value: {:error, :enoent}打开第二个文件失败，返回元组的首元素是:error。这使得匹配失败，错误信息表明第二个元素包含了失败原因——enoent是 Unix 中”文件不存在”的意思。 Lists【列表】: 我们已经见过了 Elixir 的列表字面量语法[1, 2, 3]。你因此可能会认为列表和其他语言中的数组很像，但并不是。（实际上，元组才是 Elixir 中跟数组最接近的）相反，列表实际上是链表的数据结构。       列表的定义         列表可以是空的，也可以是头部和尾部。 头部包含一个值，尾部本身就是一个列表。   （如果你用过 Lisp 语言，会看到二者很像。） 后面在《Lists and Recursion》章中会讲到，列表的递归定义是 Elixir 编程的核心。 处于其实现方式，列表很容易线性遍历，但是以随机顺序访问它们则代价昂贵。（要获取第 n 个元素，必须要扫描 n - 1 个前面的元素。）获取列表的头部并提取尾部则总是开销很小的。 列表还有一个性能特征。还记得我们说过所有的 Elixir 数据结构都是不可变的吗？这意味一个列表一旦被创建就永不可变。所以，如果我们要移除其头部，保留尾部，不必拷贝这个列表，相反可以返回一个指向尾部的指针。这是将在第 7 章《Lists and Recursion》中介绍的所有列表遍历技巧的基础。 Elixir 对于列表有一些特别的操作符： iex&gt; [1,2,3] ++ [4,5,6][1, 2, 3, 4, 5, 6]iex&gt; [1, 2, 3, 4] -- [2, 4][1, 3]iex&gt; 1 in [1,2,3,4]trueiex&gt;  wombat  in [1, 2, 3, 4]falseKeyword Lists【关键字列表】由于我们经常需要键值对的简单列表，Elixir 提供了一种快捷方式。如果我们这样写： [ name:  Dave , city:  Dallas , likes:  Programming  ]Elixir 会把它转换成以二元元组作为元素的列表： [ {:name,  Dave }, {:city,  Dallas }, {:likes,  Programming } ]而且，当一个关键字列表作为函数最后一个参数时，Elixir 允许我们不写中括号。 DB. save record, [ {:use_transaction, true}, {:logging,  HIGH } ]上面的代码可以简写成： DB. save record, use_transaction: true, logging:  HIGH 在任何预期得到一个值的列表的上下文中，如果关键字列表是其最后一项，我们也可以不要括号。 iex&gt; [1, fred: 1, dave: 2][1, {:fred, 1}, {:dave, 2}]iex&gt; {1, fred: 1, dave: 2}{1, [fred: 1, dave: 2]}Maps【映射】: 映射是键值对的集合。映射字面量像这样： %{ key =&gt; value, key =&gt; value }下面是一些示例： iex&gt; states = %{  AL  =&gt;  Alabama ,  WI  =&gt;  Wisconsin  }%{ AL  =&gt;  Alabama ,  WI  =&gt;  Wisconsin }iex&gt; responses = %{ { :error, :enoent } =&gt; :fatal, { :error, :busy } =&gt; :retry }%{{:error, :busy} =&gt; :retry, {:error, :enoent} =&gt; :fatal}iex&gt; colors = %{ :red =&gt; 0xff0000, :green =&gt; 0x00ff00, :blue =&gt; 0x0000ff }%{blue: 255, green: 65280, red: 16711680}第一个例子，键是字符串。第二个的是元组，第三个的是原子。尽管一个映射的所有键通常都会是相同的类型，但这并非必须。 iex&gt; %{  one  =&gt; 1, :two =&gt; 2, {1,1,1} =&gt; 3 }%{:two =&gt; 2, {1, 1, 1} =&gt; 3,  one  =&gt; 1}如果键是原子，你可以使用和关键字列表相同的简写方式： iex&gt; colors = %{ red: 0xff0000, green: 0x00ff00, blue: 0x0000ff }%{blue: 255, green: 65280, red: 16711680}你也能在映射的键中使用表达式： iex&gt; name =  José Valim  José Valim iex&gt; %{ String. downcase(name) =&gt; name }%{ josé valim  =&gt;  José Valim }为什么我们同时拥有映射和关键字列表？映射只允许键是唯一的，而关键字列表的键可以重复。映射是高效的（特别是当其内容增长时），而且还可用于 Elixir 的模式匹配中，这个后续章节中会讨论到。 一般来说，使用关键字列表来执行命令行参数和传递选项等操作，在需要关联数组时使用映射。 Accessing a Map: 你通过映射的键来获取值。中括号的语法适用于所有映射： iex&gt; states = %{  AL  =&gt;  Alabama ,  WI  =&gt;  Wisconsin  }%{ AL  =&gt;  Alabama ,  WI  =&gt;  Wisconsin }iex&gt; states[ AL ] Alabama iex&gt; states[ TX ]niliex&gt; response_types = %{ { :error, :enoent } =&gt; :fatal,. . . &gt; { :error, :busy } =&gt; :retry }%{{:error, :busy} =&gt; :retry, {:error, :enoent} =&gt; :fatal}iex&gt; response_types[{:error,:busy}]:retry如果键是原子，可以使用点号表示法： iex&gt; colors = %{ red: 0xff0000, green: 0x00ff00, blue: 0x0000ff }%{blue: 255, green: 65280, red: 16711680}iex&gt; colors[:red]16711680iex&gt; colors. green65280当使用点号表示法时，如果没有对应的键就会得到一个 KeyError 的错误。 【待续】 "
    }, {
    "id": 50,
    "url": "/2020/07/programming-elixir-1-6-chapter-3/",
    "title": "《Programming Elixir >= 1.6》第三章：不可变性",
    "body": "2020/07/01 - 《Programming Elixir &gt;= 1. 6》第三章介绍了 Elixir 的另一块基石，不可变性（Immutability）。我阅读本书之后的个人体会是，Elixir 一共有三大基石：模式匹配、不可变性和 OTP，以这三块基石为底，José Valim（Elixir 的创建者。他的名字是葡萄牙语，应读作：何塞·瓦里姆）构建起了整个语言的宏伟大厦。 Dave Thomas 同样认识到了这一点，所以紧接着前一章说完模式匹配，这一章就马上对不可变性进行描述，而不是开始堆砌常规语法的说明。因为前两者是 Elixir 基础编程的基石，而 OTP 是 Elixir 并发编程的基石。理解了模式匹配和不可变性，再看后续的基础语法时很多地方才能融会贯通，否则就会一头雾水：OO里不是这样的啊？ 来看一下 Dave Thomas 对 Elixir 的不可变性是如何介绍的吧。 【下面是正文】 3. Immutability: 如果你听说过函数式编程狂热爱好者，那你应该知道人们针对不可变性做了大量工作——事实上对于一门函数式语言，数据一旦创建就不可被更改。 确实，Elixir 强制实行了不可变数据。 为什么？ You Already Have (Some) Immutable Data: 先忘掉 Elixir 一会儿。想一想你现在选用的编程语言，假设你写了这样的代码： count = 99do_something_with(count)print(count)你期望输出结果 99。当它没有这样输出时你会很吃惊。在你心中，99 就应该一直是 99。 现在，你当然能绑定一个新的值到你的变量，但这不能改变 99 就是 99 的事实。 想象一下，你到了一个无法值得依赖的语言世界中编程——在这里，你的一些代码可能在多处同时运行，且能改变 99 的值。函数do_something_with在后台被调用着，99 作为参数被传递进去。而传递过去的参数内容能被改变。突然间，99 变成了 100。 你彻底混乱了（理所当然）。更糟的是，你无法再保证自己的代码一定能得到正确的运行结果了。 还是你上面的语言，看下这个： array = [ 1, 2, 3 ]do_something_with(array)print(array)和前面一样，你还是期望print方法输出[1, 2, 3]。但很多语言中，do_something_with会把接收的 array 作为指针。当它改变第二个元素或者完全删除内容时，输出将不再符合你的期望。这样看你的代码以及它做了什么就更困难了。 再进一步——在多个线程，且每个都能访问该 array 的情况下，谁能知道全部线程都开始修改操作后这个 array 的状态会怎样？ 导致所有这些问题的原因就在于很多语言中的复合数据结构都是可变的——你可以修改其全部或部分的内容。当你的代码会在多处同时去操作数据时，悲剧就产生了。 巧合的是，在我撰写本章的那天，Jessica Kerr (@jessitron) 恰好发推说：  GOTO was evil because we asked, “how did I get to this point of execution?” Mutability leaves us with, “how di I get to this state?” 说得对极了。 Immutable Data Is Known Data: Elixir 避开了这些问题。Elixir 中，所有的值都是不可变的。即使是特别复杂的嵌套列表（list）、数据库记录——都跟最简单的整数一样，全都不可变。 在 Elixir 中，一旦一个变量指向了一个列表（list），如[1, 2, 3]，你就能确信它总是指同样的值，除非你重新绑定它到其他。这就让并发的处理不会再是困扰了。 那么如何把 100 加到[1, 2, 3]的每个元素上呢？Elixir 通过产生一个初始对象的拷贝并包含新的值来实现。初始对象保持不变，这样刚才的操作不会影响到任何使用该初始对象的代码。 这完美契合了编程就是关于数据转换的思想。当我们修改[1, 2, 3]时，并不直接修改，而是把它转换为新的数据。 Performance Implications of Immutability: 很容易就会认定这种方式效率低下。毕竟，任何情况下你只要修改数据就必须创建一份它的拷贝，而且会留下太多旧数据作为垃圾回收。那么我们来依次看一下。 Copying Data: 尽管一般都认为数据拷贝是效率低下的，但这里恰恰相反。因为 Elixir 知道现有数据不可变，所以在构建新结构时，就能部分或整体的重用它。 来看下面的代码。（它使用了一个新操作符，[head|tail]，用来生成一个新列表。head作为其第一元素，tail作为余下的元素。我们后面会用一整章来讲列表和递归。这里先直接用好了。） iex&gt; list1 = [ 3, 2, 1 ][3, 2, 1]iex&gt; list2 = [ 4 | list1 ][4, 3, 2, 1]很多语言中，list2 会通过创建一个包含 4，3，2，1 的新列表来得到。list1 中的三个元素会被复制到 list2 的尾部。因为 list1 可变，所以这是必须的。 但是 Elixir 知道 list1 永不改变，所以它只需要简单地构建一个头部为 4 和尾部为 list1 的新列表就行了。 Garbage Collection: 另一个关于数据转换式语言性能的诟病是，当你从旧数据创建新数据时（拷贝方式），经常会遗留下不再使用的旧数据。这会留下太多东西在内存堆栈中，而不得不让垃圾回收器去处理它们。 很多现代语言都有垃圾回收器，而开发者们越来越怀疑其作用——它们会在背后悄悄地影响性能。 然而 Elixir 厉害的地方在于，你可以在代码中用很多很多的进程，而每个进程都有自己的内存堆栈。程序的数据在这些进程中是分开的，所以每个堆栈都非常非常小，远比把全部数据放在一个大的内存堆栈中要小。结果就是，垃圾回收器运行更快。如果一个进程在它的堆栈满了之前就终止，其所有数据被丢弃——那甚至根本都不需要垃圾回收器了。 Coding with Immutable Data: 一旦你接受了这个概念，那么以不可变数据来编程会简单的出奇。你只需要记住任何转换数据的函数都会返回一个该数据的新拷贝。因此，我们永远不会把一个字符串转换为大写字母，而是返回一个该字符串被大写字母化后的拷贝。 iex&gt; name =  elixir  elixir iex&gt; cap_name = String. capitalize name Elixir iex&gt; name elixir 如果你之前是用面向对象语言，也许你会不喜欢我们使用String. capitalize name而不是name. capitalize()的写法。但在面向对象语言中，对象通常都拥有可变的状态。当你进行诸如name. capitalize()之类的调用时，无法立即清楚这是要更改名称的内部表示，或返回大写字母副本，还是两者都有。有太多产生歧义的空间了。 函数式语言总是转换数据，从不会去修改它。其语法都会时时刻刻提醒我们。 理论知识讲得够多了。该是开始学习它的时候了。下一章我们将快速概览基本的数据类型和一些语法，也会看到函数（function）和模块（module）。 "
    }, {
    "id": 51,
    "url": "/2020/06/rspec-9-tips/",
    "title": "写好RSpec的9个技巧",
    "body": "2020/06/30 - 看到这篇关于 Rspec 的9个技巧，觉得不错，就简略翻译一下，便于今后查阅。 【以下是正文】 编写好的测试用例跟编写好的代码一样重要。好的 specs 将如同好的文档那样帮助识别 bug。 这儿有 9 个提升 RSpec 的技巧。 有两条原则贯穿这些技巧：  DRY 针对正确的目标在正确的地方使用正确的工具我们开始吧！！！ 1、把文件组织放在正确的位置: 对于每个测试用例有三个基本的 block：  Setup —— before,let,let! Assert —— it Teardown —— after因此，代码应该放置于对应的合适位置。 # BAD#describe '#sync' do it 'updates the local account balance' do  local_account. open  transfer(1000)  expect { local_account. sync }. to change { local_account. balance }. by(1000)  widthdraw_all  local_account. close endend# GOOD#describe '#sync' do subject { local_account. sync } before do # Setup  local_account. open  transfer(1000) end after do # Teardown  widthdraw_all  local_account. close end it 'updates the local account balance' do # Assert  expect { subject }. to change { local_account. balance }. by(1000) endend2、避免 mock global classes/modules/objects: Global classes/modules/objects 趋向于被用在当前测试空间之外的多个地方。对这些组件的 Mock 将会违背单元测试的隔离原则，这会导致相关的边界效应。 在 Mock class 的new方法时，这个原则更为有效。 # BAD#class UserService attr_reader :user def verify_email  # . . .   email_service = EmailService. new(user)  email_service. send_confirmation_email  # . . .  endenddescribe UserService do describe '#verify_email' do  before do   email_service = double(:email_service)   # BAD: Mocking `new` method of EmailService   allow(EmailService). to receive(:new). and_return(email_service)   allow(email_service). to receive(:send_confirmation_email)  end endend# GOOD#class UserService attr_reader :user def verify_email  email_service = generate_email_service  email_service. send_confirmation_email end private # You can also use memoization if ONLY 1 instance of EmailService is needed def generate_email_service  EmailService. new(user) endenddescribe UserService do describe '#verify_email' do  before do   email_service = double(:email_service)   # GOOD: Mocking its own method `generate_email_service`   allow(described_class). to receive(:generate_email_service). and_return(email_service)   allow(email_service). to receive(:send_confirmation_email)  end endend3、使用 instance_double 代替 double: 当你要创建一个 class 实例的 mock 时，instance_double是更安全的选择。跟double不同，如果被 mock 的行为被实现为所提供 class 的实例方法，则 instance_double会抛出异常。与double相比，这允许我们捕获更深层次的问题。 class FootballPlayer def shoot  # . . . shoot. . .  endendmessi = instance_double(FootballPlayer)allow(messi). to receive(:shoot) # OKallow(messi). to receive(:shoot). with('power') # Wrong numbers of argumentsallow(messi). to receive(:score) # Player does not implement: scoreronaldo = double('FootballPlayer')allow(ronaldo). to receive(:shoot) # OKallow(ronaldo). to receive(:shoot). with('power') # OK - but silent failureallow(ronaldo). to receive(:score) # OK - but silent failure4、对于测试目标使用 DESCRIBE，对于 scenario 使用 CONTEXT: 这只是一种让你的代码读起来更流利的方式。 describe UserStore do describe '. create' do  context 'when user does not exists' do   it 'creates a new user' do    # . . .    end   describe 'the newly created user' do    it 'has the correct attributes' do     # . . .     end   end  end  context 'when user already exists' do   it 'raises error' do    # . . .    end  end endend5、DESCRIBE 和 CONTEXT 的实现内容紧挨着其语句下方写: 这对于确保测试在所 describe 的 context 下进行 set up 很重要。也帮助了在 context 彼此之间进行区分。 describe 'FootballPlayer' do let(:speed) { 50 } let(:shooting) { 50 } let(:player) do  create(:football_player,   speed: speed,   shooting: shooting,  ) end describe '#position' do  subject { player. position }  context 'when the player is fast' do   let(:speed) { 98 } # implements 'when the user is fast'   it { is_expected. to eq 'winger' }  end  context 'when the player shoots well' do   let(:shooting) { 90 } # implements 'when the player shoots well'   it { is_expected. to eq 'striker' }  end  context 'when the player is injured' do   before { player. injure } # implements `when the player is injured`   it { is_expected. to eq 'benched' }   context 'when the player uses doping' do # both injured and using doping    before { player. use_doping }    it { is_expected. to eq 'midfielder' }   end  end endend6、可能的话，使用 bulk 方法: 这是为了告诉阅读者，所有的 assertion 都是针对同一个 subject。 同时，这也是 DRY 的体现。 # BADit 'has correct attributes' do expect(user. name). to eq 'john' expect(user. age). to eq 20 expect(user. email). to eq 'john@ruby. com' expect(user. gender). to eq 'male' expect(user. country). to eq 'us'end# GOODit 'has correct attributes' do expect(user). to have_attributes(  name: 'john',  age: 20,  email: 'john@ruby. com',  gender: 'male',  country: 'us', )end7、理解在 RSpec 中 transaction 是如何工作的: 默认情况下，transaction 由每个 example 进行创建和封装。这允许一个 example 内的所有数据库操作进行回滚以确保下一个 example 处于一个干净的状态。 在诸如before(:context)或before(:all)的某些 hook 内创建数据库记录在上述默认 transaction 行为下将不会被回滚。这将导致脏数据及相应的 race conditions。 ontext 'context 1' do before(:context) do  create(:user) # WON'T BE ROLLED-BACK end before do  create(:user) # will be rolled-back end # . . . endcontext 'context 2' do before(:context) do  create(:user) # WON'T BE ROLLED-BACK end # . . . end# BY NOW, THERE ARE 2 USER RECORDS COMMITED TO DATABASE8、对于 mock 避免使用 expect: 尽管 expect 可被用于 mock 目标，但严格意义上 expect 是用于 assertion 的。 这种情况下，使用 allow 才是正确的。 而且，要提醒我们自己一下，mock 是测试的 setup 场景的一部分，而不是 assertion 场景的。 # BAD: expect. . . and_returnit 'returns the sync value' do expect(service). to receive(:sync). and_return(value) # mix between setup and assertion expect(subject). to eq valueend# GOODbefore do allow(service). to receive(:sync). and_return(value) # Set upenddescribe 'the service' do it 'syncs' do  expect(service). to receive(:sync) } # assert endendit { is_expected. to eq value } # assert9、对于有类似模式的测试用例使用 configs: 这是 DRY 的体现，也让阅读者更易于理解。 # BAD#describe '. extract_extension' do subject { described_class. extract_extension(filename) } context 'when the filename is empty' do  let(:filename) { '' }  it { is_expected. to eq '' } end context 'when the filename is video123. mp4' do  let(:filename) { 'video123. mp4' }  it { is_expected. to eq 'mp4' } end context 'when the filename is video. edited. mp4' do  let(:filename) { 'video. edited. mp4' }  it { is_expected. to eq 'mp4' } end context 'when the filename is video-edited' do  let(:filename) { 'video-edited' }  it { is_expected. to eq '' } end context 'when the filename is . mp4' do  let(:filename) { '. mp4' }  it { is_expected. to eq '' } endend# GOOD#describe '. extract_extension' do subject { described_class. extract_extension(filename) } test_cases = [  '' =&gt; '',  'video123. mp4' =&gt; 'mp4'  'video. edited. mp4' =&gt; 'mp4'  'video-edited' =&gt; ''  '. mp4' =&gt; '' ] test_cases. each do |test_filename, extension|  context  when filename = #{test_filename}  do   let(:filename) { test_filename }   it { is_expected. to eq extension }  end endend"
    }, {
    "id": 52,
    "url": "/2020/06/programming-elixir-1-6-chapter-2/",
    "title": "《Programming Elixir >= 1.6》第二章：模式匹配",
    "body": "2020/06/29 - 《Programming Elixir &gt;= 1. 6》的结构形式与一般的编程语言入门书完全不同。常规的编程语言书一般都会从基本语法开始讲起，无非数字、字符串、函数、变量等等。但这本书不是。Dave Thomas 意识到 Elixir 拥有如此与众不同的语法特性，他认为让读者首先接触并理解这种特殊性才是更好学习该语言的方式，所以在第一步就首先介绍了 Elixir 最为特殊也最为重要的模式匹配（Pattern Matching）。 而我认为这也是本书极为卓越的一点。因为模式匹配（Pattern Matching）对熟悉了其他语言的开发者而言，极有可能是一个巨大的观念冲击。同时，它也是 Elixir 语法最重要的一块基石。如果你不能接受它，后面的内容就根本没有再看下去的必要了。所谓提纲挈领，纲举目张，模式匹配（Pattern Matching）就是这个“纲”，理解了它，其他 Elixir 的语法就能很容易理解了。而 Dave Thomas 敏锐地认识到了这一点，不再按部就班地进行常规语法介绍，而是把模式匹配（Pattern Matching）提到了本书正式内容的最前面，正显示了其卓越的语言嗅觉。 由于模式匹配（Pattern Matching）是如此重要，所以我想在这里放出本章的全文翻译是值得的。 让我们来看看 Dave Thomas 在这里是怎么介绍的吧。 【下面是正文】 2. Pattern Matching（模式匹配）: 我们从前一章开始说 Elixir 产生了一种不同的方式来思考编程。 为了说明这一点，并且为后续的 Elixir 编程奠定基础，让我们来打量所有编程语言的一个基石——赋值。 Assignment: I Do Not Think It Means What You Think It Means. : 用 Elixir 命令行，IEx，来看一段简单的代码（记住，在命令行使用iex命令来开始 IEx，在iex&gt;后输入 Elixir 代码来运行显示结果）。 iex&gt; a = 11iex&gt; a + 34大部分程序员看到上面代码都会说，”好吧，我们把 1 赋值给变量 a，下一行再把 3 加到 a 上，最后得到 4”。 但是当我们来到 Elixir 的世界，这是错的。在 Elixir 中，等号不表示赋值，而更像是断言（assertion）。当 Elixir 能找到一种方式让等号左边等于右边时，该表达式即成立。Elixir 把等号=称作匹配（match）操作符。 上面的示例中，左边是一个变量，右边是一个整数字面量，所以 Elixir 能通过把变量 a 绑定到值 1 来使得匹配成功。你可能会争辩这不还是赋值么。但让我们接着往下看： iex&gt; a = 11iex&gt; 1 = a1iex&gt; 2 = a** (MatchError) no match of right hand side value: 1看下第 2 行代码，1 = a。这是另一个匹配，且通过了。变量 a 已经有了一个值 1（第 1 行代码中被设定的），所以等号左边和右边是相同的，匹配成功。 但是在第 3 行代码时，2 = a，抛出了错误。你可能会期望把 2 赋值给 a 来让匹配成功，然而 Elixir 只会改变等号左边变量的值——右边的变量会直接使用它的值。这行代码相当于2 = 1，因此报错了。 More Complex Matches: 首先，介绍一个小的语法。Elixir 的列表（list）可以通过使用方括号包含以逗号分隔的值来创建。一些例子如下： [  Humperdinck ,  Buttercup ,  Fezzik  ][  milk ,  butter , [  iocane , 12 ] ]回到匹配操作符： iex&gt; list = [ 1, 2, 3 ][1, 2, 3]为使匹配成功，Elixir 把变量 list 绑定为列表[1, 2, 3]。 再看看别的： iex&gt; list = [1, 2, 3][1, 2, 3]iex&gt; [a, b, c ] = list[1, 2, 3]iex&gt; a1iex&gt; b2iex&gt; c3Elixir 会尽力寻找一种方式来使等号左右两边的值相同。左边是一个包含三个变量的列表，右边是一个有三个值的列表，所以可以通过依次设置三个变量为对应位置的值来做到这一点。 Elixir 把这个过程称作模式匹配（pattern matching）。对于等号左边（称作”模式”（pattern）），如果等号右边和其有相同的结构，且左边的每个元素都能和右边对应位置的元素匹配，那么这个模式就和右边是匹配的。值和值匹配，变量通过设置为对应位置的值来匹配。 下面是更多的范例： iex&gt; list = [1, 2, [ 3, 4, 5 ] ][1, 2, [3, 4, 5]]iex&gt; [a, b, c ] = list[1, 2, [3, 4, 5]]iex&gt; a1iex&gt; b2iex&gt; c[3, 4, 5]右边相应位置和左边元素 c 对应的是子列表[3, 4, 5]，所以 c 被设置为这个值而使得匹配成功。 再看下模式包含值和变量的情况： iex&gt; list = [1, 2, 3][1, 2, 3]iex&gt; [a, 2, b ] = list[1, 2, 3]iex&gt; a1iex&gt; b3模式中的字面量 2 和右边对应位置是匹配的，所以可让变量 a，b 分别被设为 1，3 来使得匹配成功。但是…… iex&gt; list = [1, 2, 3][1, 2, 3]iex&gt; [a, 1, b ] = list** (MatchError) no match of right hand side value: [1, 2, 3]这里的 1（list 第二个元素）无法和右边相应位置元素匹配，因此没有任何变量被设置，匹配失败。可以看出这里定义了列表的一个匹配标准——要包含 3 个元素，且第二个元素为 1。 Ignoring a Value with _ (Underscore): 如果在匹配时不需要获取某一个值，我们可以使用特殊变量，_（下划线）。它在运行时会象一个变量一样，但会丢弃任何在匹配中设置给它的值。它象一个宣称”我能接受任何值”的通配符。下面是一个匹配任何有三个元素的列表，且第一个元素为 1 的例子。 iex&gt; [1, _, _] = [1, 2, 3][1, 2, 3]iex&gt; [1, _, _] = [1,  cat ,  dog ][1,  cat ,  dog ]Variables Bind Once (per Match): 一旦一个变量在匹配过程中被绑定到一个值了，它就会在接下来的匹配过程中一直保持这个值。 iex&gt; [a, a] = [1, 1][1, 1]iex&gt; a1iex&gt; [b, b] = [1, 2]** (MatchError) no match of right hand side value: [1, 2]第一个表达式成功是因为 a 首先匹配了 1，这个值然后保持用在第二个匹配中，也是 1，成功。 下一个表达式中，首先 b 匹配了 1，但到第二个匹配时，b 和相应位置的 2 去匹配，b 不能有两个不同的值，所以失败了。 然而，一个变量可以在后续的匹配中绑定到新的值，其当前的值不会用在新的匹配中。 iex&gt; a = 11iex&gt; [1, a, 3] = [1, 2, 3][1, 2, 3]iex&gt; a2那么如果你想要 Elixir 在匹配中强制使用变量的当前值要怎么办呢？给它加上前缀^（脱字符）。Elixir 中把这个称作pin操作符。 iex&gt; a = 11iex&gt; a = 22iex&gt; ^a = 1** (MatchError) no match of right hand side value: 1This also works if the variable is a component of a pattern:iex&gt; a = 11iex&gt; [^a, 2, 3 ] = [ 1, 2, 3 ][1, 2, 3]iex&gt; a = 22iex&gt; [ ^a, 2 ] = [ 1, 2 ]** (MatchError) no match of right hand side value: [1, 2]关于模式匹配，另外有一个重要的部分。我们将在后面的章节 Lists and Recursion 中介绍。 Another Way of Looking at the Equals Sign: Elixir 的模式匹配和 Erlang 的很相近（主要的区别在于 Elixir 允许在匹配中把之前已有绑定值的变量重新设置新的值，Erlang 则只允许对变量设置一次值）。 Joe Armstrong，Erlang 的创建者，把 Erlang 中的等号和代数中的进行了类比。当你写下方程x = a + 1时，并不是把a + 1的值赋给x，而是在断言（asserting）表达式x和a + 1有相同的值而已。当你知道了x的值，就能计算出a的值，反之亦然。 他的观点是，当你第一次遇到命令式编程语言时是不得不抛弃等号=的代数含义，现在是重新捡回它们的时候了。 这就是我之所以把模式匹配作为本书第一章的原因。它是 Elixir 的核心之一——被广泛用于条件语句、函数调用和执行中。 说真的，我期望让你对编程语言有些不同的想法，并且也向你展示了一些现有预设在 Elixir 中并不起作用。 说到现有的预设……下一章会打破另一个不容置疑的东西。你现有的编程语言可能被设计为很容易修改数据。毕竟，那就是程序要干的事，对吧？Elixir 不是。让我们来看看一门所有数据都不可变的语言。 "
    }, {
    "id": 53,
    "url": "/2020/06/programming-elixir-1-6-chapter-1/",
    "title": "《Programming Elixir >= 1.6》第一章(节选)",
    "body": "2020/06/28 - 《Programming Elixir &gt;= 1. 6》的第一章标题为“Take the Red Pill”。毫无疑问，这个说法的出处源自《黑客帝国1》中的红蓝小药片背景。不言而喻，Dave Thomas 老爷子明显是想表示如果你选择了 Elixir，就意味着选择了编程的“真相”。懂这个梗的人看到这里自然会会心一笑，有了想探究一下这个“真相”到底是什么的兴趣。 让我们来看看 Dave Thomas 在这里是怎么说的吧。 【下面是正文】 1. Take the Red Pill: Elixir 以小巧而现代的语法封装了函数式编程，包括不可变状态和基于 Actor 模式的并发。并且它运行在以工业化强度、高性能以及分布式著称的 Erlang VM 上。那么这一切意味着什么？ 这意味着你可以不用再担心目前困扰你的那些难题了。你不再需要苦苦思索多线程环境下确保数据一致性的事儿，也不用再想太多应用切分的事儿，同时最重要的是，你可以享受以一种不同的方式来编写代码。 Programming Should Be About Transforming Data: 如果你来自面向对象编程的世界，那你会习惯于用类和实例来思考问题。类定义行为，对象控制状态。开发者花费时间在复杂的类继承关系上，并尝试针对问题来建模，就好比维多利亚时期的科学家创建庞杂的蝴蝶分类学一样。 当我们围绕对象编写代码时，我们要考虑的是状态。大部分时间都花在调用对象的方法和把它们传给其他对象上。基于此，对象更新它们的状态，或其他对象的状态。这个世界中，类就是国王般的存在——它定义每个实例能做什么，它掌控着实例数据的状态。我们的目标是把数据隐藏起来。 但这并不是真实的世界。在真实的世界里，我们不想建模抽象等级（因为现实中并没有这么多真实的级别）。我们只想把事情完成，不想维护什么状态。 现在，比如，我有一些空的文件，把它们转换成一些包含文本的文件。然后我把它们转换成你可以阅读的格式。还有，某个地方的 web 服务器会把你的下载请求转换为一个包含请求下载内容的 HTTP 响应。 我不想隐藏数据，我只想转换它。 Combine Transformations with Pipelines: Unix 用户都习惯于小而精的命令行工具哲学，用不同的方式来随意组合。每个这样的工具都有内容输入、转换并以下一个工具所需要的格式来输出。 这种哲学极其灵活，使得工具复用度极高。这些工具甚至能以其作者做梦都想不到的方式来组合使用。因此极大地倍增了工具相互间作用的潜能。 这也是极为可靠的——每个小的工具只做好一件事，意味着其很容易测试。 还有另外的好处。命令管道能同时运行。如果我写： $ grep Elixir *. pml | wc -l那么单词计数程序，wc，会跟 grep 命令同时运行。因为 wc 程序会消费 grep 的输出，只要 grep 有输出内容产生出来。一旦 grep 结束，几乎没有任何延迟就能得到单词计数的结果。 为给你一点感觉，这里有一个 Elixir 的函数，名为 pmap。它输入一个 collection 和 function，返回一个把 collection 每个元素都调用 function 后的结果作为元素的列表。但……它是把每个元素转换都放到单独的进程中。先不要关心那些细节。 defmodule Parallel do def pmap(collection, func) do  collection  |&gt; Enum. map(&amp;(Task. async(fn -&gt; func. (&amp;1) end)))  |&gt; Enum. map(&amp;Task. await/1) endend我们可以运行这个函数来得到从 1 到 1000 的平方数。 result = Parallel. pmap 1. . 1000, &amp;(&amp;1 * &amp;1)当然，我刚刚启动了 1000 个后台进程，完全充分利用了自己电脑的全部多核处理器。 这段代码你可能还很不习惯，没关系，等你读到本书的一半之后，就能自己写出这样风格的代码了。 Functions Are Data Transformers: Elixir 让我们用和上述 Unix 同样的方式来解决问题。而除了命令行工具外，我们更拥有了函数。我们能把它们按照自己的意愿尽情使用。这些函数越小（更专注），我们越能灵活地组合使用它们。 只要想，我们可以让这些函数并行地运行——Elixir 拥有简单而又强大的机制在它们之间通信。这可不是从你爸爸那个时候起就令人头疼的进程或线程——我们这里要谈论的是仅仅在一台电脑上就运行上百万个任务的潜力，以及在上百台电脑上的运行。Bruce Tate 对此的想法是这样的：“大多数开发者把线程和进程看作地狱一般；Elixir 开发者却把它们当作一种重要的简洁方案”。当我们跟随着此书的深入讲解，你会开始明白他这话的意思。 这种转换理念是函数式编程的核心：函数将其输入转换为输出。三角函数 sin 就是一个例子——给它 π⁄4，返回 0. 7071。一个 HTML 模版系统是一个函数，当它接收一个包含占位符和相应键值对的模版时，就生成一个完整的 HTML 文档。 但这种强大是有代价的。你将不得不抛弃很多之前已有的编程理念。很多原有的直觉都将变成错的。这会成为一种阻力，因为你会感到自己完全变成了一个“新”手。 从我个人角度看，这恰恰是乐趣所在。你并非在一夜之间就学会面向对象的编程，当然也不可能一顿早饭的时间就成为函数式编程的专家。 不过某种角度看，你将开始以不同的方式来思考问题，并且会发现自己用很少的代码就能完成令人惊讶的事情。你会发现自己编写的小段代码能够被反复使用，而且通常以意想不到的方式（就象上面的 wc 和 grep）。 你看待世界的视角甚至也开始改变，因为你不再考虑责任而开始考虑完成任务。每个人都会同意这是有趣的。 ……（关于如何配置 Elixir 环境的一些步骤说明，略过）…… Think Different(ly): 这是一本不同凡”想”的书——接受一些对于编程的看法并非其全部：  面向对象并不是代码设计的唯一方法 函数式编程并不需要非常复杂或精确 编程的基础并不是赋值、if 语句和循环 并发不需要锁，信号量，监视器，以及类似的东西 进程并不是代价昂贵的资源 元编程并不是随便添加到语言上的东西 即使它是工作，编程也能很有趣当然，我并非要说 Elixir 就是那种灵丹妙药（好吧，技术确实是，你懂我的意思）。它不是编写代码的唯一方式。但它跟主流是如此不同，学习它会为你提供更多的视角，能让你拓展思维而看到新的思考编程的方式。 那么，让我们开始吧。 "
    }, {
    "id": 54,
    "url": "/2020/06/vim-increase-numbers/",
    "title": "Vim 批量递增数字的技巧",
    "body": "2020/06/24 - 众所周知，Vim 自带的默认快捷键⌃-A、⌃-X可以对单个数字进行增减操作，这在碰到适用的场景时当然非常方便。但是，编程中另一种场景也是经常遇到的： 你可能要同时对多行上的多个变量命名，名称中带有数字，且需要依次递增。 比如 Vim 中可以通过yy4p快速复制生成这样的多行变量 car01car01car01car01car01那么如何把其快速改写为 car01car02car03car04car05这样的结果呢？Vim 是否有快捷键或什么技巧来高效处理这个场景？还是说只能针对每个变量名一个一个手工去改？ Vim 是如此强大，答案当然是肯定的。 看本文头部的 Gif 操作动图就一目了然了。 "
    }, {
    "id": 55,
    "url": "/2020/06/programming-elixir-1-6-preface/",
    "title": "《Programming Elixir >= 1.6》序言",
    "body": "2020/06/23 - Elixir 是一门奇妙的语言。我本身是一名 Ruby 开发者，自从第一次接触到 Elixir，就被它类似 Ruby 的语法而同时又拥有的一系列独特特性所吸引。要学习一门新语言，当然是从最经典的书籍开始。而著名的老爷子 Dave Thomas 本身就具有业内卓越的号召力，他亲自撰写的《Programming Elixir &gt;= 1. 6》自然是我的首选。于是就从这本书开始我的 Elixir 之旅了。 看这本书的同时，我萌发了翻译的念头。前些年自己的不少同事都曾做过这样的事，现在有了这个契机，于是打算亲自尝试一回，也算是一种不错的经历。于是一边看一边翻译，历时差不多三个月就基本完成了。 当然由于版权原因，不可能在我的博客上放出所有的内容。所以挑选了一些 Elixir 中最有独特性、技术层面最优美的相关章节，准备做成一个系列，陆续登到自己的博客上。 我想第一站从这本书的序言开始是最恰当的。这是 Dave Thomas 老爷子出于自身对 Elixir 语言亲身感受的一个心理历程的描述，很值得一看。 【下面是正文】  A Vain Attempt at a Justification, Take Two:  我是个编程语言爱好者。我乐于尝试各种语言，喜欢思考它们的设计和实现。  1998年，作为狂热的 comp. lang. misc 邮件组读者，我偶遇了 Ruby 这门语言。我下载、编译，并从此爱上了它。任何时候你爱上一个东西往往很难解释为什么。它跟我是如此默契，也有足够的深度让我保持对它的兴趣。  15年过去了，这期间我一直在寻找新的能带给我同样感受的东西。  我曾经遇到过 Elixir，但因为某种原因并没有被它吸引。但在我开始写这本书第一版的几个月前，我和 Corey Haines 聊了。我向他哀嚎自己想通过有吸引力的方式向人们展示函数式编程的概念，却不想如那些呆板的学院派书籍一样。他告诉我再去试试 Elixir。我试了，然后找到了当初遇见 Ruby 时的那种感觉。  所以现在的我进入了“危险状态”——我想要让其他的人们知道 Elixir 有多牛，我想传道。所以我写了本书。但我不想再写一本900页的镐头书，我想要让本书简明扼要又激动人心。因此我不打算考究所有的细节，列出全部的语法，以及完整的库函数列表，齐备的 OTP 选项，等等……  相反，我打算带给你们 Elixir 作为编程模型的强大和美。我想激励您参与其中，然后给出能填补空白的在线资源。  但最主要的是，我希望你从中获得乐趣。  3年过去了，Elixir 也在进步。Phoenix，它相应的开发框架，以函数式编程的一整套方案把快乐带给了开发者们。这个精彩的项目使得在 Linux 为基础的微控制器中编写 Elixir 代码变得如此容易。Elixir 本身也在成长，举办了许多全球性、国家或区域性的开发者大会。出现了更多的招聘 Elixir 开发者的岗位。  我自己也同样在进步。但我仍然每天都会使用 Elixir。我刚刚结束了在南卫理公会大学的第二年助教工作——用 Elixir 的魅力诱惑未来的程序员。我写了一个 Elixir 的在线教程。  现在我接到了撰写本书的要求。说实话，这并非必须，因为 Elixir 1. 6 跟 1. 3 相比并没有那么大的差别使得上一版书就不能看了。但我个人觉得 Elixir 已经成熟，所以现在我要做点不同的事，也乐意把这些分享给你们。 "
    }, {
    "id": 56,
    "url": "/2020/06/dhh-citadel-pattern-for-majestic-monolith/",
    "title": "“雄伟巨石” 可以成为 “城堡”",
    "body": "2020/06/22 - 【本文首发于 RubyChina 社区】DHH 在 2020. 04. 08 发表了一篇最新博客 “The Majestic Monolith can become The Citadel”，继续讨论对微服务的一点看法，提出了一种与微服务相对的“城堡”模式。在 Twitter 上也引发了不少关注，搜关键字“The Majestic Monolith”就能看到很多。这是原文链接：https://m. signalvnoise. com/the-majestic-monolith-can-become-the-citadel/ 我按照个人理解，粗略翻译了一下。看到的朋友可以对照原文来看，以防遗漏作者的本意。 【下面是正文】 “雄伟巨石”可以成为“城堡”: 大多数的 Web 应用应该都是从一块“雄伟巨石”开始其生涯的：一个单一代码库做其所需的所有事儿。与之相对的是一群 Service，不管这些 Service 是“微”还是“大一点”的，都试图把应用切成孤岛，每个仅做整体工作的一小片而已。 大多数的 Web 应用都将以“雄伟巨石”形态在其一生中都能持续提供很好的服务。这种模式的上限很高，比大部分人幻想成为架构师时所能想象的要高得多。 但是，尽管如此，“雄伟巨石”仍然会有需要一些帮助的那一天。也许你正在与庞大的团队打交道，其中的人们相互磕磕绊绊（即使这样，别忘了有很多非常大的组织依然在使用 monorepo 模式！）。或者你终究会遇到极端负载下的性能或可用性问题，这在“雄伟巨石”的技术选择范围内无法轻松解决。你的第一直觉将是改进“雄伟巨石”直到其能够应对问题，做了这一切而没成功时，你才会考虑下一步。 下一步就是“城堡”，保持“雄伟巨石”在中心位置，但用一系列的“基地”对其支援，每个分离出应用程序职能一个小的子集。“基地”使得“雄伟巨石”得以卸下其不同行为的一个切片，（这些不同行为）要么是出于组织原因，要么是出于性能或实现的原因。 一个在 Basecamp 的这种例子是我们的旧 chat 应用 Campfire。它是在 2005 年构建的，那时 Ajax 和其他 JavaScript 技术都还很新颖，所以它基于 polling 而不是现代 chat app 目前使用的长连接。这意味着每个客户端连接到系统都会每三秒触发一个请求来询问“是否有我的新消息？”。大多数这些请求都会回答“不，没有”，但为了获取这个答案，你仍然不得不对请求进行身份验证，查询数据库，等等。 与应用程序的其他相比，这个服务的性能特征大不相同。在任何给定时间，它都将占所有请求的99%。它也是一个确实很简单的系统。在 Ruby 中，它仅仅 20 行代码长度而已，如果我没记错的话。换句话说，这是一个极好的“基地”候选人！ 所以我们就（为它）构筑了一个“基地”。这么些年来，在日光下使用每种高性能编程语言来重写这种“基地”成为了我们的乐趣，因为通常只用短短几百行代码就能搞定，不管什么语言。所以我们使用了 C，C++，Go，Erlang，还有些我都忘记了。 但这显然是一种“基地”！应用程序的其他部分继续作为 Ruby on Rails 构建的“雄伟巨石”。我们没有试图把整个 app 切成小的 Service，每个以不同语言来编写。不，我们只是分离出一个单独的“基地”。这是一个“城堡”的建设。 随着越来越多的人们意识到对于微服务的追逐会走上一条死胡同，“钟摆将会再摆动回来“。“雄伟巨石”在这儿等待着微服务的“难民”。如果他们确实做到了大型应用程序的规模，那么“城堡”这种可以扩展的模式足以让你安心。 "
    }, {
    "id": 57,
    "url": "/2020/06/update-blog-to-jekyll-4-1-with-github-actions/",
    "title": "在 Github Actions 支持下升级博客到 Jekyll 4.1",
    "body": "2020/06/18 - 我一直使用 Github Page 作为自己的博客，简单，够用。而博客是使用 Jekyll 来搭建的，这也是 Github 官方的推荐方式之一。最近发现 Jekyll 已经到了 4. 1. 0，于是打算把博客也升级到最新版。说干就撸起袖子开始。 博客现状: 不过稍微快速调研了一下，发现一个悲催的事情：Github 官方目前还是只支持到 Jekyll 3. 8. 7 而已。虽然这个版本也不算太旧，但最近一次更新也是两年前了。 另外，Jekyll 4 所支持的 Plugin，目前的 Github Page 也暂时还不支持。 怎么办呢？ 当然是继续寻找技术层面的解决方案先，而且也很快就找到了——Github 官方去年发布的 Github Actions 就是解决方案！ Github Actions: Github Actions 简单来说就是 Github 官方推出的一套 CI/CD 工具，把 build、test、deploy 直接和你的 Github 代码库整合起来，再也不用去寻找第三方的工具。毕竟自家的东西用起来最称心不是。 Github Actions 的基本流程就是，当你有代码 push 到 Github 代码库时，就会触发 Actions 中事先定义好的 Workflow，对你的代码进行 build、test、deploy等操作。这正好满足了新 Jekyll 版本的要求。 升级到 Jekyll 4. 1: 先 Google 找到一套 Free 的 Jekyll 4 的 Theme（我最终选择的是Memoirs Jekyll Theme）。然后在本地按其要求对原有代码和文件进行替换、修改等常规操作不提。Push 之前当然需要先在本地预览下效果，那么在代码库下先运行 bundle install安装 Jekyll 4. 1 等 Gem 包后，再运行 bundle exec jekyll serve --watch这样就可以让 Jekyll 运行在本地的 4000 端口。打开浏览器，输入 http://localhost:4000就能看到新 Theme 的模样了。 Jekyll 4 的 Github Actions 支持: 进行必要的修改完成，确定无误后，接下来就进行配置 Github Actions 的环节了。这也是最关键的部分，决定了新博客能否被 Github 正确地解析显示。如果不对，博客就无法被人访问了。 幸运的是，这个需求是如此广泛，Jekyll 官方已经给出了一个 Github Actions 的配置范例：Github Actions for Jekyll 4。从我自己配置好的最终结果来看，基本上只要安装这个官方文档来就万事大吉。这里只简单列出几个比较关键的点，简单说一下。 创建 Github Actions 的 Workflow: 在代码库根路径下创建. github/workflows，然后在该目录下创建一个yaml文件，文件名比如叫gh-pages. yml。 这个yaml文件的内容可以直接拷贝文档上的范例，因为都是通用的： name: Build and deploy Jekyll site to GitHub Pageson: push:  branches:   - masterjobs: github-pages:  runs-on: ubuntu-16. 04  steps:   - uses: actions/checkout@v2   - uses: helaili/jekyll-action@2. 0. 1. . . 这个yaml实际上就是定义了 Github Actions 的一个 workflow。当然其具体规范和含义，可以参考 Github 的官方文档说明，这里就不赘述了。 关于 JEKYLL_PAT: 唯一一个需要注意的地方，就是JEKYLL_PAT这里。这是设定了一个 secret 的环境变量，JEKYLL_PAT是需要设置的一个 Github 的Personal Access Token，具体设置方法参考上面 Jekyll 文档中Providing permissions这一节的步骤来做即可。 大功告成: 全部完成之后，把所有新代码 Push 上 Github，然后打开对应代码库的 Actions 标签。如果上述配置都正确无误的话，就会看到这里会出现一个正在运行中的 Github Actions 的 Workflow。接下来要做的就是静静地等待…… 5分钟后（包括上面 Workflow 运行时间和 Github 更新博客内容的时间一起），重新打开我的博客，全新的界面出现在面前！试试点击一些链接，确认都没有问题，所有页面完全显示正确，站内搜索和 Disqus 评论等功能一切正常。 至此博客升级顺利完成！ "
    }, {
    "id": 58,
    "url": "/2017/03/prefer-ember-js-over-angular-and-react-js/",
    "title": "为什么我偏爱 Ember.js 胜过 Angular 和 React.js",
    "body": "2017/03/26 - 前几天看到了这篇文章：Why I prefer Ember. js over Angular &amp; React. js，觉得对于国内期望了解 Ember. js 的开发者来说是一个不错的介绍。于是和该文的作者 Paul Shan 联系取得翻译的授权，翻译了过来。译文如下： 从我开始写 JavaScript 已经有5年了。无论是开发项目，指导别人，还是发布文章，JavaScript 都给了我极大的满足感。感谢 JavaScript！ 在过去5年里我使用过很多 JavaScript 框架，不管做后端还是前端。但 Ember 从2015年中期就从我的开发世界中消失了。幸运的是1个月前机缘凑巧我又参加了一个用 Ember 做的前端项目。起初我并不太在意这点，只把它当作不过又一个日常开发而已。但随着项目的深入我开始思考自己关于这三个前端框架的体验，并在今天把它记录下来。 声明:  该文并非是要抨击 Angular 或 React。 该文并非要很深入探讨技术层面的部分，而是作为一个开发人员对其真实使用体验的吐露。 标题中我使用了“偏爱”而非“推荐”的字眼，因为“推荐”往往要和具体项目具体场景相关，而“偏爱”则是一种更普适性的描述。Ember 令人满意之所在: 原生感受: 说实话，最近几年来我对 JavaScript 越来越不满。我不喜欢有人试图用工程的模式来对待 JavaScript。我喜爱 JavaScript 就仅仅是 JavaScript 而已。这也是我为什么不喜欢 TypeScript，以及 Angular 2。我看不到任何用处来多学习另一门语言，只为了试图引入一些丑陋的类（我知道是可选的）和语法。React 至少在这一点上做的比 Angular 2 好一些，但你依旧要面对 jsx 的问题。 相反，Ember 使用了纯 JavaScript。你只用写 JavaScript。Ember 提供了很多 api，却没有额外的语法。这让我作为 JavaScript 开发者而言感觉很棒。 约定大于配置: 相信我，尽可能地减少配置相关代码对你的项目将有巨大的好处。首先，代码会变得少而清爽。其次，约定将是通用的，任何新加入的开发者都能了解发生了什么。对于那些之前从没有使用过 Ember 的人来说，只要你遵循了约定来命名文件和变量，Ember 就能自己处理好剩下的事情。 所见过的最好文档: Ember guides 和 Ember API 的文档可能是我开发生涯中见过的最好技术文档。即使是一个初学者都能很容易理解并上手。Ember 的官方论坛对于解决疑问也很有帮助。 最好的构建体验: Ember-CLI 是 Ember 的一大杀器。即使 Angular 都试图借鉴 Ember 的这一工具来开发自己的 CLI。使用 ember-cli 你可以快速构建一个预定义好目录文件架构的项目，而这样的架构是经过社区的讨论和实践所验证过的。当项目开始构建时，无论团队是否对此有经验，都完全不用担心有没有遵循最佳实践的问题。对于 React 而言这里就可能存在风险，因为它只是一个库并非是一个框架。 强制性的最佳实践: 即使你的团队中有很棒的开发者，有时候迫于上线的压力他们也会写出坏的代码。而 Ember 会至少在某些层面上强制性地让你采取最佳实践的方式。举个例子，你不应该把业务逻辑写到模版中。Ember 的模版里只可以使用迭代 iteration 和带布尔参数的 helper 帮助方法，这样你就根本别考虑在模版里写业务逻辑的事儿了。 开发效率的提升: 我知道，在三个框架里 Ember 可能是学习曲线最陡也最难学的那一个。但是一旦你掌握了它，你就能开发一个项目超快，远胜过 Angular 或 React。“约定大于配置”和 ember-cli 就是最主要的两个原因。 团队工作: 如果你公司的所有团队（甚至即使有人不在公司办公）是在开发多个 Ember 项目，使用 ember-cli 构建工具，那么他们每个人的项目目录文件架构会非常相似，而且可以几乎不花费什么时间成本就进行项目的切换，并立刻投入开发和提交代码。这实际变相提高了公司实际上的开发效率。 不属于公司只属于社区: Angular 属于 Google，React 属于 Facebook。而 Ember 来自于社区，也只为了社区。Ember 核心开发团队的开发者们都来自于各自公司实际 Ember 项目的成员，这恰好是 Ember 的最大不同之处：他们不仅是框架的开发者，更是框架的使用者。这让他们能始终贴近现实，紧接地气。 友好的版本发布: 我幸运地在 Angular 2 发布时已经离开了之前的 Angular 项目，但我的朋友要为项目升级到 Angular 2 头疼和抓狂了。与此相反，Ember 2 发布时没有任何变化。是的，你没听错。它没有任何变化。Ember 1. 13. 0 和 2. 0 在使用上完全相同，因为 Ember 是采取渐进式的策略来对 1. x 开放新功能，以便让使用 Ember 的实际项目能进行完全无痛地升级。这正是 Ember 核心团队成员“不仅是框架的开发者，更是框架的使用者”的最好体现。 Ember 没有缺点了吗？: 任何事物都有两面性，Ember 也不例外。Ember 也有自己的缺点，诸如陡峭的学习曲线，稍慢的页面渲染，框架体积较大等。已经有很多的技术文章对这些框架进行过比较了。但本文更多的是在开发实践体验上我个人的一些感受。我相信这些框架中不管选用哪一个，就技术而言都能帮你完成项目。但还有一些除此之外的因素影响到你项目的实施和进展。在考虑了各种实际情况、业务、场景之后，你就可以做出最佳的决策来启动项目了。 相关文章:  Is React. js tougher and confusing than Ember or Angular? A complete guide to Ember – Ember. js Tutorial What’s new in Angular 2. 0? Why it’s rewritten – addressing few confusions Plus minus kind of calculation and comparison in Ember. js template, handlebars? MVC vs Flux – which one is better? Scaffolding and understanding an ember application – Ember. js Tutorial part 2"
    }, {
    "id": 59,
    "url": "/2017/03/use-alfred-to-auto-upload-screenshot-to-qiniu-and-generate-the-markdown-image-url/",
    "title": "使用 Alfred workflow 上传截图到七牛并自动生成外链URL",
    "body": "2017/03/17 - 今年准备多写点Blog文章了。 前两周抽了些时间把 Github page 重新弄了下：Jekyll 升了个级，Theme 换了个顺眼的，自我介绍写了点新的。。正万事俱备，就要动笔的时候，发现了个有点不爽的问题——图片。 Jekyll 是用 markdown 格式来写文章，而写 markdown 的时候要放入一张图片到文章里，可就没那么友好了。不像 Office Word 之类软件，可以直接拖入图片到文章中，markdown 是通过插入 ![xxx](http://xxx. com/xxx. png) 这样的代码来显示图片的，那么至少得经过这样几步：  上传图片到网络上的一个图片空间（图床） 拷贝该图床上这张图片的对应URL代码 粘贴到 markdown 文章中，改写为上述那样合适的形式更不用说一篇文章如果需要的图片稍微多一点，每张图片都这样来一遍操作，那得把人烦死，那点子写作的欲望早就会被扔到爪洼国了。 身为一名 Dev，碰到这个情况，当然要用程序员的方式来解决问题！ 我选用的国内图床是七牛。七牛对免费用户还不错，只要通过了它的实名认证，就可以有10G的空间，10G的访问流量／月等（详细服务可去参看七牛官网）。 准备七牛空间: 要使用七牛的服务，大概需要这样几步：  注册七牛开发者账号 https://portal. qiniu. com/signup?code=3looau4kfei4y 登录七牛开发者平台，添加一个存储空间。要给这个存储空间取个名字，七牛给这个名字取了个术语叫 Bucket 在“个人中心 &gt; 密钥管理”中查看AccessKey 和 SecretKey（下面的上传工具会用到） 使用七牛官方提供的 command line tool：qshell 可在本地电脑进行管理和上传（之前的 qsync 已经被废弃了）qshell 的下载和用法可在其官网 http://developer. qiniu. com/code/v6/tool/qshell. html 找到。本文不多赘述。 要上传单个图片，我们需要使用 qshell 的 fput 命令: qshell fput &lt;Bucket&gt; &lt;Key&gt; &lt;LocalFile&gt;Bucket 就是上面提到的存储空间名字。 例如：qshell fput your-bucket xyz. png /path/to/image/xyz. png 对主要流程的思考: 现在，图床有了，怎样上传图片也知道了，该是考虑整个目标实现流程的时候了。 重申一下目标：自动化完成 “上传图片 –&gt; 获取对应 markdown 图片URL” 整个过程。 分析一下，每次添加一个截图到文章里，大概可分为七步，其中会用到一些工具：  使用 OSX 系统的快捷键：⌘+⇧+⌃+4 截图，并存入剪贴板中 使用 pngpaste 把剪贴板的图片写入到一个临时文件 OSX 自带截图工具默认 PNG 格式，图像文件体积偏大。七牛虽然提供 10G 的免费空间，但积少成多，能省一点是一点，所以尽量缩减图片体积是必要的。我用了 pngquant 来优化临时文件的图像大小。 把优化后的临时文件用 qshell 上传到七牛 上传成功后自动删除临时文件 自动生成七牛空间对应上传图片的 markdown 图片URL代码 最后，把 markdown 图片URL代码存入系统剪贴板这一系列步骤自动化完成后，我就只需按 ⌘+V 即可把 markdown 图片URL代码粘贴到文章里了。 上面用到的三个工具：pngpaste，pngquant，qshell，其中前两者都可以通过 OSX 的 Homebrew 来安装。 创建 Alfred workflow: 思路有了，就开始干吧！ 对于这样需要把一系列流程步骤进行自动化处理的事情，毫无疑问首选 OSX 上的“神兵利器” —— Alfred。Alfred 的 Workflow 正适合用来干这事。 要使用 Alfred 的 workflow，得购买它的 PowerPack 版本。Free 版本未开放 workflow 功能。相信我，掏这笔银子是绝对值得的！ 在 Alfred 的 “Workflows” 窗口，点 + 号创建一个 “Empty Workflow”，名字取你喜欢的。 在这个空白 workflow 中，需要先创建一个触发器，来开始整个工作流。 在右侧窗口中右键菜单中选 “Inputs –&gt; Keyword” 添加一个 Keyword 触发器。我设定的 keyword 是 qiniu，title 描述为“上传剪切板图片到七牛并获取外链URL”。这个title 是让在 Alfred 的命令窗口输入 qiniu 时显示该命令用来干嘛的。 然后再添加一个 action。 在右侧窗口中右键菜单中选 “Actions –&gt; Run Script” 添加一个脚本执行器。在输入框中输入如图代码。qiniu. rb 文件会在稍后添加，它是整个工作流的核心部分，上述七个步骤的2～6步（即除去首尾之外的全部）工作都将由它来完成。 至于为什么选用 Ruby 脚本，原因很简单 —— 我是一名 Ruby developer。 这第二步的 action 执行后，会输出 markdown 图片URL代码，所以需要再添加最后一步：把代码存入系统剪贴板。Alfred 已经帮你准备好了这个常见的操作，直接使用它就好： 在右侧窗口中右键菜单中选 “Outputs –&gt; Copy to Clipboard” 添加 “Copy to Clipboard” 执行器。一切默认即可。 最后，我们还应该添加一个 OSX 系统提醒，让这个 workflow 的使用体验更友好。 在右侧窗口中右键菜单中选 “Outputs –&gt; Post Notification” 添加 “Post Notification” 执行器。title 写为“截图上传成功！Markdown URL已复制到剪贴板”，其他使用默认。 这样整个 workflow 所有环节都已经构建好了，我们只需用线将上面四个部分连起来，以组成一个工作流，即可大功告成！ 2 - 6 步的代码实现: 现在来看看要怎样实现 2 - 6 个步骤。一步一步来。 第2步: * 使用 pngpaste 把剪贴板的图片写入到一个临时文件pngpaste 的使用很简单： pngpaste /path/to/temp. png第3步: * 用 pngquant 来优化临时文件的图像大小pngquant 用法： pngquant -f -o /path/to/temp. png /path/to/temp. png-f 参数确保覆盖源文件 第4步: * 把优化后的临时文件用 qshell 上传到七牛qshell 使用其 fput 命令来上传： qshell fput xfyuan markdown/xyz. png /path/to/temp. pngxfyuan 是我七牛空间的Bucket名称。 markdown/ 是使用了七牛的命名空间形式作为区分，我用 markdown 来表示跟 Blog 文章相关的图片。 第5步: * 上传成功后自动删除临时文件这个不用多说： rm -f /path/to/temp. png第6步: * 自动生成七牛空间对应上传图片的 markdown 图片URL代码七牛的免费开发者账户会得到一个上传文件外链的默认域名（在存储空间管理页面可看到），我的是： 而上传图片的名称就是上面的 markdown/xyz. png，二者拼起来即可得到对应的外链访问URL： http://omwi90lh4. bkt. clouddn. com/markdown/xyz. pngmarkdown 的图片格式即为： ![markdown/xyz. png](http://omwi90lh4. bkt. clouddn. com/markdown/xyz. png)qiniu. rb 代码: qiniu. rb 就是把上述各步代码放到一起，加上必要的一些处理，确保其正常运行和基本的异常处理即可。 全部代码如下： #!/usr/bin/env rubyrequire 'date'qiniu_domain =  http://omwi90lh4. bkt. clouddn. com qiniu_bucket = 'xfyuan'qiniu_scope = 'markdown'folder    = '~/works/_tmp/'filename =  #{DateTime. now. strftime('%Y%m%d%H%M%S')}. png upload_file =  #{folder}#{filename} # paste screenshot to a temp filesystem  . /pngpaste #{upload_file} if File. file? File. expand_path(upload_file) # optimize png system  . /pngquant -f -o #{upload_file} #{upload_file}  # use qiniu qshell tool to upload screenshot image qiniu_filename =  #{qiniu_scope}/#{filename}  system  . /qshell fput #{qiniu_bucket} #{qiniu_filename} #{upload_file} &gt; /dev/null  # remove temp file system  rm -f #{upload_file}  # generate markdown image url qiniu_image_url =  #{qiniu_domain}/#{qiniu_filename}  markdown_image_url =  ![#{qiniu_filename}](#{qiniu_image_url})  print markdown_image_urlelse print  剪切板没有图片！ end有几个值得注意的地方：  临时文件使用当前的时间戳作为文件名 qshell 上传文件用到了 &gt; /dev/null 避免其上传成功后输出的一大堆信息干扰到最后的剪贴板内容 应该判断当前剪贴板上是否有图像，如果没有就中断退出，给予警告提示最后的重要一步: 还有一个问题：qiniu. rb 文件放到哪里才能让 Alfred workflow 执行时能找到并正确运行？ 很简单。 打开 Keyword 触发器设置窗口： 看到 “Cancel” 按钮左边那个图标了？点击它，会在 Finder 中自动打开该 Workflow 的对应目录。把 qiniu. rb 文件放到这里，Workflow 运行时就能找到。 同理，上述步骤中用到的三个工具：pngpaste，pngquant，qshell，为了让 Workflow 运行时能找到它们，我们也应该把它们的执行文件各自拷贝放到这个目录，否则代码是无法正常工作的。 另外，还可以稍微美化一下 Workflow，例如给它加个图标。 最后，Workflow 的对应目录会是这样： 最终的使用体验: 本文的全部图片插入都通过使用该 Alfred Workflow 来完成。 Alfred 命令窗口： 上传成功，系统弹窗提醒： 轻松快速粘贴到文章里： 干得漂亮！ "
    }, {
    "id": 60,
    "url": "/2017/03/the-inheritance-of-rails-render-path/",
    "title": "Rails render path 继承关系一例",
    "body": "2017/03/13 - 近两天在 Rails 的开发中，突然发现了关于 render path 的一个之前未曾注意到的有趣地方。 代码中有一个父类 Controller，假设为 FruitsController；两个子类 Controller，假设为 ApplesController，OrangesController。 ApplesController#show，OrangesController#show 的 View 中，大部分都是一样的，已经抽出来了一个 partial 模版，假设为 _fruit_content. erb。这样两个 Controller#show 的模版内，都会有这么一行： render 'path/to/fruit_content'那么 _fruit_content. erb 放到哪个 path 比较合适呢？当然 shared 目录是一个选择，但由于一些其他原因已经排除了，所以不做考虑。 如果放在 views/apples下，那么 OrangesController#show 的模版内那一行 render 就必须指明 _fruit_content. erb 完整path： render 'apples/fruit_content'反之亦然。无论哪种都感觉不太符合 Rails “Convension over Configration” 的原则。 最后无意中却发现 Rails 竟然“原来对此早有准备的”～ 答案很简单。 由于 ApplesController，OrangesController 都继承于 FruitsController，所以我们只需把 _fruit_content. erb 放在 views/fruits然后无论是 ApplesController#show 还是 OrangesController#show 的模版内那一行 render 都只需写为： render 'fruit_content'即可。 搞定收工。 "
    }, {
    "id": 61,
    "url": "/2014/04/set-up-development-environment-and-workflow-using-cygwin-in-windows7/",
    "title": "Set up development environment & workflow using cygwin in windows7",
    "body": "2014/04/25 - 使用Cygwin在Windows7下构建开发环境和工作流（上）2014年4月24日下午8:09 缘起: 现在的工作由于条件限制，最终只能在Windows7下进行开发。这给习惯了OSX/Linux开发环境的我带来了很大问题。感觉好像从现代社会回到原始社会似的，开发效率一下子降低了50%以上。终于忍无可忍之下，尝试看看能否安装上Cygwin这个传说中windows下模拟linux环境的工具。最后经过一番摸索下，到底成功搞定。以此为记。 目标: 首先是开发环境，我在OSX/Linux下做开发常用的工具有这么些：  Terminal Vim Tmux Zshell Git Ctags对于工作流（Workflow），这里是指针对web开发，总有常见的一些必要工作步骤，诸如sass/less编译、css/js minify、unit/integration test、甚至包含sftp上传等，我们可以通过专门的workflow tool来把这些步骤串起来，并且让其自动化完成，无需人工去做。 而之前自己使用的工作流工具，主要是Ruby的guard及相关的一系列gem。现在nodejs很火，其中的工作流工具Grunt/Gulp看起来也很厉害的样子，所以这次准备尝试看看。 本文准备分成（上）（下）两部分，本篇是第一部分，主要看看在Cygwin下的开发环境搭建。下一篇再讲工作流搭建和使用。 闲话不多说，开始吧。 开发环境: Install Nodejs: 因为最新的Nodejs已经不支持Cygwin了，所以必须手动安装。 去Nodejs官网下载windows的安装包http://nodejs. org/dist/v0. 10. 26/x64/node-v0. 10. 26-x64. msi。运行，直接默认安装好即可。 Install Cygwin: 接下来当然是安装Cygwin。去Cygwin官网下载安装文件包http://Cygwin. com/setup-x86_64. exe，运行。 一路next，安装路径默认为C:\Cygwin64。“Local Package Directory”为临时文件路径，任意即可。 在“Internet Connect”界面，如果要通过Proxy就选第三项，填入Proxy Host和Port。 在“Select Package”界面，选择你准备安装的软件包。大致选择这些：  vim git curl wget tmux ssh ncurse(安装才可使用clear命令) zshell ctags zip unzip ping procps tree ruby imagemagick pngquantCygwin这里的选择方式和普通软件相比有些不同。你要在那个双箭头的图标上点击，就能看到旁边的文字会在install, skip, reinstall, uninstall等状态切换。你要确保想安装的软件包状态在“install”就对了。 Next确认无误后，Cygwin就会联网下载软件包进行安装了。 最后的finish界面建议勾选在桌面创建快捷方式。 全部安装完成后，双击桌面图标，如果一切正常，会看到Cygwin的窗口出现。再度看到熟悉的Linux Terminal黑色窗口，还真有些小激动的感觉。 良好的开端！ Settings: 要想仅仅凭借Cygwin默认的设置就进行高效的开发，那是远远不够的，我们必须要进行一些设置才行。一起来看看吧。 Cygwin: 首先是Cygwin自身。点击Cygwin窗口左上角，弹出菜单中选择“options”，显示设置窗口。有这么一些较重要的地方需要调整：  Terminal –&gt; Type change to “xterm-256color”； Looks –&gt; Cursor change to “Block” Text –&gt; Local change to “en_US”, Character set change to “UTF-8”;点击“OK”保存。 Bash: 使用命令：vim . bashrc，按照自己习惯修改bash设置。我自己常用的配置可参见我的github：bashrc 很重要的一点是，如果需要proxy联网，必须增加如下3行在bashrc： export http_proxy=http://yourname:yourpasswd@proxyaddress:portexport https_proxy=https://yourname:yourpasswd@proxyaddress:portuse_proxy=on另外一个必须设置的，为了我们能在Cygwin中正常运行node, npm命令，需要添加一行（如果之前的node安装在默认路径的话）： export PATH=$PATH: /cygdrive/c/Program Files/nodejs/ /cygdrive/c/就是c:\盘。Cygwin下可以简单使用命令cd c:来进入c:盘。 Tmux: Tmux默认设置和键盘快捷键并不好用。我自己修改了一些常用的配置和快捷键映射，参见我的github：tmux. conf Vim: 最重要的Vim编辑器，其配置自然也是需要自定义才能用起来得心应手。我使用了Vundle来管理vim的plugin。具体的配置可参见我的github：vimrc，我的github：vimrc. bundle Git: 我自己常用的配置可参见我的github：gitconfig Zshell（可选）: zshell是比bash更好用的一个shell，我现在已经离不开它了。要用zshell，一定少不了oh-my-zsh！ oh-my-zsh: 去https://github. com/robbyrussell/oh-my-zsh，参照其说明，我们在Cygwin中运行： curl -L http://install. ohmyz. sh | sh这会自动帮你把oh-my-zsh安装好。这里有点特殊的是，因为是Cygwin，所以最后无法自动切换到zshell，需要我们手工处理一下。vim打开： vim /etc/passwd找到对应你windows7登录账号的那一行，把最后的“/bin/bash”改为“/bin/zsh”。重启Cygwin，大功告成！ Zshell Setting: 使用vim . zshrc编辑zshell的设置，大部分应该都可以参考上面对bashrc的配置即可。我自己常用的配置可参见我的github：zshrc 值得注意的一个地方是，oh-my-zsh自带了很多plugin，在~/. oh-my-zsh/plugins下即可看到，已经帮你做好了很多事情，直接拿来用就好了。找到. zshrc这一行： plugins=(git)加入你需要的即可，例如： plugins=(git tmux history ruby)使用zshell有时会报类似这样的error： compdef: xxxxxxx这时需要运行这个命令： compinit回车后，对询问的问题，输入“y”，再回车。然后会发现home folder下会生成. zcompdump这个文件，同时还会存在一个. zcompdump-XXXXXXX。这时你需要记下后者的名字，然后删掉后者，再把前者名字改名为后者的。重启cygwin就看到恢复正常了。 Ruby: 因为工作用到sass，所以必须要配置好相关Ruby环境。我们来看看具体过程。 我们在安装Cygwin时已经装好了ruby，版本是1. 9. 3p448。虽然不是目前最新的2. x版本，但对于工作而言也足够了。 接下来是安装sass相关的gem。 如果能直接联网，就再简单不过了。因为我们是用到compass来编译sass，所以直接运行： gem install compass完成。 如果不能直接联网（例如内部限制等原因），那么可以通过本地安装的方式来做。我们需要先把compass的gem包下载到本地。例如对于compass，我们打开其gem页面http://rubygems. org/gems/compass，点击页面中的“Download”，即可下载gem包。 需要注意的是页面下部“Runtime Dependencies”那里，如果有列出其他gem，那么每个都需要下载下来。例如compass就列出了三个：chunky_png, fssm, sass。所以我们需要连同compass一共下载4个gem。 全部下载后，在Cygwin下进入对应的目录，运行如下命令： gem install --local chunky_png-x. x. x. gemgem install --local fssm-x. x. x. gemgem install --local sass-x. x. x. gemgem install --local compass-x. x. x. gemx. x. x是各自版本号。注意务必先安装dependencies的gem，最后安装compass。 这样compass就已可以使用了。 另外，这里强烈推荐安装一个gem：tmuxinator，它将使我们的tmux如虎添翼。具体使用方法会在下篇工作流中讲到。 小结: 至此，Cygwin的开发环境基本就算就绪了。 现在我在Cygwin中，可以进行几乎全部日常开发工作：  使用 vim/ctags 进行代码编辑； 使用 git 进行版本控制； 随时可 ssh 远程连入 server 进行操作； 使用命令compass watch监控并自动编译sass； zshell/tmux/tmuxinator 使工作效率更高效；工作流: （待续，见下篇） "
    }, {
    "id": 62,
    "url": "/2013/08/use-vagrant-and-chef-to-create-virtual-environment-quickly/",
    "title": "Use vagrant and chef to create virtual environment quickly",
    "body": "2013/08/15 - Vagrant＋Chef 快速部署虚拟机环境2013年8月15日上午10:50 前言: 在上一篇《程序员的神兵利器－Vagrant》讲到了Vagrant实在是我们程序员“居家旅行、杀人灭口”的必备工具。 当时说到使用Box源做好一个虚拟机后，一般都需要安装各种软件包和PHP、Ruby等相关的东西直到最后做好一个完整的服务器环境。 稍微有过一些SA管理员类似经验的人都会明白，这个过程实在不能算是愉快，完全是一个枯燥又乏味、不断重复再重复的累赘事。做好一台服务器环境花上好几个小时那是常有的事儿。万一中途发现遗漏了什么东西忘记安装，十有八九只能从头再来。等你好不容易做完，抬头一看，边上还有好几台机器等着你继续呢，就眼前一黑…… 好了，还是打住，不要再回顾这样辛酸的血泪史了——从现在开始可以让它一去不复返，只要你用上Vagrant。 概述: Vagrant使用了Chef来帮助我们实现快速、自动化部署的目标。SA管理员要感谢OPSCODE社区为大家做出了如此出色的Chef来极大减轻他们的工作量。（题外说一句，另一个目前很火的部署工具Puppet也很出色，更棒的是Vagrant同样内置了对Puppet的支持。不过Puppet不是本文讲述的对象，有兴趣的读者可以自己去研究。） Vagrant和Chef都是用Ruby语言写成的，再联想到当今如Rails这样不凡的东西，不禁要额外感慨一下，诞生了如此多Ruby写就的卓越作品，到底是因为Ruby要比其他语言超出一筹，还是因为全世界那些更有创造力的人才都跑到Ruby圈子里去了？更也许是因为Ruby如此优秀，以至于吸引了那么多有创造力的人才？ Chef简介: 要理解Vagrant＋Chef快速部署的实现，首先需要简单了解一下Chef。 Chef实际上就是一个专业的部署工具。对于每个软件包的安装过程，Chef定义了一整套的规范，按照这个规范的多个文件，各司其职，最后实现完整安装好整个软件包。具体细节这里我们不用关心，只需要知道这些文件最后都放在一个目录内，这个目录就叫做一个“cookbook”。每个cookbook里都可以包含多个“recipe”。 以Git为例，我们来看看一个cookbook的目录结构： . ├── Berksfile├── CHANGELOG. md├── CONTRIBUTING├── Gemfile├── LICENSE├── README. md├── TESTING. md├── attributes│   └── default. rb├── metadata. rb├── recipes│   ├── default. rb│   ├── server. rb│   ├── source. rb│   └── windows. rb└── templates  └── default    ├── git-xinetd. d. erb    ├── sv-git-daemon-log-run. erb    └── sv-git-daemon-run. erb最重要的是那三个子目录：attributes、recipes、templates。    attributes：顾名思义，目录里放置的是安装过程一些需要用到的参数设定。你可以在后面用自己的设定覆盖它；     recipes：recipe意思是“菜谱”，很形象的取名。不同的recipe实现不同的安装方式。例如default. rb会直接安装系统编译好的包（以CentOS而言，就等于yum install git），而source. rb就会采用源代码编译的方式来安装。其他类推。     templates：每个软件包安装好后，都可能有自己的conf或其他类似东西，例如Apache在/etc/httpd下的一系列conf。这些conf大部分都需要做一些调整才能用在实际环境中。你就可以把这些conf文件预先改好，放到templates下，到时Chef会自动把它们放到对应的位置上。当然了，你不能直接copy，看到后缀名erb没有？这是ruby特有的模版文件类型，你要按照正确语法来写才行。  那么每一个cookbook都需要自己从头来写么？只要你愿意当然没问题，但是完全不用去重复造轮子。OPSCODE社区已经汇集了很多做好的cookbook，相信要比你自己写的完善得多。地址在这里： OPSCODE Cookbooks：https://github. com/opscode-cookbooks 可以看到这里几乎囊括了所有常见和不常见的软件包，什么Apache、Nginx、MySQL、Git、PHP等等，应有尽有。只管拿来用吧。 另一个概念是“role”。所谓role，就是一系列cookbook的组合，例如Apache＋MySQL＋PHP＋Memcached 这样。实际上可以看出，role才是我们最后真正需要的东西。 有了对Chef了解的基础，现在可以步入正题了。 Vagrant＋Chef实现快速部署: Vagrantfile的chef部分: Vagrant对Chef的使用方式全在每个虚拟机的Vagrantfile设定文件内，默认是注释掉的，去掉注释后会是这样： config. vm. provision :chef_solo do |chef| chef. cookbooks_path =  . . /my-recipes/cookbooks  chef. roles_path =  . . /my-recipes/roles  chef. data_bags_path =  . . /my-recipes/data_bags  chef. add_recipe  mysql  chef. add_role  web  # You may also specify custom JSON attributes: chef. json = { :mysql_password =&gt;  foo  }end可以看到上述提到的各个Chef术语，现在不用担心看不明白了。 前三行是对cookbooks、roles、data_bags三个目录地址的设定。特别要强调的是，地址都是以Vagrantfile所在目录为基准的。 我们可以直接在Vagrantfile添加recipe，如同上面的add_recipe  mysql 这样；也可以通过chef. json = { :mysql_password =&gt;  foo  }来设定attribute。 但是不建议这么做，因为我们往往需要安装很多软件包，如果一一放到这里就太臃肿了，而且也不便于其他虚拟机复用。 我们采用role方式: 假设Vagrantfile所在目录叫vm，我们在vm的同级新建目录chef，chef下新建目录cookbooks、roles、data_bags。 从上面提到的opscode cookbooks github使用git clone方式下载你需要的各个cookbook，放到cookbooks目录下，例如我用到的cookbook有这么些： https://github. com/opscode-cookbooks/apache2. githttps://github. com/opscode-cookbooks/mysql. githttps://github. com/opscode-cookbooks/php. githttps://github. com/opscode-cookbooks/yum. githttps://github. com/opscode-cookbooks/openssl. githttps://github. com/opscode-cookbooks/build-essential. githttps://github. com/opscode-cookbooks/xml. githttps://github. com/yevgenko/cookbook-php-fpm. githttps://github. com/opscode-cookbooks/nginx. githttps://github. com/opscode-cookbooks/ohai. githttps://github. com/opscode-cookbooks/runit. githttps://github. com/opscode-cookbooks/vim. githttps://github. com/opscode-cookbooks/git. githttps://github. com/opscode-cookbooks/tmux. githttps://github. com/opscode-cookbooks/memcached. githttps://github. com/opscode-cookbooks/logrotate. githttps://github. com/opscode-cookbooks/man. githttps://github. com/fnichol/chef-ruby_build. githttps://github. com/opscode-cookbooks/postgresql. githttps://github. com/opscode-cookbooks/passenger_apache2. githttps://github. com/fnichol/chef-rbenv. git接下来就是创建所需要的role文件了。所有role文件都放在roles目录下，新建一个role：web-server. rb： name  web-server override_attributes(  nginx =&gt; {   version  =&gt;  1. 4. 2 ,   source =&gt; {    modules =&gt; [     http_stub_status_module ,     http_ssl_module ,     http_gzip_static_module    ]  } },  mysql  =&gt; {   server_root_password  =&gt;  1234 ,   server_repl_password  =&gt;  1234 ,   server_debian_password  =&gt;  1234  })run_list(  recipe[yum::yum] ,  recipe[yum::epel] ,  recipe[yum::remi] ,  recipe[yum::repoforge] ,  recipe[build-essential] ,  recipe[openssl] ,  recipe[man] ,  recipe[vim] ,  recipe[git] ,  recipe[tmux] ,  recipe[logrotate::global] ,  recipe[xml] ,  recipe[ohai] ,  recipe[runit] ,  recipe[memcached] ,  recipe[mysql::server] ,  recipe[nginx::source] ,  recipe[php-fpm] ,  recipe[php] ,  recipe[php::module_curl] ,  recipe[php::module_gd] ,  recipe[php::module_mcrypt] ,  recipe[php::module_memcache] ,  recipe[php::module_memcached] ,  recipe[php::module_mysql] ,  recipe[php::module_sqlite3] )name  web-server 表示这个role的名字。 override_attributes 正如之前所说，可以覆盖cookbook默认的attribute，设定成我们自己需要的。 run_list 表示了整个安装用到的全部cookbook列表。稍微要注意下的是双冒号的那些行，如recipe[mysql::server]。这表示mysql cookbook下有多个recipe，而我们只使用server那个，其他的不需要。 最后一步: 现在离大功告成只差最后一步了！我们要让Vagrant知道怎样引用所创建的role，修改Vagrantfile chef部分如下： config. vm. provision :chef_solo do |chef| chef. cookbooks_path =  . . /chef/cookbooks  chef. roles_path =  . . /chef/roles  chef. data_bags_path =  . . /chef/data_bags  # chef. add_recipe  mysql  chef. add_role  web-server  # You may also specify custom JSON attributes: # chef. json = { :mysql_password =&gt;  foo  }end见证奇迹的时刻: 好了，让我们确保网络畅通，在命令行输入vagrant up，回车！ 当当当当——开始见证Vagrant＋Chef的奇迹时刻吧！ 泡一杯咖啡，坐在屏幕前，慢慢地欣赏上面瀑布般不停滚动流出的信息文字： [2013-08-13T15:07:02+00:00] INFO: *** Chef 11. 4. 4 ***[2013-08-13T15:07:03+00:00] INFO: Setting the run_list to [ role[web-server] ] from JSON[2013-08-13T15:07:03+00:00] INFO: Run List is [role[web-server]]……[2013-08-13T15:20:30+00:00] INFO: package[php-mysql] installing php-mysql-5. 4. 17-2. el6. remi from remi repository[2013-08-13T15:20:51+00:00] INFO: template[nginx. conf] sending reload action to service[nginx] (delayed)[2013-08-13T15:20:51+00:00] INFO: bash[compile_nginx_source] sending restart action to service[nginx] (delayed)[2013-08-13T15:20:51+00:00] INFO: service[nginx] restarted[2013-08-13T15:20:51+00:00] INFO: Chef Run complete in 827. 681006003 seconds[2013-08-13T15:20:51+00:00] INFO: Running report handlers[2013-08-13T15:20:51+00:00] INFO: Report handlers complete共用时827秒，一台升级到CentOS最新系统版本，并且安装好Nginx＋MySQL＋PHP＋Memcached＋Git＋Tmux＋Vim的Web Server热腾腾出炉了！ 一气呵成啊！套句广告词，“一顺到底才叫爽”！Vagrant＋Chef就是有这么酷。 关于role: 收拾下激动的心情（相信我，我当时第一次看到这个结果时也真的被震撼了！），再说两句role。如同上面的web-server. rb那样，我们完全可以针对不同的服务器环境做好不同的role，例如DB Server来个db-server. rb，Rails Server来个rails-server. rb……以此类推。到时候，想要什么样的服务器，一转眼就端盘上菜了。 然后，就没有然后了。一劳永逸说的就是这个。繁重的工作从此远去，过程反而成为乐趣。 最后: Vagrant＋Chef，可以用于快速部署我们的开发服务器环境，或者是对于线上生产服务器环境的模拟。但总而言之，归根结底都是虚拟机环境，这是由Vagrant本身所决定的。 有读者很可能会问，那么对于真实的服务器环境怎么办呢？也能做到这样一气呵成的部署吗？ 当然是可以的。但这个就和Vagrant没有关系了，这时要用到的是Chef。关于Chef的详细讲述将又是另一篇好大文章了。 "
    }, {
    "id": 63,
    "url": "/2013/08/vagrant-is-a-powerfull-tool-for-developer/",
    "title": "A powerful tool for developer, Vagrant",
    "body": "2013/08/14 - 程序员的神兵利器－Vagrant2013年8月14日上午10:50 参考网址：: Vagrant官网 Railscasts: Virtual Machines with Vagrant 缘起: 最近发现了一个超酷的东西——Vagrant，一见之下惊为天人。真是又高兴又伤心，高兴的是这东西简直太棒了，从此自己的“兵器谱”里又增加一件利器；伤心的是怎么现在才看到，实在是相见恨晚啊。 什么是Vagrant: 简单来说，Vagrant 就是一个虚拟机的集成管理器。 我们用它可以快速创建虚拟机，可以快速部署好所需的各种环境，无论你想要开发环境或是上线环境都能一键搞定。甚至你可以部署多台都没问题。 想想看，假设一台Application Server，再加一台Master Database + 一台Slave Database，也许可以再来一台Monitor Server等等，全部都可以用你那台开发工作的电脑来实现，只要有足够内存。是不是很牛的样子？所以说，要让我们不用Vagrant，给个理由先？ 安装: 我们所需要的实际是 Vagrant＋Virtualbox 这两样东西。 Vagrant可以在上面列出的官网去下载软件包，目前最新版本是1. 2. 7。 Vagrant支持的是Virtualbox这个虚拟机软件。Virtualbox是开源的，它以前是独立发布，现在已经被Oracle收购了，直接去官网下载安装即可。另外，Vagrant还支持VMvare虚拟机，不过这个功能是收费的，一般就不用考虑了。 基本使用: 添加Box源: 我们要先确定使用什么系统，是准备用Ubuntu，还是上Centos，或者BSD系列。确定好后，我们就可以到vagrantbox. es这个地方查找。Vagrant把每个打包好的虚拟机系统叫做box。这里都是网络上的热心人已经打包好的box，各种系统都有，必有一款能满足你的需要。 例如我习惯使用CentOS，就可以这样来添加一个box到Vagrant里： vagrant box add CentOS-64 http://developer. nrel. gov/downloads/vagrant-boxes/CentOS-6. 4-x86_64-v20130427. box这样Vagrant就会下载这个box，下载完成后添加到自己的box列表里。可以用： vagrant box list来查看。如果网速不够快，一个技巧就是可以先把这个box下载到本地，然后在上面命令中使用本地文件地址就好了。 每个box都相当一个系统的安装源，接下来我们就要用到了。 开始第一个虚拟机: 我们新建一个工作目录vm，然后在vm里使用这个命令： vagrant init CentOS-64然后我们会看到如下提示信息： A `Vagrantfile` has been placed in this directory. You are nowready to `vagrant up` your first virtual environment! Please readthe comments in the Vagrantfile as well as documentation on`vagrantup. com` for more information on using Vagrant. Vagrant在vm目录生成了一个“Vagrantfile”的设定文件。全部的设定都在这个文件里，我们先不管，以后再研究。现在先让虚拟机跑起来！ 输入命令： vagrant up稍微等待一段时间后，我们就会看到Vagrant会输出很多行信息： Bringing machine 'default' up with 'virtualbox' provider. . . [default] Setting the name of the VM. . . [default] Clearing any previously set forwarded ports. . . [default] Creating shared folders metadata. . . [default] Clearing any previously set network interfaces. . . [default] Preparing network interfaces based on configuration. . . [default] Forwarding ports. . . [default] -- 22 =&gt; 2222 (adapter 1)[default] Booting VM. . . [default] Waiting for VM to boot. This can take a few minutes. [default] VM booted and ready for use![default] Configuring and enabling network interfaces. . . [default] Mounting shared folders. . . [default] -- /vagrantYes！虚拟机已经正式在运行了。 那么怎么连上去呢？照常规那样开一个ftp软件，输入ip、帐号、密码去连接吗？No！ vagrant ssh这个命令直接就把你送进虚拟机去了。默认帐户是vagrant，密码一样。现在开始折腾这台机器吧…… 另外，虚拟机已经有一个/vagrant目录，和我所在的vm目录是直接映射的。vm里的任何文件都能在/vagrant目录里看到和使用。酷吧？ 打包自己的Box: 当你把自己的虚拟机做好所需的环境，例如我的CentOS-64会升级好yum软件包，安装好MySQL、PHP等环境后，不想以后每次都把这个过程重来一次，或者假如在团队里不想每个成员的开发环境各自五花八门乱七八糟，我就可以把这个做好的CentOS-64打包出来，分享给团队成员。 vagrant package稍等一点时间后，Vagrant会在vm目录下输出一个package. box的文件。没错，这完全跟我们在上面vagrantbox. es下载的box文件一样的，实际上那里热心人分享的box文件都是这么来的。所以可以用同样的方法添加到box列表里去，例如： vagrant box add CentOS-64S package. box这样以后我们就可以直接使用这个新box来生成虚拟机： vagrant init CentOS-64S新虚拟机直接就是一个做好的PHP Server。 原来的CentOS-64这个box完全可以删掉了，还可以节省一点空间： vagrant box remove CentOS-64虚拟机的设定: 上面说到Vagrant会为每个虚拟机都生成一个Vagrantfile设定文件。 用任意文本编辑器打开它，如果你对Ruby语言有一点了解的话，就会明白实际整个内容都是Ruby Code。而且Vagrant很贴心的准备了详尽到有些啰嗦的注释给你，这里强烈建议你仔细看看。实际对于每个选项设定都说的非常清楚了。 主要的设定大概有这么些： 注：下面提到的设定有些是默认注释掉的，没有开启，务必要取消注释才会生效。 config. vm. box =  CentOS-64 这指定了虚拟机使用哪个Box源。 config. vm. network :forwarded_port, guest: 80, host: 8080这个设定非常牛，它会把Host机器（就是安装Vagrant的机器）的8080端口转发（forwoard）到虚拟机的80端口。例如你部署到虚拟机的网站运行后，当你在Host机器上浏览器打开http://localhost:8080后，就会自动转到虚拟机正在运行的Apache或Nginx 80端口服务，也就是访问部署的网站。实际上，这个功能是架起了Host机器和虚拟机之间沟通的桥梁。 以此类推，我们可以增加更多的端口转发，如常用的MySQL 3306端口，Rails 的3000端口等。 config. vm. network :private_network, ip:  192. 168. 33. 10 config. vm. network :public_network这两个是设定网络连接方式。前者把虚拟机网络设定为私有模式，和你Host机器同一网络的其他电脑是看不到它的。后者相反，设为公开模式，和你Host机器有类似的IP，同一网络的其他电脑都能看到它。一般都采用前者，而且IP也建议不以“192. 168”开头，以免冲突，例如可以设为“66. 66. 66. 10”这样…… vb. customize [ modifyvm , :id,  --memory ,  1024 ]这个可以手工设定虚拟机使用多少内存，根据你自己情况来定就好。类似Linux这样的Server，一般512就够了。 其他的设定还有些，不是特别重要，读者可以自行去了解看看了。 多台虚拟机组合: 我们的产品正式部署上线的时侯，经常都可能不是一台而采用多台服务器的情况。例如 Database 要和 Application 分开，Database 有时还有 Master、Slave 之分，有时还需要 Balance Server 等等。更重要的是，这么多台机器之间都是需要相互沟通的。那么在正式部署前进行实况模拟就是很有必要的了。 这时就正是Vagrant的英雄用武之地，这可是Vagrant的“杀手级功能”。重头戏登场！ 我们来架设一台Web Server，一台Master Database ＋ 一台Slave Database 的组合。 在Vagrantfile设定文件里改成这样： Vagrant. configure( 2 ) do |config| config. vm. define :web do |web|  web. vm. box =  CentOS-64   web. vm. network :private_network, ip:  66. 66. 66. 10  end config. vm. define :db0 do |db|  db. vm. box =  CentOS-64   db. vm. network :private_network, ip:  66. 66. 66. 20  end config. vm. define :db1 do |db|  db. vm. box =  CentOS-64   db. vm. network :private_network, ip:  66. 66. 66. 21  endend各项设定如同上述，只是名称分别使用了“web”、“db0”、“db1”，并且设定了不同的IP。 然后我们使用vagrant up启动，可以看到跑出来很多行信息，注意到每行前面都有类似“[web]”、“[db0]”这样的开头，表示各自是哪一台机器。信息流停止后，表示我们的虚拟机组合跑起来了！ SSH可以指定连到哪一台去： vagrant ssh webvagrant ssh db0酷得一塌糊涂啊！！ 再来看看各台虚拟机之间的连通。 先进去到Web Server： vagrant ssh web在web机器里连接db0: ssh 66. 66. 66. 20轻而易举大功告成！ 还有更牛的东西在后头呢。Vagrant允许你单独启动组合里某一台虚拟机，例如： vagrant up web只启动了Web Server，Database 机器并没有启动。 Vagrant甚至允许你在启动时使用正则表达式： vagrant up /db[0-9]/现在你启动全部Database Server了，即使你有db0,db1,db2…db9 这么多台也没问题。 现在对Vagrant有一个大概的了解了吧，就赶紧去试一试吧，相信你会像我一样喜欢上它。 "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><span class='body'>"+ body +"</span><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-primary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><small><span class='body'>"+ body +"</span><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});